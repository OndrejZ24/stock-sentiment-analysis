{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6c71c795",
   "metadata": {},
   "source": [
    "# Stock Sentiment Analysis - Preprocessing Pipeline\n",
    "\n",
    "This notebook performs the preprocessing pipeline for stock sentiment analysis:\n",
    "1. Database Connection - Connect to Oracle database and fetch the data\n",
    "2. Ticker Data Loading - Fetch and load NASDAQ/NYSE ticker symbols\n",
    "3. Data Harmonization - Merge posts and comments into unified schema (zatim je to takto, i kdyz trackovat komentare pro urcity posty neni skrze primarni a cizi klic tak tezky, ale pokud je ticke v postu, ten samy ticker se zakonite nemusi resit v komentari pod tim u know. proto je zatim gold standard brat sentiment (pozdeji) z komentu/postu ktery ticker/y primo obsahuje. na tohle musime dat meeting)\n",
    "4. Data Cleaning - Removing invalid, deleted, and short texts proste cisteni \n",
    "5. Feature Engineering - Add temporal and engagement features (ruzny casovy prurezy atd - pro budouci poreby)\n",
    "6. Ticker Detection - Identify stock ticker mentions with high precision \n",
    "7. Text Normalization - Prepare text for sentiment analysis\n",
    "8. Export Results - Save processed data for further analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6a1346aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLTK rdy\n",
      "spaCy rdy\n",
      "Imports loaded successfully letzgooo!\n",
      "spaCy rdy\n",
      "Imports loaded successfully letzgooo!\n"
     ]
    }
   ],
   "source": [
    "# Imports and Configuration\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import logging\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "from datetime import datetime\n",
    "from typing import List, Set, Dict, Any\n",
    "from tqdm import tqdm\n",
    "from io import StringIO\n",
    "import urllib.request\n",
    "import urllib.parse\n",
    "\n",
    "# Environment and database\n",
    "try:\n",
    "    from dotenv import load_dotenv\n",
    "    load_dotenv()\n",
    "    DOTENV_AVAILABLE = True\n",
    "except ImportError:\n",
    "    DOTENV_AVAILABLE = False\n",
    "\n",
    "try:\n",
    "    import oracledb\n",
    "    ORACLE_AVAILABLE = True\n",
    "except ImportError:\n",
    "    ORACLE_AVAILABLE = False\n",
    "    print(\"Oracle DB not available. Jeste nejsme cooked -> pip install oracledb\")\n",
    "\n",
    "# Check for NLP libraries\n",
    "try:\n",
    "    import nltk\n",
    "    NLTK_AVAILABLE = True\n",
    "    print(\"NLTK rdy\")\n",
    "except ImportError:\n",
    "    NLTK_AVAILABLE = False\n",
    "    print(\"NLTK not available -> pip install nltk\")\n",
    "\n",
    "try:\n",
    "    import spacy\n",
    "    SPACY_AVAILABLE = True\n",
    "    print(\"spaCy rdy\")\n",
    "except ImportError:\n",
    "    SPACY_AVAILABLE = False\n",
    "    print(\"spaCy not available -> pip install spacy\")\n",
    "\n",
    "# Import functions from utils\n",
    "from utils import (\n",
    "    get_oracle_connection,\n",
    "    get_all_us_tickers,\n",
    "    detect_tickers_in_text,\n",
    "    apply_ticker_detection,\n",
    "    harmonize_schema,\n",
    "    drop_invalid_texts,\n",
    "    deduplicate_and_normalize_types,\n",
    "    add_temporal_features,\n",
    "    add_engagement_features,\n",
    "    apply_text_normalization,\n",
    "    remove_financial_stopwords,\n",
    "    remove_stopwords_spacy\n",
    ")\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s [%(levelname)s] %(message)s\")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Configuration constants\n",
    "MIN_TEXT_LENGTH = 10\n",
    "RETRY_DELAY = 10\n",
    "\n",
    "print(\"Imports loaded successfully letzgooo!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6c10b68e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP 1: Importing Reddit Data from Oracle Database\n",
      "Oracle credentials found pojdme se pripojit :)\n",
      "Oracle connection successful!\n",
      "Database connection successful letzgoo ðŸš€\n",
      "Importing Reddit data from existing tables...\n",
      "Oracle connection successful!\n",
      "Database connection successful letzgoo ðŸš€\n",
      "Importing Reddit data from existing tables...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/33/r5z8ht2928103xc7s_sb_xx80000gq/T/ipykernel_86568/860778154.py:33: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df_posts = pd.read_sql_query(query_posts, conn)\n",
      "/var/folders/33/r5z8ht2928103xc7s_sb_xx80000gq/T/ipykernel_86568/860778154.py:44: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df_comments = pd.read_sql_query(query_comments, conn)\n",
      "/var/folders/33/r5z8ht2928103xc7s_sb_xx80000gq/T/ipykernel_86568/860778154.py:44: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df_comments = pd.read_sql_query(query_comments, conn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Database connection closed\n",
      "\n",
      "Posts imported: (5000, 10)\n",
      "Comments imported: (5000, 7)\n",
      "Posts columns: ['AUTHOR', 'TITLE', 'CREATED_UTC', 'ID', 'IS_ORIGINAL_CONTENT', 'SCORE', 'BODY', 'SUBREDDIT', 'UPVOTE_RATIO', 'URL']\n",
      "Comments columns: ['AUTHOR', 'CREATED_UTC', 'ID', 'PARENT_POST_ID', 'SCORE', 'BODY', 'SUBREDDIT']\n",
      "\n",
      "Step 1 Complete: Loaded 5000 posts and 5000 comments. Letzgoo\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Database Connection and Data Loading\n",
    "\n",
    "print(\"STEP 1: Importing Reddit Data from Oracle Database\")\n",
    "\n",
    "# Check if Oracle DB is available and credentials are set\n",
    "oracle_credentials_available = (\n",
    "    ORACLE_AVAILABLE and \n",
    "    os.getenv('db-username') and \n",
    "    os.getenv('db-password') and\n",
    "    os.getenv('db-dsn')\n",
    ")\n",
    "\n",
    "if oracle_credentials_available:\n",
    "    print(\"Oracle credentials found pojdme se pripojit :)\")\n",
    "    \n",
    "    conn = get_oracle_connection()\n",
    "\n",
    "    if conn:\n",
    "        print(\"Database connection successful letzgoo ðŸš€\")\n",
    "        \n",
    "        # Import data from database\n",
    "        print(\"Importing Reddit data from existing tables...\")\n",
    "        try:\n",
    "            # Query to import posts \n",
    "            query_posts = \"\"\"\n",
    "                SELECT \n",
    "                    author, title, created_utc, id, is_original_content,\n",
    "                    score, DBMS_LOB.SUBSTR(body, 4000, 1) as body, \n",
    "                    subreddit, upvote_ratio, url\n",
    "                FROM historical_posts \n",
    "                WHERE ROWNUM <= 5000\n",
    "            \"\"\"\n",
    "            df_posts = pd.read_sql_query(query_posts, conn)\n",
    "            \n",
    "            # Query to import comments \n",
    "            query_comments = \"\"\"\n",
    "                SELECT \n",
    "                    author, created_utc, id, parent_post_id, score,\n",
    "                    DBMS_LOB.SUBSTR(body, 4000, 1) as body,\n",
    "                    subreddit\n",
    "                FROM historical_comments \n",
    "                WHERE ROWNUM <= 5000\n",
    "            \"\"\"\n",
    "            df_comments = pd.read_sql_query(query_comments, conn)\n",
    "            \n",
    "            # Close connection immediately after data import\n",
    "            conn.close()\n",
    "            print(\"Database connection closed\")\n",
    "\n",
    "            print(f\"\\nPosts imported: {df_posts.shape}\")\n",
    "            print(f\"Comments imported: {df_comments.shape}\")\n",
    "            \n",
    "            if len(df_posts) > 0:\n",
    "                print(f\"Posts columns: {list(df_posts.columns)}\")\n",
    "            \n",
    "            if len(df_comments) > 0:\n",
    "                print(f\"Comments columns: {list(df_comments.columns)}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error importing data from database: {e}\")\n",
    "            if conn:\n",
    "                try:\n",
    "                    conn.close()\n",
    "                except:\n",
    "                    pass\n",
    "            df_posts = pd.DataFrame()\n",
    "            df_comments = pd.DataFrame()\n",
    "            \n",
    "    else:\n",
    "        print(\"Failed to connect to database (tak to je v pici - check logs)\")\n",
    "        df_posts = pd.DataFrame()\n",
    "        df_comments = pd.DataFrame()\n",
    "        \n",
    "else:\n",
    "    print(\"Oracle database credentials not configured\")\n",
    "    print(\"Skipping database import...\")\n",
    "    df_posts = pd.DataFrame()\n",
    "    df_comments = pd.DataFrame()\n",
    "\n",
    "print(f\"\\nStep 1 Complete: Loaded {len(df_posts)} posts and {len(df_comments)} comments. Letzgoo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9bd8a27f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading cached ticker data from us_tickers.csv\n",
      "Loaded 8022 cached tickers (zatim dobry)\n",
      "\n",
      "Ticker data summary:\n",
      "  Total tickers: 8022\n",
      "  Exchanges: {'NASDAQ': 5142, 'NYSE': 2880}\n",
      "  Major tickers found: {'AMZN', 'GOOGL', 'TSLA', 'NVDA', 'AAPL', 'META', 'MSFT'}\n",
      "\n",
      "Step 2 Complete: Loaded 8022 ticker symbols letzgoo ðŸš€\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Ticker Data Loading\n",
    "# Check if we have cached ticker data\n",
    "us_tickers_path = \"us_tickers.csv\"\n",
    "\n",
    "if os.path.exists(us_tickers_path):\n",
    "    print(f\"Loading cached ticker data from {us_tickers_path}\")\n",
    "    try:\n",
    "        tickers_df = pd.read_csv(us_tickers_path, dtype=str)\n",
    "        # Normalizing column names\n",
    "        tickers_df.columns = [c.strip().lower().replace(\" \", \"_\") for c in tickers_df.columns]\n",
    "        if 'ticker' in tickers_df.columns:\n",
    "            tickers_df['ticker'] = tickers_df['ticker'].astype(str).str.upper().str.strip()\n",
    "        print(f\"Loaded {len(tickers_df)} cached tickers (zatim dobry)\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading cached data (ou nou): {e}\")\n",
    "        tickers_df = None\n",
    "else:\n",
    "    print(\"Fetching fresh ticker data\")\n",
    "    tickers_df = None\n",
    "\n",
    "# If no cached data or fresh data\n",
    "if tickers_df is None or len(tickers_df) == 0:\n",
    "    try:\n",
    "        tickers_df = get_all_us_tickers()\n",
    "        if len(tickers_df) > 0:\n",
    "            tickers_df.to_csv(us_tickers_path, index=False)\n",
    "            print(f\"Fetched and cached {len(tickers_df)} US tickers (so far so good)\")\n",
    "        else:\n",
    "            print(\"No ticker data retrieved (fuck)\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching ticker data (fuck): {e}\")\n",
    "        tickers_df = pd.DataFrame()\n",
    "\n",
    "if len(tickers_df) > 0:\n",
    "    print(f\"\\nTicker data summary:\")\n",
    "    print(f\"  Total tickers: {len(tickers_df)}\")\n",
    "    print(f\"  Exchanges: {tickers_df['exchange'].value_counts().to_dict()}\")\n",
    "    \n",
    "    # Check for major tickers - pojistka proste\n",
    "    major_tickers = {'AAPL', 'TSLA', 'MSFT', 'AMZN', 'GOOGL', 'NVDA', 'META'}\n",
    "    found_major = set(tickers_df['ticker']) & major_tickers\n",
    "    print(f\"  Major tickers found: {found_major}\")\n",
    "    \n",
    "else:\n",
    "    print(\"No ticker data available (well fuck)\")\n",
    "    \n",
    "print(f\"\\nStep 2 Complete: Loaded {len(tickers_df)} ticker symbols letzgoo ðŸš€\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "21117e4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/simon2/Library/CloudStorage/OneDrive-Personal/UCÌŒENIÌ/TextovaÌ analytika 2/stock-sentiment-analysis/utils.py:303: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  unified = pd.concat([posts, comments], ignore_index=True, sort=False)\n",
      "INFO:utils:Harmonized schema. Unified dataframe shape: (10000, 12)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unified dataframe shape: (10000, 12)\n",
      "Unified columns: ['author', 'title', 'created_utc', 'id', 'is_original_content', 'score', 'text', 'subreddit', 'upvote_ratio', 'url', 'parent_post_id', 'type']\n",
      "\n",
      "Data distribution:\n",
      "  post: 5000 rows\n",
      "  comment: 5000 rows\n",
      "\n",
      "Step 3 Complete: Unified 10000 rows. Fuck yeaaah ðŸš€\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Data Harmonization - sloucime posty a komenty (zatim bez nejaky hierarchie)\n",
    "\n",
    "\n",
    "if len(df_posts) > 0 or len(df_comments) > 0:\n",
    "    # combine data\n",
    "    df_unified = harmonize_schema(df_posts, df_comments)\n",
    "    print(f\"Unified dataframe shape: {df_unified.shape}\")\n",
    "    print(f\"Unified columns: {list(df_unified.columns)}\")\n",
    "    \n",
    "    # Show data type distribution\n",
    "    type_counts = df_unified['type'].value_counts()\n",
    "    print(f\"\\nData distribution:\")\n",
    "    for dtype, count in type_counts.items():\n",
    "        print(f\"  {dtype}: {count} rows\")\n",
    "    \n",
    "else:\n",
    "    print(\"No data to harmonize\")\n",
    "    df_unified = pd.DataFrame()\n",
    "\n",
    "print(f\"\\nStep 3 Complete: Unified {len(df_unified)} rows. Fuck yeaaah ðŸš€\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "979d1037",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:utils:Dropped invalid/short texts. Remaining rows: 6829\n",
      "INFO:utils:Deduplicated and normalized types.\n",
      "INFO:utils:Deduplicated and normalized types.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting with 10000 rows\n",
      "    After removing invalid texts: 6829 rows (-3171)\n",
      "    After deduplication: 6829 rows (tohle by melo byt idealne kladny cislo u know)\n",
      "  Original rows: 10000\n",
      "  Cleaned rows: 6829\n",
      "  Removed: 3171 (31.7%)\n",
      "\n",
      "Data types after cleaning:\n",
      "  comment: 4677 rows\n",
      "  post: 2152 rows\n",
      "\n",
      "Step 4 Complete: 6829 clean rows\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Data Cleaning\n",
    "\n",
    "if len(df_unified) > 0:\n",
    "    print(f\"Starting with {len(df_unified)} rows\")\n",
    "    \n",
    "    # Step 4a: Remove invalid texts\n",
    "    df_cleaned = drop_invalid_texts(df_unified, min_len=MIN_TEXT_LENGTH)\n",
    "    print(f\"    After removing invalid texts: {len(df_cleaned)} rows (-{len(df_unified) - len(df_cleaned)})\")\n",
    "    \n",
    "    # Step 4b: Deduplicate and normalize types\n",
    "    df_cleaned = deduplicate_and_normalize_types(df_cleaned)\n",
    "    print(f\"    After deduplication: {len(df_cleaned)} rows (tohle by melo byt idealne kladny cislo u know)\")\n",
    "    \n",
    "    # Show cleaning results\n",
    "    if len(df_cleaned) > 0:\n",
    "        print(f\"  Original rows: {len(df_unified)}\")\n",
    "        print(f\"  Cleaned rows: {len(df_cleaned)}\")\n",
    "        print(f\"  Removed: {len(df_unified) - len(df_cleaned)} ({((len(df_unified) - len(df_cleaned))/len(df_unified)*100):.1f}%)\")\n",
    "        \n",
    "        # Show data types after cleaning\n",
    "        print(f\"\\nData types after cleaning:\")\n",
    "        type_counts = df_cleaned['type'].value_counts()\n",
    "        for dtype, count in type_counts.items():\n",
    "            print(f\"  {dtype}: {count} rows\")\n",
    "    else:\n",
    "        print(\"No data remaining after cleaning - tohle neni uplne dobry :)\")\n",
    "        \n",
    "else:\n",
    "    print(\"No data to clean (jsme v prdeli)\")\n",
    "    df_cleaned = pd.DataFrame()\n",
    "\n",
    "print(f\"\\nStep 4 Complete: {len(df_cleaned)} clean rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d42e97b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Feature Engineering\n",
    "\n",
    "if len(df_cleaned) > 0:\n",
    "    df_features = add_temporal_features(df_cleaned)\n",
    "    df_features = add_engagement_features(df_features)\n",
    "    \n",
    "    print(f\"Enhanced dataframe shape: {df_features.shape}\")\n",
    "    \n",
    "    # new features\n",
    "    new_features = ['date', 'hour', 'day_of_week', 'month', 'is_weekend', \n",
    "                   'text_length', 'word_count', 'score_log1p']\n",
    "    print(f\"\\nNew features added: {[f for f in new_features if f in df_features.columns]}\")\n",
    "    \n",
    "    # Show feature statistics\n",
    "    if 'text_length' in df_features.columns:\n",
    "        print(f\"  Text length: min={df_features['text_length'].min()}, \"\n",
    "              f\"mean={df_features['text_length'].mean():.1f}, \"\n",
    "              f\"max={df_features['text_length'].max()}\")\n",
    "    \n",
    "    if 'word_count' in df_features.columns:\n",
    "        print(f\"  Word count: min={df_features['word_count'].min()}, \"\n",
    "              f\"mean={df_features['word_count'].mean():.1f}, \"\n",
    "              f\"max={df_features['word_count'].max()}\")\n",
    "    \n",
    "    if 'day_of_week' in df_features.columns:\n",
    "        day_counts = df_features['day_of_week'].value_counts()\n",
    "        print(f\"  Day distribution: {day_counts.to_dict()}\")\n",
    "    \n",
    "    if 'is_weekend' in df_features.columns:\n",
    "        weekend_pct = df_features['is_weekend'].mean() * 100\n",
    "        print(f\"  Weekend posts: {weekend_pct:.1f}%\")\n",
    "    \n",
    "else:\n",
    "    print(\"No data for feature engineering (upsis)\")\n",
    "    df_features = pd.DataFrame()\n",
    "\n",
    "print(f\"\\nStep 5 Complete: {len(df_features)} rows with enhanced features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18f9518c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Ticker Detection \n",
    "\n",
    "if len(df_features) > 0 and len(tickers_df) > 0:\n",
    "    print(f\"Detecting tickers in {len(df_features)} texts using {len(tickers_df)} symbols\")\n",
    "    \n",
    "    # improved ticker detection\n",
    "    df_with_tickers = apply_ticker_detection(df_features, tickers_df)\n",
    "    \n",
    "    # results\n",
    "    ticker_stats = df_with_tickers['n_tickers'].value_counts().sort_index()\n",
    "    total_with_tickers = (df_with_tickers['n_tickers'] > 0).sum()\n",
    "    print(f\"  Total rows: {len(df_with_tickers)}\")\n",
    "    print(f\"  Rows with tickers: {total_with_tickers} ({total_with_tickers/len(df_with_tickers)*100:.1f}%)\")\n",
    "    print(f\"  Rows without tickers: {len(df_with_tickers) - total_with_tickers}\")\n",
    "    \n",
    "    print(f\"\\nTicker count distribution:\")\n",
    "    for count, rows in ticker_stats.head(10).items():\n",
    "        print(f\"  {count} tickers: {rows} rows\")\n",
    "    \n",
    "    # Show if any tickers were found\n",
    "    ticker_examples = df_with_tickers[df_with_tickers['n_tickers'] > 0]\n",
    "    if len(ticker_examples) > 0:\n",
    "        print(f\"\\nTicker detection successful: Found {len(ticker_examples)} rows with ticker mentions\")\n",
    "    else:\n",
    "        print(f\"\\nNo tickers detected - this indicates high precision (no false positives)\")\n",
    "        print(\"Testing detection with synthetic examples:\")\n",
    "        \n",
    "        # Test with known ticker-rich text (projistotu)\n",
    "        ticker_set = set(tickers_df['ticker'])\n",
    "        test_texts = [\n",
    "            \"I'm buying $AAPL and TSLA today\",\n",
    "            \"MSFT and GOOGL are performing well\",\n",
    "            \"Just some random text without tickers\"\n",
    "        ]\n",
    "        \n",
    "        for test_text in test_texts:\n",
    "            detected = detect_tickers_in_text(test_text, ticker_set)\n",
    "            print(f\"    '{test_text}' â†’ {detected}\")\n",
    "    \n",
    "else:\n",
    "    print(\"No data or tickers available for detection\")\n",
    "    df_with_tickers = pd.DataFrame()\n",
    "\n",
    "print(f\"\\nStep 6 Complete: Processed {len(df_with_tickers)} rows for ticker detection\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ccc4a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 7: Text Normalization and Stopword Removal\n",
    "\n",
    "if len(df_with_tickers) > 0:\n",
    "\n",
    "    # Basic text normalization (creates 'sentiment_ready_text' column)\n",
    "    df_final = apply_text_normalization(df_with_tickers, keep_tickers=True)\n",
    "    \n",
    "    # Apply additional stopword removal to improve the text further\n",
    "    \n",
    "    if SPACY_AVAILABLE:\n",
    "        print(\"Using spaCy\")\n",
    "        df_final['sentiment_ready_text'] = df_final['sentiment_ready_text'].apply(\n",
    "            lambda x: remove_stopwords_spacy(x, preserve_tickers=True)\n",
    "        )\n",
    "        stopword_method = \"spaCy\"\n",
    "    elif NLTK_AVAILABLE:\n",
    "        print(\"Using NLTK\")\n",
    "        df_final['sentiment_ready_text'] = df_final['sentiment_ready_text'].apply(\n",
    "            lambda x: remove_financial_stopwords(x, preserve_tickers=True)\n",
    "        )\n",
    "        stopword_method = \"NLTK\"\n",
    "    else:\n",
    "        print(\"Using built-in stopword removal (lame as fuck)\")\n",
    "        df_final['sentiment_ready_text'] = df_final['sentiment_ready_text'].apply(\n",
    "            lambda x: remove_financial_stopwords(x, preserve_tickers=True)\n",
    "        )\n",
    "        stopword_method = \"Built-in\"\n",
    "    \n",
    "    print(f\"Text normalization complete using {stopword_method}\")\n",
    "    print(f\"Final dataframe shape: {df_final.shape}\")\n",
    "    print(f\"Final columns: {list(df_final.columns)}\")\n",
    "    \n",
    "    # Final statistics\n",
    "    print(f\"\\nFinal dataset statistics:\")\n",
    "    print(f\"  Total rows: {len(df_final)}\")\n",
    "    print(f\"  Rows with tickers: {(df_final['n_tickers'] > 0).sum()}\")\n",
    "    print(f\"  Average original text length: {df_final['text_length'].mean():.1f} characters\")\n",
    "    print(f\"  Average word count: {df_final['word_count'].mean():.1f} words\")\n",
    "    \n",
    "    # Show text processing impact\n",
    "    avg_original_length = df_final['text'].str.len().mean()\n",
    "    avg_sentiment_ready_length = df_final['sentiment_ready_text'].str.len().mean()\n",
    "    \n",
    "    print(f\"\\nimpact:\")\n",
    "    print(f\"  Original text length: {avg_original_length:.1f} chars\")\n",
    "    print(f\"  Sentiment-ready text length: {avg_sentiment_ready_length:.1f} chars\")\n",
    "    print(f\"  Reduction from normalization: {((avg_original_length - avg_sentiment_ready_length) / avg_original_length * 100):.1f}%\")\n",
    "    print(f\"  Stopword removal method: {stopword_method}\")\n",
    "    \n",
    "    if 'type' in df_final.columns:\n",
    "        type_dist = df_final['type'].value_counts()\n",
    "        print(f\"  Content distribution: {type_dist.to_dict()}\")\n",
    "    \n",
    "else:\n",
    "    print(\"No data for text normalization\")\n",
    "    df_final = pd.DataFrame()\n",
    "\n",
    "print(f\"\\nStep 7 Complete: {len(df_final)} rows ready for sentiment analysis (letzgoo ðŸš€)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5320390",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 8: Export Sentiment-Ready Data\n",
    "\n",
    "if len(df_final) > 0:\n",
    "    print(f\"Available columns: {list(df_final.columns)}\")\n",
    "    \n",
    "    # Export to CSV \n",
    "    output_file = \"sentiment_ready_data.csv\"\n",
    "    \n",
    "    # Updated key columns including exchange information\n",
    "    sentiment_columns = [\n",
    "        'id', 'text', 'sentiment_ready_text', 'type', 'subreddit', \n",
    "        'created_utc', 'score', 'mentioned_tickers', 'n_tickers', 'ticker_exchanges',\n",
    "        'text_length', 'word_count', 'date', 'hour', 'day_of_week']\n",
    "    \n",
    "    # Keep only columns that exist\n",
    "    export_columns = [col for col in sentiment_columns if col in df_final.columns]\n",
    "    export_df = df_final[export_columns].copy()\n",
    "    \n",
    "    # Save to CSV\n",
    "    export_df.to_csv(output_file, index=False)\n",
    "    print(f\"Exported {len(export_df)} rows to {output_file}\")\n",
    "    print(f\"Exported columns: {export_columns}\")\n",
    "    \n",
    "    # Show exchange distribution\n",
    "    if 'ticker_exchanges' in export_df.columns:\n",
    "        exchange_dist = export_df[export_df['ticker_exchanges'] != '']['ticker_exchanges'].value_counts()\n",
    "        print(f\"\\nExchange distribution (rows with tickers):\")\n",
    "        for exchange, count in exchange_dist.items():\n",
    "            print(f\"  {exchange}: {count} rows\")\n",
    "    \n",
    "    print(f\"\\nData is ready for sentiment analysis\")\n",
    "    print(f\"Use the 'sentiment_ready_text' column for sentiment modeling\")\n",
    "    print(f\"Use the 'mentioned_tickers' column for ticker information\")\n",
    "    print(f\"Use the 'ticker_exchanges' column for exchange information (NYSE/NASDAQ/BOTH)\")\n",
    "    \n",
    "else:\n",
    "    print(\"No data to export (gg well played)\")\n",
    "\n",
    "print(f\"\\nPreprocessing Pipeline Complete (letzgoo ðŸš€)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
