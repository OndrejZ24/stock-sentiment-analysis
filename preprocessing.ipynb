{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6c71c795",
   "metadata": {},
   "source": [
    "# Preprocessing — Overview\n",
    "\n",
    "1. Setup — imports & environment\n",
    "2. Database — load `historical_posts` and `historical_comments` from Oracle\n",
    "3. Load tickers — load and normalize ticker list (NASDAQ + NYSE)\n",
    "4. Data harmonization <br>\n",
    "   4.1 Unify schema into `df_unified`<br>\n",
    "   4.2 Placeholder-only rows analysis<br>\n",
    "   4.3 Remove [removed]/[deleted] placeholders<br>\n",
    "   4.4 Merge `title` + `body` for posts into `text`<br>\n",
    "5. EDA — overview, missingness, text length, temporal analysis <br>\n",
    "   5.1 Data types & schema overview<br>\n",
    "   5.2 Column value distributions<br>\n",
    "   5.3 Overview — counts & missingness<br>\n",
    "   5.4 Text characteristics — word count histograms<br>\n",
    "   5.5 Temporal analysis — day/hour distribution<br>\n",
    "   5.6 Engagement metrics — upvote ratio<br>\n",
    "   5.7 Integrity and duplicates — ID uniqueness<br>\n",
    "6. Data Cleaning — remove invalid/empty records, deduplicate, drop unnecessary columns\n",
    "7. Feature engineering — text_length, word_count, temporal features, engagement features, normalized_score\n",
    "8. Ticker detection <br>\n",
    "   8.1 Detect `mentioned_tickers`, `n_tickers`<br>\n",
    "   8.2 Filter false positive tickers using stopwords<br>\n",
    "   8.3 Ticker inheritance — inherit parent post tickers to comments<br>\n",
    "   8.4 Inheritance success check<br>\n",
    "   8.5 Data Type Conversion<br>\n",
    "9. Text normalization — lowercase, remove URLs/punctuation, stopword removal\n",
    "10. Export — convert mentioned_tickers to CSV-friendly string, export `preprocessed_data.csv`\n",
    "11. Database Export — save to Oracle table `preprocessed_data`\n",
    "\n",
    "**Output:** `preprocessed_data.csv` + Oracle table `preprocessed_data`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b04f2632",
   "metadata": {},
   "source": [
    "**Reddit API documentation (column descriptions):** https://praw.readthedocs.io/en/stable/code_overview/models/submission.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "affc3561",
   "metadata": {},
   "source": [
    "**Recommended:** Install Data Wrangler extension by Microsoft for VS Code. It provides data quality metrics and lets you view/refresh DataFrames as you process data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5358f6b5",
   "metadata": {},
   "source": [
    "**Tip:** Use the OUTLINE view in the left sidebar for easy navigation through this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b2e4feb",
   "metadata": {},
   "source": [
    "**Most functions are defined in `utils.py` and imported here.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccb93896",
   "metadata": {},
   "source": [
    "**ENJOY THE RUN**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71bb957f",
   "metadata": {},
   "source": [
    "## 1 Setup — Imports & environment\n",
    "\n",
    "Načteme potřebné knihovny a ověříme dostupnost spaCy/NLTK a DB driveru. To nám řekne, které nástroje můžeme použít dál pro čištění a tokenizaci textu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a1346aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Setup: Imports and Configuration\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import logging\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "from datetime import datetime\n",
    "from typing import List, Set, Dict, Any\n",
    "from tqdm import tqdm\n",
    "from io import StringIO\n",
    "import urllib.request\n",
    "import urllib.parse\n",
    "\n",
    "# Environment and database imports\n",
    "try:\n",
    "    from dotenv import load_dotenv\n",
    "    load_dotenv()\n",
    "    DOTENV_AVAILABLE = True\n",
    "except ImportError:\n",
    "    DOTENV_AVAILABLE = False\n",
    "\n",
    "try:\n",
    "    import oracledb\n",
    "    ORACLE_AVAILABLE = True\n",
    "except ImportError:\n",
    "    ORACLE_AVAILABLE = False\n",
    "    print(\"Oracle DB not available. Install with: pip install oracledb\")\n",
    "\n",
    "# Checking for NLP libraries\n",
    "available_libs = []\n",
    "try:\n",
    "    import nltk\n",
    "    NLTK_AVAILABLE = True\n",
    "    available_libs.append(\"NLTK\")\n",
    "\n",
    "    # Check if NLTK stopwords are downloaded\n",
    "    try:\n",
    "        nltk.data.find('corpora/stopwords')\n",
    "        print(\"NLTK stopwords data found\")\n",
    "    except LookupError:\n",
    "        print(\"NLTK stopwords not found. Attempting to download...\")\n",
    "        try:\n",
    "            nltk.download('stopwords', quiet=True)\n",
    "            print(\"NLTK stopwords downloaded successfully\")\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to download NLTK stopwords: {e}\")\n",
    "            print(\"   Will use fallback stopword list\")\n",
    "except ImportError:\n",
    "    NLTK_AVAILABLE = False\n",
    "\n",
    "try:\n",
    "    import spacy\n",
    "    SPACY_AVAILABLE = True\n",
    "    available_libs.append(\"spaCy\")\n",
    "\n",
    "    # Check if spaCy model is available\n",
    "    try:\n",
    "        spacy.load(\"en_core_web_sm\")\n",
    "        print(\"spaCy 'en_core_web_sm' model found\")\n",
    "    except OSError:\n",
    "        print(\"spaCy model 'en_core_web_sm' not found\")\n",
    "        print(\"   Install with: python -m spacy download en_core_web_sm\")\n",
    "        print(\"   Will fall back to NLTK if available\")\n",
    "except ImportError:\n",
    "    SPACY_AVAILABLE = False\n",
    "\n",
    "# Import functions from utils\n",
    "from utils import (\n",
    "    get_oracle_connection,\n",
    "    get_all_us_tickers,\n",
    "    detect_tickers_in_text,\n",
    "    apply_ticker_detection,\n",
    "    load_ticker_stopwords,\n",
    "    apply_ticker_stopword_filter,\n",
    "    harmonize_schema,\n",
    "    drop_invalid_texts,\n",
    "    deduplicate_and_normalize_types,\n",
    "    add_temporal_features,\n",
    "    add_engagement_features,\n",
    "    apply_text_normalization,\n",
    "    remove_financial_stopwords,\n",
    "    remove_stopwords_spacy\n",
    ")\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s [%(levelname)s] %(message)s\")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Configuration constants\n",
    "MIN_TEXT_LENGTH = 10\n",
    "RETRY_DELAY = 10\n",
    "\n",
    "print(f\"Step 1: imports loaded — available: {', '.join(available_libs) if available_libs else 'none'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb70ac2c",
   "metadata": {},
   "source": [
    "## 2 Database — Load Posts & Comments\n",
    "\n",
    "Connect to Oracle database and load posts and comments from `historical_posts` and `historical_comments` tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c10b68e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Database Connection: Loading Reddit Data from Oracle\n",
    "\n",
    "print(\"Step 2: importing Reddit data from Oracle database\")\n",
    "\n",
    "# Check if Oracle DB is available and credentials are set in environment variables\n",
    "oracle_credentials_available = (\n",
    "    ORACLE_AVAILABLE and\n",
    "    os.getenv('db-username') and\n",
    "    os.getenv('db-password') and\n",
    "    os.getenv('db-dsn')\n",
    ")\n",
    "\n",
    "if oracle_credentials_available:\n",
    "    print(\"Oracle credentials found\")\n",
    "\n",
    "    conn = get_oracle_connection()\n",
    "\n",
    "    if conn:\n",
    "        print(\"Database connection successful\")\n",
    "\n",
    "        # Importing data from database\n",
    "        try:\n",
    "            query_posts = \"\"\"\n",
    "                SELECT\n",
    "                    author, title, created_utc, id, is_original_content,\n",
    "                    score, DBMS_LOB.SUBSTR(body, 4000, 1) as body,\n",
    "                    subreddit, upvote_ratio, url\n",
    "                FROM historical_posts\n",
    "                ORDER BY created_utc DESC\n",
    "                FETCH FIRST 25000 ROWS ONLY\n",
    "            \"\"\"\n",
    "            df_posts = pd.read_sql_query(query_posts, conn)\n",
    "\n",
    "            query_comments = \"\"\"\n",
    "                SELECT\n",
    "                    author, created_utc, id, parent_post_id, score,\n",
    "                    DBMS_LOB.SUBSTR(body, 4000, 1) as body,\n",
    "                    subreddit\n",
    "                FROM historical_comments\n",
    "                ORDER BY created_utc DESC\n",
    "                FETCH FIRST 25000 ROWS ONLY\n",
    "            \"\"\"\n",
    "            df_comments = pd.read_sql_query(query_comments, conn)\n",
    "\n",
    "            # Closing connection immediately after data import so that it does not stay open longer than needed\n",
    "            conn.close()\n",
    "\n",
    "            print(f\"\\nPosts imported: {df_posts.shape}\")\n",
    "            print(f\"Comments imported: {df_comments.shape}\")\n",
    "\n",
    "            if len(df_posts) > 0:\n",
    "                print(f\"Posts columns: {list(df_posts.columns)}\")\n",
    "\n",
    "            if len(df_comments) > 0:\n",
    "                print(f\"Comments columns: {list(df_comments.columns)}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error importing data from database: {e}\")\n",
    "            if conn:\n",
    "                try:\n",
    "                    conn.close()\n",
    "                except:\n",
    "                    pass\n",
    "            df_posts = pd.DataFrame()\n",
    "            df_comments = pd.DataFrame()\n",
    "\n",
    "    else:\n",
    "        print(\"Failed to connect to database - check logs.\")\n",
    "        df_posts = pd.DataFrame()\n",
    "        df_comments = pd.DataFrame()\n",
    "\n",
    "else:\n",
    "    print(\"Oracle database credentials not configured — set them in .env file or environment variables\")\n",
    "    df_posts = pd.DataFrame()\n",
    "    df_comments = pd.DataFrame()\n",
    "\n",
    "print(f\"\\nStep 2 complete: Loaded {len(df_posts)} posts and {len(df_comments)} comments.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ba52502",
   "metadata": {},
   "source": [
    "## 3 Load Tickers — US Stock Ticker List\n",
    "\n",
    "Load and normalize ticker list from NASDAQ and NYSE. Check ticker distribution by exchange."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bd8a27f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Ticker Data Loading: Fetch NASDAQ/NYSE Symbols\n",
    "us_tickers_path = \"inputs/us_tickers.csv\"\n",
    "\n",
    "if os.path.exists(us_tickers_path):\n",
    "    try:\n",
    "        tickers_df = pd.read_csv(us_tickers_path, dtype=str)\n",
    "        tickers_df.columns = [c.strip().lower().replace(\" \", \"_\") for c in tickers_df.columns]\n",
    "        if 'ticker' in tickers_df.columns:\n",
    "            tickers_df['ticker'] = tickers_df['ticker'].astype(str).str.upper().str.strip()\n",
    "        print(f\"Loaded {len(tickers_df)} cached tickers\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading cached data: {e}\")\n",
    "        tickers_df = None\n",
    "else:\n",
    "    tickers_df = None\n",
    "\n",
    "# If no cached data or fresh data\n",
    "if tickers_df is None or len(tickers_df) == 0:\n",
    "    try:\n",
    "        tickers_df = get_all_us_tickers()\n",
    "        if len(tickers_df) > 0:\n",
    "            tickers_df.to_csv(us_tickers_path, index=False)\n",
    "            print(f\"Fetched and cached {len(tickers_df)} US tickers\")\n",
    "        else:\n",
    "            print(\"No ticker data retrieved.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching ticker data: {e}\")\n",
    "        tickers_df = pd.DataFrame()\n",
    "\n",
    "if len(tickers_df) > 0:\n",
    "    print(f\"  Exchanges: {tickers_df['exchange'].value_counts().to_dict()}\")\n",
    "\n",
    "else:\n",
    "    print(\"No ticker data available\")\n",
    "\n",
    "print(f\"\\nStep 3 complete: Loaded {len(tickers_df)} ticker symbols. Letzgoo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a288a0b5",
   "metadata": {},
   "source": [
    "## 4 Data Harmonization — Unify Posts & Comments\n",
    "\n",
    "Merge posts and comments into one `df_unified` with consistent columns (id, text, type, parent_post_id, etc.)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62b336fa",
   "metadata": {},
   "source": [
    "### 4.1 Unify Posts & Comments\n",
    "\n",
    "Merge **df_posts** and **df_comments**, remap column names, and create unified `text` column. Columns that exist only for posts (or only for comments) will have NAs in the other type—this is expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21117e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Data Harmonization: Merge Posts and Comments\n",
    "\n",
    "if len(df_posts) > 0 or len(df_comments) > 0:\n",
    "    # combining data\n",
    "    df_unified = harmonize_schema(df_posts, df_comments)\n",
    "    print(f\"Unified dataframe shape: {df_unified.shape}\")\n",
    "    print(f\"Unified columns: {list(df_unified.columns)}\")\n",
    "\n",
    "    # data type distribution\n",
    "    type_counts = df_unified['type'].value_counts()\n",
    "    print(f\"\\nData distribution:\")\n",
    "    for dtype, count in type_counts.items():\n",
    "        print(f\"  {dtype}: {count} rows\")\n",
    "\n",
    "else:\n",
    "    print(\"No data to harmonize\")\n",
    "    df_unified = pd.DataFrame()\n",
    "\n",
    "print(f\"\\nStep 4.1 complete: Unified {len(df_unified)} rows. Letzgoo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a09e66d",
   "metadata": {},
   "source": [
    "### 4.2 Placeholder-Only Rows Analysis\n",
    "\n",
    "Some rows contain only placeholders ([removed]/[deleted]) with no actual content. These will be removed in the next step. Analyze how many such rows exist and their distribution by type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0439c146",
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(df_unified) == 0:\n",
    "    print(\"No data\")\n",
    "else:\n",
    "    def is_placeholder(s):\n",
    "        if pd.isna(s):\n",
    "            return True\n",
    "        s = str(s).strip().lower()\n",
    "        return s == '' or s in {'[deleted]', '[removed]', 'nan'}\n",
    "\n",
    "    text_placeholder = df_unified['text'].apply(is_placeholder)\n",
    "    title_placeholder = df_unified.get('title', pd.Series(False, index=df_unified.index)).apply(is_placeholder)\n",
    "\n",
    "    to_drop_mask = (\n",
    "        ((df_unified['type'] == 'post') & title_placeholder & text_placeholder) |\n",
    "        ((df_unified['type'] == 'comment') & text_placeholder)\n",
    "    )\n",
    "\n",
    "    if to_drop_mask.any():\n",
    "        print(\"\\nPlaceholder-Only Rows by Type:\")\n",
    "\n",
    "        for t, grp in df_unified[to_drop_mask].groupby('type'):\n",
    "            print(f\"  {t.upper()}: {len(grp):,} rows will be deleted\")\n",
    "\n",
    "            if t == 'post':\n",
    "                for col in ['title', 'text']:\n",
    "                    removed = (grp[col].str.strip().str.lower() == '[removed]').sum()\n",
    "                    deleted = (grp[col].str.strip().str.lower() == '[deleted]').sum()\n",
    "                    print(f\"    {col}: [removed]={removed:,}, [deleted]={deleted:,}\")\n",
    "            else:\n",
    "                txt = grp['text'].astype(str).str.strip().str.lower()\n",
    "                removed = (txt == '[removed]').sum()\n",
    "                deleted = (txt == '[deleted]').sum()\n",
    "                empty = (txt == '').sum()\n",
    "                print(f\"    text: [removed]={removed:,}, [deleted]={deleted:,}, empty/NA={empty:,}\")\n",
    "    else:\n",
    "        print(\"No placeholder-only rows found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e0b9e32",
   "metadata": {},
   "source": [
    "**Analysis shows placeholder-only rows are mostly comments (usually [removed]). These represent ~1% of data and will be deleted in the next step.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "534dc37f",
   "metadata": {},
   "source": [
    "### 4.3 Remove [removed]/[deleted] Placeholders\n",
    "\n",
    "Replace [removed]/[deleted] placeholders with empty strings to avoid issues during feature engineering (e.g., \"Title text. [removed]\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92ebd542",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define placeholder checker\n",
    "def is_placeholder(s):\n",
    "    if pd.isna(s):\n",
    "        return True\n",
    "    s = str(s).strip().lower()\n",
    "    return s == '' or s in {'[deleted]', '[removed]', 'nan'}\n",
    "\n",
    "# turning placeholders into empty string\n",
    "if 'title' in df_unified.columns:\n",
    "    df_unified['title'] = df_unified['title'].apply(lambda x: '' if is_placeholder(x) else str(x).strip())\n",
    "if 'text' in df_unified.columns:\n",
    "    df_unified['text'] = df_unified['text'].apply(lambda x: '' if is_placeholder(x) else str(x).strip())\n",
    "\n",
    "# Dropping posts where both title and text are empty after cleaning\n",
    "before = len(df_unified)\n",
    "\n",
    "# Create masks for empty posts and comments\n",
    "if 'title' in df_unified.columns:\n",
    "    mask_empty_post = (df_unified['type'] == 'post') & (df_unified['title'].astype(str).str.strip() == '') & (df_unified['text'].astype(str).str.strip() == '')\n",
    "else:\n",
    "    mask_empty_post = (df_unified['type'] == 'post') & (df_unified['text'].astype(str).str.strip() == '')\n",
    "\n",
    "mask_empty_comment = (df_unified['type'] == 'comment') & (df_unified['text'].astype(str).str.strip() == '')\n",
    "\n",
    "# For comments we will drop those with empty body. For posts we will drop only if both title+text empty\n",
    "to_drop = mask_empty_post | mask_empty_comment\n",
    "df_unified = df_unified[~to_drop].reset_index(drop=True)\n",
    "after = len(df_unified)\n",
    "print(f\"We removed {before - after} placeholder-only rows (kept posts with title if present).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81e6dbfb",
   "metadata": {},
   "source": [
    "### 4.4 Merge Title + Body for Posts into `text` Column\n",
    "\n",
    "For posts, merge `title` with body text. Titles often contain key information or ticker mentions. After merging, drop the `title` column (ticker detection will use the merged `text` column)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5b013ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.4 Enhanced Text: Merge Title + Body for Posts\n",
    "\n",
    "if len(df_unified) > 0:\n",
    "\n",
    "    # For posts: concatenate title + body\n",
    "    # For comments: keep body only (no title)\n",
    "    def create_enhanced_text(row):\n",
    "        if row['type'] == 'post':\n",
    "            title = str(row.get('title', '')).strip()\n",
    "            body = str(row.get('text', '')).strip()\n",
    "            # Combine title and body with separator\n",
    "            if title and title != 'nan' and body and body != 'nan':\n",
    "                return f\"{title}. {body}\"\n",
    "            elif title and title != 'nan':\n",
    "                return title\n",
    "            elif body and body != 'nan':\n",
    "                return body\n",
    "            else:\n",
    "                return ''\n",
    "        else:\n",
    "            # Comments: use text only\n",
    "            return str(row.get('text', '')).strip()\n",
    "\n",
    "    df_unified['enhanced_text'] = df_unified.apply(create_enhanced_text, axis=1)\n",
    "\n",
    "    # Replace 'text' column with enhanced version\n",
    "    df_unified['text'] = df_unified['enhanced_text']\n",
    "    df_unified.drop(columns=['enhanced_text'], inplace=True)\n",
    "\n",
    "    # We can safely drop title column since it's now merged into text (and later ticker detection is done on the \"text\" column)\n",
    "    if 'title' in df_unified.columns:\n",
    "        df_unified.drop(columns=['title'], inplace=True)\n",
    "        print(\"Dropped 'title' column (merged into 'text')\")\n",
    "\n",
    "    print(f\"Done\")\n",
    "\n",
    "else:\n",
    "    print(\"No data to enhance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8739232b",
   "metadata": {},
   "source": [
    "## 5 Exploratory Data Analysis (EDA)\n",
    "\n",
    "Projdeme kvalitu dat a základní statistiky, abychom věděli jak data dál (pokud je to potřeba) upravit."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5af08cff",
   "metadata": {},
   "source": [
    "### 5.1 Data Types Check & Schema Overview\n",
    "\n",
    "Check and convert data types for all columns. This ensures correct data processing in subsequent steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bfcbf1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.1 Data types check\n",
    "\n",
    "if len(df_unified) > 0:\n",
    "    print(f\"Dataset shape: {df_unified.shape}\")\n",
    "\n",
    "    for col in df_unified.columns:\n",
    "        dtype = df_unified[col].dtype\n",
    "        print(f\"  {col:20} : {str(dtype):15}\")\n",
    "\n",
    "else:\n",
    "    print(\"No data available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e1d2842",
   "metadata": {},
   "source": [
    "**Convert to correct data types:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bd5f837",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DateTime conversion\n",
    "if 'created_utc' in df_unified.columns:\n",
    "    df_unified['created_utc'] = pd.to_datetime(df_unified['created_utc'], unit='s', errors='coerce')\n",
    "\n",
    "# Numeric conversions\n",
    "numeric_cols = {'score': 'float64', 'upvote_ratio': 'float64'}\n",
    "for col, dtype in numeric_cols.items():\n",
    "    df_unified[col] = pd.to_numeric(df_unified[col], errors='coerce').astype(dtype)\n",
    "\n",
    "\n",
    "# Categorical conversions\n",
    "for col in ['type', 'subreddit']:\n",
    "    df_unified[col] = df_unified[col].astype('category')\n",
    "\n",
    "# String conversions\n",
    "for col in ['id', 'text', 'author', 'url']:\n",
    "    df_unified[col] = df_unified[col].astype(pd.StringDtype())\n",
    "\n",
    "# Handle parent_post_id (preserve NaN)\n",
    "if 'parent_post_id' in df_unified.columns:\n",
    "    df_unified['parent_post_id'] = df_unified['parent_post_id'].astype(pd.StringDtype())\n",
    "\n",
    "# Drop constant column\n",
    "if 'is_original_content' in df_unified.columns:\n",
    "    df_unified.drop(columns=['is_original_content'], inplace=True)\n",
    "\n",
    "print(f\"Data types after conversion:\")\n",
    "for col in df_unified.columns:\n",
    "    dtype = df_unified[col].dtype\n",
    "    non_null = df_unified[col].notna().sum()\n",
    "    print(f\"{col} : {str(dtype)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d09e02bd",
   "metadata": {},
   "source": [
    "### 5.2 Column value distributions\n",
    "\n",
    "Prozkoumáme rozložení hodnot ve sloupcích - počty unikátních hodnot, konstantní sloupce a základní distribuce kategorických proměnných."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b09d964",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.2 Column Value Distributions\n",
    "\n",
    "if len(df_unified) > 0:\n",
    "    print(\"\\nUnique Value Counts:\")\n",
    "\n",
    "    for col in df_unified.columns:\n",
    "        n_unique = df_unified[col].nunique()\n",
    "        n_total = len(df_unified)\n",
    "        pct_unique = (n_unique / n_total) * 100\n",
    "\n",
    "        print(f\"  {col:20} : {n_unique:,} unique values ({pct_unique:.1f}% of total)\")\n",
    "\n",
    "    print(f\"\\nStep 5.2 completed\")\n",
    "\n",
    "else:\n",
    "    print(\"No data available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "664b963e",
   "metadata": {},
   "source": [
    "**[is_original_content] has only value 0 (constant), so it will be dropped. No other data quality issues detected.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dcaee71",
   "metadata": {},
   "source": [
    "### 5.3 Overview — counts & missingness \n",
    "\n",
    "Spočítáme počty, podíly chybějících hodnot a zjistíme, kolik záznamů je smazaných nebo prázdných (NAs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5316784c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.3 Data Quality & Completeness\n",
    "if len(df_unified) > 0:\n",
    "\n",
    "    print(\"\\nNAs by Column:\")\n",
    "    for col in df_unified.columns:\n",
    "        missing_count = df_unified[col].isna().sum()\n",
    "        if missing_count > 0:\n",
    "            missing_pct = (missing_count / len(df_unified)) * 100\n",
    "            print(f\"  {col}: {missing_count:,} ({missing_pct:.1f}%)\")\n",
    "\n",
    "else:\n",
    "    print(\"No data available for quality analysis.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "249908ea",
   "metadata": {},
   "source": [
    "Missing values in [is_original_content], [upvote_ratio], [url], [parent_post_id] are structural (posts vs. comments have different columns). In data cleaning, we'll drop is_original_content and url. **[parent_post_id] must be kept for ticker inheritance.**\n",
    "\n",
    "Very short texts (≤10 chars) will be removed in Step 6."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "476c830b",
   "metadata": {},
   "source": [
    "### 5.4 Text Characteristics\n",
    "\n",
    "Analyze text lengths, word counts, and percentiles to identify very short texts for removal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d022867d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.4 Text Characteristics\n",
    "\n",
    "if len(df_unified) > 0 and 'text' in df_unified.columns:\n",
    "\n",
    "    # Calculating text statistics\n",
    "    valid_texts = df_unified[df_unified['text'].notna() & (df_unified['text'] != '')]\n",
    "\n",
    "    if len(valid_texts) > 0:\n",
    "        text_lengths = valid_texts['text'].astype(str).str.len()\n",
    "\n",
    "        word_counts = valid_texts['text'].astype(str).str.split().str.len()\n",
    "\n",
    "        print(f\"\\nWord Count Statistics (Overall):\")\n",
    "        print(f\"  Mean: {word_counts.mean():.1f} words\")\n",
    "        print(f\"  Median: {word_counts.median():.1f} words\")\n",
    "        print(f\"  Min: {word_counts.min()} words\")\n",
    "        print(f\"  Max: {word_counts.max()} words\")\n",
    "\n",
    "        if 'type' in valid_texts.columns:\n",
    "            posts = valid_texts[valid_texts['type'] == 'post']\n",
    "            comments = valid_texts[valid_texts['type'] == 'comment']\n",
    "\n",
    "            if len(posts) > 0 and len(comments) > 0:\n",
    "                post_word_counts = posts['text'].astype(str).str.split().str.len()\n",
    "                comment_word_counts = comments['text'].astype(str).str.split().str.len()\n",
    "\n",
    "                fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 5))\n",
    "\n",
    "                # Posts\n",
    "                ax1.hist(post_word_counts, bins=50, edgecolor='black', alpha=0.7, color='steelblue')\n",
    "                ax1.axvline(post_word_counts.mean(), color='red', linestyle='--', linewidth=2, label=f'Mean: {post_word_counts.mean():.1f}')\n",
    "                ax1.axvline(post_word_counts.median(), color='green', linestyle='--', linewidth=2, label=f'Median: {post_word_counts.median():.1f}')\n",
    "                ax1.set_xlabel('Number of Words')\n",
    "                ax1.set_ylabel('Frequency')\n",
    "                ax1.set_title(f'Posts - Word Count Distribution (n={len(posts):,})')\n",
    "                ax1.legend()\n",
    "                ax1.grid(axis='y', alpha=0.3)\n",
    "\n",
    "                # Comments\n",
    "                ax2.hist(comment_word_counts, bins=50, edgecolor='black', alpha=0.7, color='coral')\n",
    "                ax2.axvline(comment_word_counts.mean(), color='red', linestyle='--', linewidth=2, label=f'Mean: {comment_word_counts.mean():.1f}')\n",
    "                ax2.axvline(comment_word_counts.median(), color='green', linestyle='--', linewidth=2, label=f'Median: {comment_word_counts.median():.1f}')\n",
    "                ax2.set_xlabel('Number of Words')\n",
    "                ax2.set_ylabel('Frequency')\n",
    "                ax2.set_title(f'Comments - Word Count Distribution (n={len(comments):,})')\n",
    "                ax2.legend()\n",
    "                ax2.grid(axis='y', alpha=0.3)\n",
    "\n",
    "                plt.tight_layout()\n",
    "                plt.show()\n",
    "            else:\n",
    "                print(\"  Not enough data for separate histograms\")\n",
    "\n",
    "        # Short texts identification\n",
    "        very_short = (text_lengths <= MIN_TEXT_LENGTH).sum()\n",
    "        print(f\"\\nQuality Issues:\")\n",
    "        print(f\"  Texts ≤{MIN_TEXT_LENGTH} chars: {very_short:,} ({very_short/len(valid_texts)*100:.1f}%)\")\n",
    "\n",
    "else:\n",
    "    print(\"No text data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d9e4445",
   "metadata": {},
   "source": [
    "**Distribution looks reasonable. Posts are longer than comments on average. For sentiment analysis, there's no concept of \"outlier\" in word count—all lengths are valid.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9960d814",
   "metadata": {},
   "source": [
    "### 5.5 Temporal Analysis\n",
    "\n",
    "Analyze temporal distribution of posts/comments (daily/hourly patterns and date range)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddec0014",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.5 Temporal Analysis\n",
    "\n",
    "if len(df_unified) > 0 and 'created_utc' in df_unified.columns:\n",
    "\n",
    "    # Converting to datetime\n",
    "    valid_dates = df_unified[df_unified['created_utc'].notna()].copy()\n",
    "\n",
    "    if len(valid_dates) > 0:\n",
    "        valid_dates['datetime'] = pd.to_datetime(valid_dates['created_utc'], unit='s')\n",
    "\n",
    "        # Date range\n",
    "        print(f\"\\nDate Range:\")\n",
    "        print(f\"  Earliest: {valid_dates['datetime'].min()}\")\n",
    "        print(f\"  Latest: {valid_dates['datetime'].max()}\")\n",
    "        print(f\"  Span: {(valid_dates['datetime'].max() - valid_dates['datetime'].min()).days} days\")\n",
    "\n",
    "        valid_dates['date'] = valid_dates['datetime'].dt.date\n",
    "        valid_dates['hour'] = valid_dates['datetime'].dt.hour\n",
    "        valid_dates['day_of_week'] = valid_dates['datetime'].dt.day_name()\n",
    "        valid_dates['month'] = valid_dates['datetime'].dt.month\n",
    "\n",
    "        # Posts per day\n",
    "        daily_counts = valid_dates.groupby('date').size()\n",
    "        print(f\"  Mean posts per day: {daily_counts.mean():.1f}\")\n",
    "\n",
    "        # temporal patterns\n",
    "        print(f\"\\nTemporal Distribution Charts:\")\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 5))\n",
    "\n",
    "        day_order = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
    "        day_counts = valid_dates['day_of_week'].value_counts()\n",
    "        day_counts_ordered = [int(day_counts.get(d, 0)) for d in day_order]\n",
    "        ax1.bar(range(7), day_counts_ordered, color='steelblue', alpha=0.7, edgecolor='black')\n",
    "        ax1.set_xlabel('Day of Week')\n",
    "        ax1.set_ylabel('Number of Posts')\n",
    "        ax1.set_title('Activity by Day of Week')\n",
    "        ax1.set_xticks(range(7))\n",
    "        ax1.set_xticklabels(['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun'])\n",
    "        ax1.grid(axis='y', alpha=0.3)\n",
    "\n",
    "        hour_counts = valid_dates['hour'].value_counts()\n",
    "        hours = list(range(24))\n",
    "        counts = [hour_counts.get(h, 0) for h in hours]\n",
    "        ax2.bar(hours, counts, color='coral', alpha=0.7, edgecolor='black')\n",
    "        ax2.set_xlabel('Hour of Day')\n",
    "        ax2.set_ylabel('Number of Posts')\n",
    "        ax2.set_title('Hourly Activity Distribution')\n",
    "        ax2.set_xticks(range(0, 24, 2))\n",
    "        ax2.grid(axis='y', alpha=0.3)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    print()\n",
    "else:\n",
    "    print(\"Warning: No temporal data available for analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e2e94f5",
   "metadata": {},
   "source": [
    "**Peak activity: Tuesday/Wednesday, afternoon/evening hours.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee6e2f8b",
   "metadata": {},
   "source": [
    "### 5.6 Upvote ratio analysis\n",
    "\n",
    "Analyzujeme upvote_ratio - poměr kladných hodnocení (upvotes / (upvotes + downvotes)). Tato metrika je dostupná pouze pro posty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "333fbfc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.6 Upvote Ratio Analysis\n",
    "\n",
    "if len(df_unified) > 0:\n",
    "    if 'upvote_ratio' in df_unified.columns:\n",
    "        posts = df_unified[df_unified['type'] == 'post']\n",
    "\n",
    "        if len(posts) > 0 and 'upvote_ratio' in posts.columns:\n",
    "            upvote_ratios = posts['upvote_ratio'].dropna()\n",
    "\n",
    "            if len(upvote_ratios) > 0:\n",
    "                print(f\"  Mean: {upvote_ratios.mean():.3f}\")\n",
    "                print(f\"  Median: {upvote_ratios.median():.3f}\")\n",
    "                print(f\"  Min: {upvote_ratios.min():.3f}\")\n",
    "                print(f\"  Max: {upvote_ratios.max():.3f}\")\n",
    "\n",
    "            print(f\"\\nUpvote Ratio Histogram:\")\n",
    "            fig, ax = plt.subplots(1, 1, figsize=(10, 6))\n",
    "\n",
    "            # Histogram\n",
    "            ax.hist(upvote_ratios, bins=30, edgecolor='black', alpha=0.7, color='green')\n",
    "            ax.axvline(upvote_ratios.mean(), color='red', linestyle='--', linewidth=2,\n",
    "                        label=f'Mean: {upvote_ratios.mean():.3f}')\n",
    "            ax.axvline(upvote_ratios.median(), color='blue', linestyle='--', linewidth=2,\n",
    "                        label=f'Median: {upvote_ratios.median():.3f}')\n",
    "            ax.set_xlabel('Upvote Ratio')\n",
    "            ax.set_ylabel('Frequency')\n",
    "            ax.set_title(f'Distribution of Upvote Ratios (n={len(upvote_ratios):,} posts)')\n",
    "            ax.legend()\n",
    "            ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "\n",
    "else:\n",
    "    print(\"Warning: No data available for upvote ratio analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8669df45",
   "metadata": {},
   "source": [
    "**Upvote ratio ranges 0-1, generally positive (mean ~0.9).**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35952e01",
   "metadata": {},
   "source": [
    "### 5.7 Data Integrity & Duplicates\n",
    "\n",
    "Check for duplicate IDs and other integrity issues (missing IDs, very short texts)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f0eaad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.7 Data Integrity & Duplicates\n",
    "\n",
    "if len(df_unified) > 0:\n",
    "\n",
    "    # Duplicate IDs\n",
    "    if 'id' in df_unified.columns:\n",
    "        total_ids = len(df_unified)\n",
    "        unique_ids = df_unified['id'].nunique()\n",
    "        duplicate_ids = total_ids - unique_ids\n",
    "\n",
    "        print(f\"  Total rows: {total_ids:,}\")\n",
    "        print(f\"  Unique IDs: {unique_ids:,}\")\n",
    "        print(f\"  Duplicate IDs: {duplicate_ids:,} ({duplicate_ids/total_ids*100:.2f}%)\")\n",
    "\n",
    "        if duplicate_ids > 0:\n",
    "            dup_counts = df_unified['id'].value_counts()\n",
    "            most_duplicated = dup_counts[dup_counts > 1].head(5)\n",
    "            print(f\"\\n  Most duplicated IDs:\")\n",
    "            for id_val, count in most_duplicated.items():\n",
    "                print(f\"    {id_val}: appears {count} times\")\n",
    "\n",
    "    # potential issues\n",
    "    print(f\"\\nPotential Data Issues:\")\n",
    "    issues_found = 0\n",
    "\n",
    "    critical_fields = ['id', 'text', 'created_utc']\n",
    "    for field in critical_fields:\n",
    "        if field in df_unified.columns:\n",
    "            na_count = df_unified[field].isna().sum()\n",
    "            if na_count > 0:\n",
    "                print(f\"  Warning {field}: {na_count:,} missing values\")\n",
    "                issues_found += 1\n",
    "\n",
    "    if 'text' in df_unified.columns:\n",
    "        deleted = df_unified['text'].isin(['[deleted]', '[removed]']).sum()\n",
    "        if deleted > 0:\n",
    "            print(f\"  Warning: Deleted/removed content: {deleted:,} rows\")\n",
    "            issues_found += 1\n",
    "\n",
    "    if 'text' in df_unified.columns:\n",
    "        very_short = (df_unified['text'].astype(str).str.len() <= 5).sum()\n",
    "        if very_short > 0:\n",
    "            print(f\"  Warning: Extremely short texts (≤5 chars): {very_short:,} rows\")\n",
    "            issues_found += 1\n",
    "\n",
    "    if issues_found == 0:\n",
    "        print(\"No major data quality issues detected\")\n",
    "\n",
    "else:\n",
    "    print(\"No data available for integrity analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91cda4a6",
   "metadata": {},
   "source": [
    "## 6 Data Cleaning\n",
    "\n",
    "Remove deleted/empty texts, handle NAs, and filter out very short records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "979d1037",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Data Cleaning: Remove Invalid and Duplicate Content\n",
    "\n",
    "print(f\"Starting with {len(df_unified)} rows\")\n",
    "\n",
    "# Step 6a: Remove invalid texts\n",
    "df_cleaned = drop_invalid_texts(df_unified, min_len=MIN_TEXT_LENGTH)\n",
    "print(f\"    After removing invalid texts: {len(df_cleaned)} rows (-{len(df_unified) - len(df_cleaned)})\")\n",
    "\n",
    "# Step 6b: Deduplicate and normalize types\n",
    "df_cleaned = deduplicate_and_normalize_types(df_cleaned)\n",
    "print(f\"    After deduplication: {len(df_cleaned)} rows\")\n",
    "\n",
    "# Step 6c: Dropping unnecessary columns (url, is_original_content, author)\n",
    "columns_to_drop = ['url', 'is_original_content', 'author']\n",
    "existing_cols_to_drop = [col for col in columns_to_drop if col in df_cleaned.columns]\n",
    "if existing_cols_to_drop:\n",
    "    df_cleaned.drop(columns=existing_cols_to_drop, inplace=True)\n",
    "    print(f\"    Dropped columns: {existing_cols_to_drop}\")\n",
    "\n",
    "# Show cleaning results\n",
    "print(f\"  Original rows: {len(df_unified)}\")\n",
    "print(f\"  Cleaned rows: {len(df_cleaned)}\")\n",
    "print(f\"  Removed: {len(df_unified) - len(df_cleaned)} ({((len(df_unified) - len(df_cleaned))/len(df_unified)*100):.1f}%)\")\n",
    "\n",
    "# Show data types after cleaning\n",
    "print(f\"\\nData types after cleaning:\")\n",
    "type_counts = df_cleaned['type'].value_counts()\n",
    "for dtype, count in type_counts.items():\n",
    "    print(f\"  {dtype}: {count} rows\")\n",
    "\n",
    "print(f\"\\nStep 6 complete: {len(df_cleaned)} clean rows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9bc0274",
   "metadata": {},
   "source": [
    "## 7 Feature Engineering\n",
    "\n",
    "Add useful features: text_length, word_count, temporal features, engagement features. Create normalized_score (per subreddit, 0-1 scale). Drop original score column and upvote_ratio (only available for posts)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d42e97b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Feature Engineering: Add Temporal and Engagement Features\n",
    "\n",
    "df_features = add_temporal_features(df_cleaned)\n",
    "df_features = add_engagement_features(df_features)\n",
    "\n",
    "# Create normalized_score per subreddit (0-1 scale)\n",
    "if 'score' in df_features.columns and 'subreddit' in df_features.columns:\n",
    "    df_features['normalized_score'] = df_features.groupby('subreddit')['score'].transform(\n",
    "        lambda x: (x - x.min()) / (x.max() - x.min() + 1e-6)  # +1e-6 -> to avoid division by zero)\n",
    "    )\n",
    "\n",
    "    print(f\"\\nPer-Subreddit Normalization Check:\")\n",
    "    for sub in df_features['subreddit'].unique():\n",
    "        sub_data = df_features[df_features['subreddit'] == sub]\n",
    "        print(f\"  {sub}:\")\n",
    "        print(f\"    Raw scores: min={sub_data['score'].min():.0f}, \"\n",
    "                f\"max={sub_data['score'].max():.0f}, \"\n",
    "                f\"mean={sub_data['score'].mean():.1f}\")\n",
    "        print(f\"    Normalized: min={sub_data['normalized_score'].min():.3f}, \"\n",
    "                f\"max={sub_data['normalized_score'].max():.3f}, \"\n",
    "                f\"mean={sub_data['normalized_score'].mean():.3f}\")\n",
    "\n",
    "    # Drop original score column after normalization\n",
    "    df_features.drop(columns=['score'], inplace=True)\n",
    "\n",
    "# Drop upvote_ratio (only for posts, creates NAs for comments)\n",
    "if 'upvote_ratio' in df_features.columns:\n",
    "    df_features.drop(columns=['upvote_ratio'], inplace=True)\n",
    "\n",
    "new_features = ['date', 'hour', 'day_of_week', 'month', 'is_weekend',\n",
    "                'text_length', 'word_count', 'normalized_score']\n",
    "print(f\"\\nNew features added: {[f for f in new_features if f in df_features.columns]}\")\n",
    "\n",
    "print(f\"\\nStep 7 complete: {len(df_features)} rows with enhanced features\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc65145b",
   "metadata": {},
   "source": [
    "## 8 Ticker Detection\n",
    "\n",
    "Detect stock tickers in text using loaded ticker list. Store detected tickers in `mentioned_tickers` and count in `n_tickers`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e06b43a",
   "metadata": {},
   "source": [
    "### 8.1 Detect Stock Ticker Mentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18f9518c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8.1 Ticker Detection: Identify Stock Ticker Mentions\n",
    "\n",
    "print(f\"Detecting tickers in {len(df_features)} texts using {len(tickers_df)} symbols\")\n",
    "\n",
    "df_with_tickers = apply_ticker_detection(df_features, tickers_df)\n",
    "\n",
    "# results\n",
    "ticker_stats = df_with_tickers['n_tickers'].value_counts().sort_index()\n",
    "total_with_tickers = (df_with_tickers['n_tickers'] > 0).sum()\n",
    "print(f\"  Total rows: {len(df_with_tickers)}\")\n",
    "print(f\"  Rows with tickers: {total_with_tickers} ({total_with_tickers/len(df_with_tickers)*100:.1f}%)\")\n",
    "print(f\"  Rows without tickers: {len(df_with_tickers) - total_with_tickers}\")\n",
    "\n",
    "print(f\"\\nTicker count distribution:\")\n",
    "for count, rows in ticker_stats.head(10).items():\n",
    "    print(f\"  {count} tickers: {rows} rows\")\n",
    "\n",
    "print(f\"\\nStep 8 complete: Processed {len(df_with_tickers)} rows for ticker detection\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba6e53a7",
   "metadata": {},
   "source": [
    "### 8.2 Filter False Positive Tickers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e86bc17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8.2 Filter False Positive Tickers\n",
    "\n",
    "ticker_stopwords = load_ticker_stopwords()\n",
    "before_count = df_with_tickers['n_tickers'].sum()\n",
    "df_with_tickers = apply_ticker_stopword_filter(df_with_tickers, ticker_stopwords)\n",
    "after_count = df_with_tickers['n_tickers'].sum()\n",
    "removed = before_count - after_count\n",
    "\n",
    "print(f\"Filtered out {removed} false positive tickers ({removed/before_count*100:.1f}%)\")\n",
    "print(f\"Remaining: {after_count} ticker mentions\")\n",
    "print(f\"\\nStep 8.2 complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abaccec9",
   "metadata": {},
   "source": [
    "### 8.3 Ticker Inheritance — Comments Inherit Parent Post Tickers\n",
    "\n",
    "Comments inherit tickers from their parent posts (union). This improves ticker coverage for comments that discuss a post without explicitly mentioning the ticker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8588d8da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8.3 Ticker Inheritance from Parent Posts\n",
    "# Save \"before inheritance\" counts for comparison\n",
    "comments_before = df_with_tickers[df_with_tickers['type'] == 'comment']\n",
    "comments_with_tickers_before = (comments_before['n_tickers'] > 0).sum()\n",
    "\n",
    "# Build map of post_id -> tickers\n",
    "posts = df_with_tickers[df_with_tickers['type'] == 'post'].copy()\n",
    "post_ticker_map = {}\n",
    "\n",
    "for _, row in posts.iterrows():\n",
    "    post_id = row['id']\n",
    "    tickers_data = row.get('mentioned_tickers', [])\n",
    "    if isinstance(tickers_data, list) and len(tickers_data) > 0:\n",
    "        post_ticker_map[post_id] = set(tickers_data)\n",
    "\n",
    "# Inheritance function: merge comment's own tickers with parent's tickers\n",
    "def inherit_parent_tickers(row):\n",
    "    if row['type'] == 'comment':\n",
    "        parent_id = row.get('parent_post_id')\n",
    "        if pd.notna(parent_id) and parent_id in post_ticker_map:\n",
    "            own_tickers = set(row.get('mentioned_tickers', []))\n",
    "            parent_tickers = post_ticker_map[parent_id]\n",
    "            merged_tickers = own_tickers.union(parent_tickers)\n",
    "            return sorted(merged_tickers)\n",
    "\n",
    "    tickers_data = row.get('mentioned_tickers', [])\n",
    "    return tickers_data if isinstance(tickers_data, list) else []\n",
    "\n",
    "# Apply inheritance\n",
    "df_with_tickers['mentioned_tickers'] = df_with_tickers.apply(inherit_parent_tickers, axis=1)\n",
    "df_with_tickers['n_tickers'] = df_with_tickers['mentioned_tickers'].apply(len)\n",
    "\n",
    "print(\"Step 8.3 completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8588d8da",
   "metadata": {},
   "source": [
    "### 8.4 Ticker Inheritance Impact Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a3481ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8.4 Ticker Inheritance Impact Analysis\n",
    "\n",
    "posts = df_with_tickers[df_with_tickers['type'] == 'post']\n",
    "comments = df_with_tickers[df_with_tickers['type'] == 'comment']\n",
    "\n",
    "posts_with_tickers = (posts['n_tickers'] > 0).sum()\n",
    "comments_with_tickers_after = (comments['n_tickers'] > 0).sum()\n",
    "\n",
    "print(f\"\\nPOSTS:\")\n",
    "print(f\"  With tickers: {posts_with_tickers} / {len(posts)} ({posts_with_tickers/len(posts)*100:.2f}%)\")\n",
    "\n",
    "print(f\"\\nCOMMENTS:\")\n",
    "print(f\"  BEFORE inheritance: {comments_with_tickers_before} / {len(comments)} ({comments_with_tickers_before/len(comments)*100:.2f}%)\")\n",
    "print(f\"  AFTER inheritance:  {comments_with_tickers_after} / {len(comments)} ({comments_with_tickers_after/len(comments)*100:.2f}%)\")\n",
    "\n",
    "print(f\"\\nOVERALL:\")\n",
    "total_with_tickers = (df_with_tickers['n_tickers'] > 0).sum()\n",
    "print(f\"  Total rows with tickers: {total_with_tickers} / {len(df_with_tickers)} ({total_with_tickers/len(df_with_tickers)*100:.2f}%)\")\n",
    "\n",
    "# Drop parent_post_id after inheritance is complete\n",
    "if 'parent_post_id' in df_with_tickers.columns:\n",
    "    df_with_tickers.drop(columns=['parent_post_id'], inplace=True)\n",
    "    print(f\"\\nDropped 'parent_post_id' column (no longer needed)\")\n",
    "\n",
    "print(\"\\nStep 8.4 completed\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4d1feef",
   "metadata": {},
   "source": [
    "### 8.5 Data Type Conversion\n",
    "\n",
    "Ensure all columns have correct data types after feature engineering and ticker detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60357b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8.5 Comprehensive Data Type Conversion\n",
    "# Datetime conversions\n",
    "df_with_tickers['created_utc'] = pd.to_datetime(df_with_tickers['created_utc'])\n",
    "df_with_tickers['date'] = pd.to_datetime(df_with_tickers['date'])\n",
    "\n",
    "# Integer conversions\n",
    "df_with_tickers['hour'] = df_with_tickers['hour'].astype('int8')\n",
    "df_with_tickers['month'] = df_with_tickers['month'].astype('int8')\n",
    "df_with_tickers['text_length'] = df_with_tickers['text_length'].astype('int32')\n",
    "df_with_tickers['word_count'] = df_with_tickers['word_count'].astype('int32')\n",
    "df_with_tickers['n_tickers'] = df_with_tickers['n_tickers'].astype('int16')\n",
    "\n",
    "# Boolean conversions\n",
    "df_with_tickers['is_weekend'] = df_with_tickers['is_weekend'].astype('bool')\n",
    "\n",
    "# Category conversions\n",
    "df_with_tickers['day_of_week'] = df_with_tickers['day_of_week'].astype('category')\n",
    "df_with_tickers['type'] = df_with_tickers['type'].astype('category')\n",
    "df_with_tickers['subreddit'] = df_with_tickers['subreddit'].astype('category')\n",
    "\n",
    "# Float conversions\n",
    "df_with_tickers['normalized_score'] = df_with_tickers['normalized_score'].astype('float32')\n",
    "\n",
    "print(f\"\\nData types after conversion:\")\n",
    "print(f\"  created_utc: {df_with_tickers['created_utc'].dtype}\")\n",
    "print(f\"  date: {df_with_tickers['date'].dtype}\")\n",
    "print(f\"  type: {df_with_tickers['type'].dtype}\")\n",
    "print(f\"  subreddit: {df_with_tickers['subreddit'].dtype}\")\n",
    "print(f\"  hour: {df_with_tickers['hour'].dtype}\")\n",
    "print(f\"  day_of_week: {df_with_tickers['day_of_week'].dtype}\")\n",
    "print(f\"  is_weekend: {df_with_tickers['is_weekend'].dtype}\")\n",
    "print(f\"  text_length: {df_with_tickers['text_length'].dtype}\")\n",
    "print(f\"  word_count: {df_with_tickers['word_count'].dtype}\")\n",
    "print(f\"  n_tickers: {df_with_tickers['n_tickers'].dtype}\")\n",
    "print(f\"  normalized_score: {df_with_tickers['normalized_score'].dtype}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0a40c9c",
   "metadata": {},
   "source": [
    "## 9 Text Normalization for FinBERT\n",
    "\n",
    "Light normalization for transformer models. FinBERT and similar models work best with natural text, so we only do minimal cleaning: remove URLs and extra whitespace. We keep punctuation, capitalization, and all words because these contain important sentiment signals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ccc4a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9. Light Text Normalization for FinBERT\n",
    "\n",
    "def light_normalize_for_finbert(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Minimal text cleaning for transformer models like FinBERT.\n",
    "    Only removes URLs and normalizes whitespace.\n",
    "    Keeps: punctuation, capitalization, stopwords, numbers - all contain sentiment signals!\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "\n",
    "    # Remove URLs (plain and markdown links)\n",
    "    text = re.sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '', text)\n",
    "    text = re.sub(r'\\[([^\\]]+)\\]\\([^\\)]+\\)', r'\\1', text)  # [text](url) -> text\n",
    "\n",
    "    # Normalize whitespace (collapse multiple spaces, remove leading/trailing)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "    return text\n",
    "\n",
    "# Apply light normalization\n",
    "print(\"Applying light normalization for FinBERT...\")\n",
    "df_final = df_with_tickers.copy()\n",
    "df_final['sentiment_ready_text'] = df_final['text'].apply(light_normalize_for_finbert)\n",
    "\n",
    "print(f\"Final dataframe shape: {df_final.shape}\")\n",
    "print(f\"Final columns: {list(df_final.columns)}\")\n",
    "\n",
    "# Show normalization impact\n",
    "print(f\"\\nNormalization Statistics:\")\n",
    "print(f\"  Total rows: {len(df_final)}\")\n",
    "print(f\"  Rows with tickers: {(df_final['n_tickers'] > 0).sum()}\")\n",
    "\n",
    "avg_original_length = df_final['text'].str.len().mean()\n",
    "avg_normalized_length = df_final['sentiment_ready_text'].str.len().mean()\n",
    "print(f\"\\n  Average original text length: {avg_original_length:.1f} chars\")\n",
    "print(f\"  Average normalized text length: {avg_normalized_length:.1f} chars\")\n",
    "print(f\"  Text preserved: {(avg_normalized_length / avg_original_length * 100):.1f}%\")\n",
    "\n",
    "# Show examples\n",
    "print(f\"\\n--- Example Transformations ---\")\n",
    "for i in range(3):\n",
    "    print(f\"\\nExample {i+1}:\")\n",
    "    print(f\"  Original: {df_final['text'].iloc[i][:150]}...\")\n",
    "    print(f\"  Normalized: {df_final['sentiment_ready_text'].iloc[i][:150]}...\")\n",
    "\n",
    "print(f\"\\nStep 9 complete: Minimal normalization applied for FinBERT.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a334bc62",
   "metadata": {},
   "source": [
    "**Final check: Drop original text column, keep only rows with at least one ticker. The sentiment_ready_text preserves natural language for FinBERT.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4312b1a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final.drop(columns = ['text'], inplace=True)\n",
    "df_final = df_final[df_final['n_tickers'] > 0].reset_index(drop=True)\n",
    "display(df_final.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b520591c",
   "metadata": {},
   "source": [
    "## 10 Export Results\n",
    "\n",
    "Save final dataset with preprocessed text and features to CSV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5320390",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10. Export Results: Save Sentiment-Ready Data\n",
    "\n",
    "print(f\"Available columns: {list(df_final.columns)}\")\n",
    "\n",
    "output_file = \"outputs/preprocessed_data.csv\"\n",
    "\n",
    "# Select columns for export\n",
    "export_columns = [\n",
    "    'id', 'sentiment_ready_text', 'type', 'subreddit',\n",
    "    'created_utc', 'normalized_score', 'mentioned_tickers',\n",
    "    'n_tickers', 'text_length', 'word_count',\n",
    "    'date', 'hour', 'day_of_week'\n",
    "]\n",
    "\n",
    "export_cols = [col for col in export_columns if col in df_final.columns]\n",
    "export_df = df_final[export_cols].copy()\n",
    "\n",
    "# Convert mentioned_tickers from list to comma-separated string\n",
    "export_df['mentioned_tickers'] = export_df['mentioned_tickers'].apply(\n",
    "    lambda x: ','.join(x) if isinstance(x, list) and len(x) > 0 else ''\n",
    ")\n",
    "print(f\"Converted 'mentioned_tickers' from list to comma-separated string format\")\n",
    "\n",
    "# Convert day_of_week from category to numeric for database compatibility\n",
    "day_mapping = {\n",
    "    'Monday': 0, 'Tuesday': 1, 'Wednesday': 2, 'Thursday': 3,\n",
    "    'Friday': 4, 'Saturday': 5, 'Sunday': 6\n",
    "}\n",
    "export_df['day_of_week'] = export_df['day_of_week'].map(day_mapping)\n",
    "print(f\"Converted 'day_of_week' from category to numeric (0-6)\")\n",
    "\n",
    "# Export to CSV\n",
    "export_df.to_csv(output_file, index=False)\n",
    "print(f\"\\nExported {len(export_df)} rows to {output_file}\")\n",
    "print(f\"Exported columns: {export_cols}\")\n",
    "\n",
    "print(f\"\\nNormalized Score Statistics:\")\n",
    "print(f\"  Mean: {export_df['normalized_score'].mean():.3f}\")\n",
    "print(f\"  Median: {export_df['normalized_score'].median():.3f}\")\n",
    "print(f\"  Range: [{export_df['normalized_score'].min():.3f}, {export_df['normalized_score'].max():.3f}]\")\n",
    "\n",
    "print(f\"\\nStep 10 complete: CSV export finished.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4acb0c13",
   "metadata": {},
   "source": [
    "## 11 Database Export — Oracle\n",
    "\n",
    "Export preprocessed data to Oracle table `preprocessed_data` for sentiment analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27571f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 11. Database Export: Save to Oracle\n",
    "conn = get_oracle_connection()\n",
    "\n",
    "if conn:\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    # Truncate existing data\n",
    "    cursor.execute(\"TRUNCATE TABLE preprocessed_data\")\n",
    "    print(\"Existing data truncated\")\n",
    "\n",
    "    # Rename 'date' to 'date_col' for database compatibility (Oracle table uses date_col)\n",
    "    db_export_df = export_df.copy()\n",
    "    if 'date' in db_export_df.columns:\n",
    "        db_export_df.rename(columns={'date': 'date_col'}, inplace=True)\n",
    "\n",
    "    # Insert statement\n",
    "    insert_sql = \"\"\"\n",
    "    INSERT INTO preprocessed_data (\n",
    "        id, sentiment_ready_text, type, subreddit,\n",
    "        created_utc, normalized_score, mentioned_tickers,\n",
    "        n_tickers, text_length, word_count,\n",
    "        date_col, hour, day_of_week\n",
    "    ) VALUES (\n",
    "        :1, :2, :3, :4, :5, :6, :7, :8, :9, :10, :11, :12, :13\n",
    "    )\n",
    "    \"\"\"\n",
    "\n",
    "    # Prepare data for batch insert\n",
    "    insert_data = []\n",
    "    for _, row in db_export_df.iterrows():\n",
    "        insert_data.append((\n",
    "            str(row['id']),\n",
    "            str(row['sentiment_ready_text']),\n",
    "            str(row['type']),\n",
    "            str(row['subreddit']),\n",
    "            row['created_utc'].timestamp() if pd.notna(row['created_utc']) else None,\n",
    "            float(row['normalized_score']) if pd.notna(row['normalized_score']) else None,\n",
    "            str(row['mentioned_tickers']) if pd.notna(row['mentioned_tickers']) else '',\n",
    "            int(row['n_tickers']) if pd.notna(row['n_tickers']) else 0,\n",
    "            int(row['text_length']) if pd.notna(row['text_length']) else 0,\n",
    "            int(row['word_count']) if pd.notna(row['word_count']) else 0,\n",
    "            row['date_col'] if pd.notna(row['date_col']) else None,\n",
    "            int(row['hour']) if pd.notna(row['hour']) else None,\n",
    "            int(row['day_of_week']) if pd.notna(row['day_of_week']) else None\n",
    "        ))\n",
    "\n",
    "    # Execute batch insert\n",
    "    cursor.executemany(insert_sql, insert_data)\n",
    "    conn.commit()\n",
    "\n",
    "    print(f\"Successfully exported {len(insert_data)} rows to Oracle table 'preprocessed_data'\")\n",
    "\n",
    "    cursor.close()\n",
    "    conn.close()\n",
    "    print(\"Database export complete!\")\n",
    "\n",
    "else:\n",
    "    print(\"Failed to connect to Oracle database\")\n",
    "    try:\n",
    "        if conn:\n",
    "            conn.close()\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "print(f\"\\nStep 11 complete: Database export finished\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
