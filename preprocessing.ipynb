{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6c71c795",
   "metadata": {},
   "source": [
    "# Stock Sentiment Analysis - Preprocessing Pipeline\n",
    "\n",
    "This notebook performs the preprocessing pipeline for stock sentiment analysis:\n",
    "1. Database Connection - Connect to Oracle database and fetch the data\n",
    "2. Ticker Data Loading - Fetch and load NASDAQ/NYSE ticker symbols\n",
    "3. Data Harmonization - Merge posts and comments into unified schema (zatim je to takto, i kdyz trackovat komentare pro urcity posty neni skrze primarni a cizi klic tak tezky, ale pokud je ticke v postu, ten samy ticker se zakonite nemusi resit v komentari pod tim u know. proto je zatim gold standard brat sentiment (pozdeji) z komentu/postu ktery ticker/y primo obsahuje. na tohle musime dat meeting)\n",
    "4. Data Cleaning - Removing invalid, deleted, and short texts proste cisteni \n",
    "5. Feature Engineering - Add temporal and engagement features (ruzny casovy prurezy atd - pro budouci poreby)\n",
    "6. Ticker Detection - Identify stock ticker mentions with high precision \n",
    "7. Text Normalization - Prepare text for sentiment analysis\n",
    "8. Export Results - Save processed data for further analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a1346aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports and Configuration\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import logging\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "from datetime import datetime\n",
    "from typing import List, Set, Dict, Any\n",
    "from tqdm import tqdm\n",
    "from io import StringIO\n",
    "import urllib.request\n",
    "import urllib.parse\n",
    "\n",
    "# Environment and database\n",
    "try:\n",
    "    from dotenv import load_dotenv\n",
    "    load_dotenv()\n",
    "    DOTENV_AVAILABLE = True\n",
    "except ImportError:\n",
    "    DOTENV_AVAILABLE = False\n",
    "\n",
    "try:\n",
    "    import oracledb\n",
    "    ORACLE_AVAILABLE = True\n",
    "except ImportError:\n",
    "    ORACLE_AVAILABLE = False\n",
    "    print(\"Oracle DB not available. Jeste nejsme cooked -> pip install oracledb\")\n",
    "\n",
    "# Check for NLP libraries\n",
    "try:\n",
    "    import nltk\n",
    "    NLTK_AVAILABLE = True\n",
    "    print(\"NLTK rdy\")\n",
    "except ImportError:\n",
    "    NLTK_AVAILABLE = False\n",
    "    print(\"NLTK not available -> pip install nltk\")\n",
    "\n",
    "try:\n",
    "    import spacy\n",
    "    SPACY_AVAILABLE = True\n",
    "    print(\"spaCy rdy\")\n",
    "except ImportError:\n",
    "    SPACY_AVAILABLE = False\n",
    "    print(\"spaCy not available -> pip install spacy\")\n",
    "\n",
    "# Import functions from utils\n",
    "from utils import (\n",
    "    create_sample_data,\n",
    "    get_oracle_connection, \n",
    "    safe_execute,\n",
    "    fetch_nasdaq_listed,\n",
    "    fetch_nyse_listed, \n",
    "    get_all_us_tickers,\n",
    "    detect_tickers_in_text,\n",
    "    apply_ticker_detection,\n",
    "    harmonize_schema,\n",
    "    drop_invalid_texts,\n",
    "    deduplicate_and_normalize_types,\n",
    "    add_temporal_features,\n",
    "    add_engagement_features,\n",
    "    normalize_text_for_sentiment,\n",
    "    apply_text_normalization,\n",
    "    # Stopword removal functions (NLTK and spaCy options)\n",
    "    remove_financial_stopwords,\n",
    "    remove_stopwords_spacy\n",
    ")\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s [%(levelname)s] %(message)s\")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Configuration constants\n",
    "MIN_TEXT_LENGTH = 10\n",
    "RETRY_DELAY = 10\n",
    "\n",
    "print(\"Imports loaded successfully letzgooo!\")\n",
    "print(f\"Pandas version: {pd.__version__}\")\n",
    "print(f\"NumPy version: {np.__version__}\")\n",
    "print(f\"Oracle DB available: {ORACLE_AVAILABLE}\")\n",
    "print(f\"Environment variables available: {DOTENV_AVAILABLE}\")\n",
    "print(\"Utils functions imported successfully\")\n",
    "print(f\"NLTK available: {NLTK_AVAILABLE}\")\n",
    "print(f\"spaCy available: {SPACY_AVAILABLE}\")\n",
    "print(\"Professional stopword removal functions imported\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c10b68e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Database Connection and Data Loading\n",
    "\n",
    "print(\"STEP 1: Importing Reddit Data from Oracle Database\")\n",
    "\n",
    "# Check if Oracle DB is available and credentials are set\n",
    "oracle_credentials_available = (\n",
    "    ORACLE_AVAILABLE and \n",
    "    os.getenv('db-username') and \n",
    "    os.getenv('db-password') and\n",
    "    os.getenv('db-dsn')\n",
    ")\n",
    "\n",
    "if oracle_credentials_available:\n",
    "    print(\"Oracle credentials found pojdme se pripojit :)\")\n",
    "    \n",
    "    conn = get_oracle_connection()\n",
    "\n",
    "    if conn:\n",
    "        print(\"Database connection successful letzgoo ðŸš€\")\n",
    "        \n",
    "        # data from db\n",
    "        print(\"Importing Reddit data from existing tables...\")\n",
    "        try:\n",
    "            # Query to import posts \n",
    "            query_posts = \"SELECT * FROM historical_posts WHERE ROWNUM <= 5000\"\n",
    "            df_posts = pd.read_sql_query(query_posts, conn)\n",
    "            \n",
    "            # Query to import comments \n",
    "            query_comments = \"SELECT * FROM historical_comments WHERE ROWNUM <= 5000\"\n",
    "            df_comments = pd.read_sql_query(query_comments, conn)\n",
    "\n",
    "            print(f\"Posts imported: {df_posts.shape}\")\n",
    "            print(f\"Comments imported: {df_comments.shape}\")\n",
    "            \n",
    "            # This converts LOB fields in posts - only fckng way it worked\n",
    "            for col in ['BODY', 'TITLE']:\n",
    "                if col in df_posts.columns:\n",
    "                    df_posts[col] = df_posts[col].astype(str)\n",
    "            \n",
    "            # Convert LOB fields in comments\n",
    "            for col in ['BODY']:\n",
    "                if col in df_comments.columns:\n",
    "                    df_comments[col] = df_comments[col].astype(str)\n",
    "            \n",
    "            print(\"LOB conversion complete\")\n",
    "            \n",
    "            if len(df_posts) > 0:\n",
    "                print(f\"\\nPosts columns: {list(df_posts.columns)}\")\n",
    "            \n",
    "            if len(df_comments) > 0:\n",
    "                print(f\"Comments columns: {list(df_comments.columns)}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error importing data from database: {e}\")\n",
    "            \n",
    "    else:\n",
    "        print(\"Failed to connect to database (tak to je v pici - check logs)\")\n",
    "        \n",
    "else:\n",
    "    print(\"Oracle database credentials not configured\")\n",
    "    conn = None\n",
    "\n",
    "print(f\"\\nStep 1 Complete: Loaded {len(df_posts)} posts and {len(df_comments)} comments. Letzgoo\")\n",
    "\n",
    "# Close connection after LOB conversion\n",
    "if 'conn' in locals() and conn:\n",
    "    try:\n",
    "        conn.close()\n",
    "        print(\"Database connection closed. Niceee ðŸš€\")\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bd8a27f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Ticker Data Loading\n",
    "\n",
    "print(\"STEP 2: Loading US Ticker Symbols\")\n",
    "\n",
    "# Check if we have cached ticker data\n",
    "us_tickers_path = \"us_tickers.csv\"\n",
    "\n",
    "if os.path.exists(us_tickers_path):\n",
    "    print(f\"Loading cached ticker data from {us_tickers_path}\")\n",
    "    try:\n",
    "        tickers_df = pd.read_csv(us_tickers_path, dtype=str)\n",
    "        # Normalizing column names\n",
    "        tickers_df.columns = [c.strip().lower().replace(\" \", \"_\") for c in tickers_df.columns]\n",
    "        if 'ticker' in tickers_df.columns:\n",
    "            tickers_df['ticker'] = tickers_df['ticker'].astype(str).str.upper().str.strip()\n",
    "        print(f\"Loaded {len(tickers_df)} cached tickers (zatim dobry)\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading cached data (ou nou): {e}\")\n",
    "        tickers_df = None\n",
    "else:\n",
    "    print(\"Fetching fresh ticker data\")\n",
    "    tickers_df = None\n",
    "\n",
    "# If no cached data or fresh data\n",
    "if tickers_df is None or len(tickers_df) == 0:\n",
    "    try:\n",
    "        tickers_df = get_all_us_tickers()\n",
    "        if len(tickers_df) > 0:\n",
    "            tickers_df.to_csv(us_tickers_path, index=False)\n",
    "            print(f\"Fetched and cached {len(tickers_df)} US tickers (so far so good)\")\n",
    "        else:\n",
    "            print(\"No ticker data retrieved (fuck)\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching ticker data (fuck): {e}\")\n",
    "        tickers_df = pd.DataFrame()\n",
    "\n",
    "if len(tickers_df) > 0:\n",
    "    print(f\"\\nTicker data summary:\")\n",
    "    print(f\"  Total tickers: {len(tickers_df)}\")\n",
    "    print(f\"  Exchanges: {tickers_df['exchange'].value_counts().to_dict()}\")\n",
    "    \n",
    "    # Check for major tickers - pojistka proste\n",
    "    major_tickers = {'AAPL', 'TSLA', 'MSFT', 'AMZN', 'GOOGL', 'NVDA', 'META'}\n",
    "    found_major = set(tickers_df['ticker']) & major_tickers\n",
    "    print(f\"  Major tickers found: {found_major}\")\n",
    "    \n",
    "else:\n",
    "    print(\"No ticker data available (well fuck)\")\n",
    "    \n",
    "print(f\"\\nStep 2 Complete: Loaded {len(tickers_df)} ticker symbols letzgoo ðŸš€\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "123aebdf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21117e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Data Harmonization - sloucime posty a komenty (zatim bez nejaky hierarchie)\n",
    "\n",
    "print(\"STEP 3: Data Harmonization\")\n",
    "\n",
    "if len(df_posts) > 0 or len(df_comments) > 0:\n",
    "    # combine data\n",
    "    df_unified = harmonize_schema(df_posts, df_comments)\n",
    "    print(f\"Unified dataframe shape: {df_unified.shape}\")\n",
    "    print(f\"Unified columns: {list(df_unified.columns)}\")\n",
    "    \n",
    "    # Show data type distribution\n",
    "    type_counts = df_unified['type'].value_counts()\n",
    "    print(f\"\\nData distribution:\")\n",
    "    for dtype, count in type_counts.items():\n",
    "        print(f\"  {dtype}: {count} rows\")\n",
    "    \n",
    "else:\n",
    "    print(\"No data to harmonize\")\n",
    "    df_unified = pd.DataFrame()\n",
    "\n",
    "print(f\"\\nStep 3 Complete: Unified {len(df_unified)} rows. Fuck yeaaah ðŸš€\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "979d1037",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Data Cleaning\n",
    "\n",
    "print(\"STEP 4: Data Cleaning\")\n",
    "\n",
    "if len(df_unified) > 0:\n",
    "    print(f\"Starting with {len(df_unified)} rows\")\n",
    "    \n",
    "    # Step 4a: Remove invalid texts\n",
    "    df_cleaned = drop_invalid_texts(df_unified, min_len=MIN_TEXT_LENGTH)\n",
    "    print(f\"    After removing invalid texts: {len(df_cleaned)} rows (-{len(df_unified) - len(df_cleaned)})\")\n",
    "    \n",
    "    # Step 4b: Deduplicate and normalize types\n",
    "    df_cleaned = deduplicate_and_normalize_types(df_cleaned)\n",
    "    print(f\"    After deduplication: {len(df_cleaned)} rows (tohle by melo byt idealne kladny cislo u know)\")\n",
    "    \n",
    "    # Show cleaning results\n",
    "    if len(df_cleaned) > 0:\n",
    "        print(f\"  Original rows: {len(df_unified)}\")\n",
    "        print(f\"  Cleaned rows: {len(df_cleaned)}\")\n",
    "        print(f\"  Removed: {len(df_unified) - len(df_cleaned)} ({((len(df_unified) - len(df_cleaned))/len(df_unified)*100):.1f}%)\")\n",
    "        \n",
    "        # Show data types after cleaning\n",
    "        print(f\"\\nData types after cleaning:\")\n",
    "        type_counts = df_cleaned['type'].value_counts()\n",
    "        for dtype, count in type_counts.items():\n",
    "            print(f\"  {dtype}: {count} rows\")\n",
    "    else:\n",
    "        print(\"No data remaining after cleaning - tohle neni uplne dobry :)\")\n",
    "        \n",
    "else:\n",
    "    print(\"No data to clean (jsme v prdeli)\")\n",
    "    df_cleaned = pd.DataFrame()\n",
    "\n",
    "print(f\"\\nStep 4 Complete: {len(df_cleaned)} clean rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d42e97b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Feature Engineering\n",
    "\n",
    "print(\"STEP 5: Feature Engineering\")\n",
    "\n",
    "if len(df_cleaned) > 0:\n",
    "    df_features = add_temporal_features(df_cleaned)\n",
    "    df_features = add_engagement_features(df_features)\n",
    "    \n",
    "    print(f\"Enhanced dataframe shape: {df_features.shape}\")\n",
    "    \n",
    "    # new features\n",
    "    new_features = ['date', 'hour', 'day_of_week', 'month', 'is_weekend', \n",
    "                   'text_length', 'word_count', 'score_log1p']\n",
    "    print(f\"\\nNew features added: {[f for f in new_features if f in df_features.columns]}\")\n",
    "    \n",
    "    # Show feature statistics\n",
    "    if 'text_length' in df_features.columns:\n",
    "        print(f\"  Text length: min={df_features['text_length'].min()}, \"\n",
    "              f\"mean={df_features['text_length'].mean():.1f}, \"\n",
    "              f\"max={df_features['text_length'].max()}\")\n",
    "    \n",
    "    if 'word_count' in df_features.columns:\n",
    "        print(f\"  Word count: min={df_features['word_count'].min()}, \"\n",
    "              f\"mean={df_features['word_count'].mean():.1f}, \"\n",
    "              f\"max={df_features['word_count'].max()}\")\n",
    "    \n",
    "    if 'day_of_week' in df_features.columns:\n",
    "        day_counts = df_features['day_of_week'].value_counts()\n",
    "        print(f\"  Day distribution: {day_counts.to_dict()}\")\n",
    "    \n",
    "    if 'is_weekend' in df_features.columns:\n",
    "        weekend_pct = df_features['is_weekend'].mean() * 100\n",
    "        print(f\"  Weekend posts: {weekend_pct:.1f}%\")\n",
    "    \n",
    "else:\n",
    "    print(\"No data for feature engineering (upsis)\")\n",
    "    df_features = pd.DataFrame()\n",
    "\n",
    "print(f\"\\nStep 5 Complete: {len(df_features)} rows with enhanced features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18f9518c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Ticker Detection \n",
    "\n",
    "print(\"STEP 6: Ticker Detection\")\n",
    "\n",
    "if len(df_features) > 0 and len(tickers_df) > 0:\n",
    "    print(f\"Detecting tickers in {len(df_features)} texts using {len(tickers_df)} symbols\")\n",
    "    \n",
    "    # improved ticker detection\n",
    "    df_with_tickers = apply_ticker_detection(df_features, tickers_df)\n",
    "    \n",
    "    # results\n",
    "    ticker_stats = df_with_tickers['n_tickers'].value_counts().sort_index()\n",
    "    total_with_tickers = (df_with_tickers['n_tickers'] > 0).sum()\n",
    "    print(f\"  Total rows: {len(df_with_tickers)}\")\n",
    "    print(f\"  Rows with tickers: {total_with_tickers} ({total_with_tickers/len(df_with_tickers)*100:.1f}%)\")\n",
    "    print(f\"  Rows without tickers: {len(df_with_tickers) - total_with_tickers}\")\n",
    "    \n",
    "    print(f\"\\nTicker count distribution:\")\n",
    "    for count, rows in ticker_stats.head(10).items():\n",
    "        print(f\"  {count} tickers: {rows} rows\")\n",
    "    \n",
    "    # Show if any tickers were found\n",
    "    ticker_examples = df_with_tickers[df_with_tickers['n_tickers'] > 0]\n",
    "    if len(ticker_examples) > 0:\n",
    "        print(f\"\\nTicker detection successful: Found {len(ticker_examples)} rows with ticker mentions\")\n",
    "    else:\n",
    "        print(f\"\\nNo tickers detected - this indicates high precision (no false positives)\")\n",
    "        print(\"Testing detection with synthetic examples:\")\n",
    "        \n",
    "        # Test with known ticker-rich text (projistotu)\n",
    "        ticker_set = set(tickers_df['ticker'])\n",
    "        test_texts = [\n",
    "            \"I'm buying $AAPL and TSLA today\",\n",
    "            \"MSFT and GOOGL are performing well\",\n",
    "            \"Just some random text without tickers\"\n",
    "        ]\n",
    "        \n",
    "        for test_text in test_texts:\n",
    "            detected = detect_tickers_in_text(test_text, ticker_set)\n",
    "            print(f\"    '{test_text}' â†’ {detected}\")\n",
    "    \n",
    "else:\n",
    "    print(\"No data or tickers available for detection\")\n",
    "    df_with_tickers = pd.DataFrame()\n",
    "\n",
    "print(f\"\\nStep 6 Complete: Processed {len(df_with_tickers)} rows for ticker detection\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ccc4a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 7: Text Normalization and Stopword Removal\n",
    "\n",
    "print(\"STEP 7: Text Normalization and Stopword Removal\")\n",
    "\n",
    "if len(df_with_tickers) > 0:\n",
    "\n",
    "    # Basic text normalization (creates 'sentiment_ready_text' column)\n",
    "    df_final = apply_text_normalization(df_with_tickers, keep_tickers=True)\n",
    "    \n",
    "    # Apply additional stopword removal to improve the text further\n",
    "    \n",
    "    if SPACY_AVAILABLE:\n",
    "        print(\"Using spaCy\")\n",
    "        df_final['sentiment_ready_text'] = df_final['sentiment_ready_text'].apply(\n",
    "            lambda x: remove_stopwords_spacy(x, preserve_tickers=True)\n",
    "        )\n",
    "        stopword_method = \"spaCy\"\n",
    "    elif NLTK_AVAILABLE:\n",
    "        print(\"Using NLTK\")\n",
    "        df_final['sentiment_ready_text'] = df_final['sentiment_ready_text'].apply(\n",
    "            lambda x: remove_financial_stopwords(x, preserve_tickers=True)\n",
    "        )\n",
    "        stopword_method = \"NLTK\"\n",
    "    else:\n",
    "        print(\"Using built-in stopword removal (lame as fuck)\")\n",
    "        df_final['sentiment_ready_text'] = df_final['sentiment_ready_text'].apply(\n",
    "            lambda x: remove_financial_stopwords(x, preserve_tickers=True)\n",
    "        )\n",
    "        stopword_method = \"Built-in\"\n",
    "    \n",
    "    print(f\"Text normalization complete using {stopword_method}\")\n",
    "    print(f\"Final dataframe shape: {df_final.shape}\")\n",
    "    print(f\"Final columns: {list(df_final.columns)}\")\n",
    "    \n",
    "    # Final statistics\n",
    "    print(f\"\\nFinal dataset statistics:\")\n",
    "    print(f\"  Total rows: {len(df_final)}\")\n",
    "    print(f\"  Rows with tickers: {(df_final['n_tickers'] > 0).sum()}\")\n",
    "    print(f\"  Average original text length: {df_final['text_length'].mean():.1f} characters\")\n",
    "    print(f\"  Average word count: {df_final['word_count'].mean():.1f} words\")\n",
    "    \n",
    "    # Show text processing impact\n",
    "    avg_original_length = df_final['text'].str.len().mean()\n",
    "    avg_sentiment_ready_length = df_final['sentiment_ready_text'].str.len().mean()\n",
    "    \n",
    "    print(f\"\\nimpact:\")\n",
    "    print(f\"  Original text length: {avg_original_length:.1f} chars\")\n",
    "    print(f\"  Sentiment-ready text length: {avg_sentiment_ready_length:.1f} chars\")\n",
    "    print(f\"  Reduction from normalization: {((avg_original_length - avg_sentiment_ready_length) / avg_original_length * 100):.1f}%\")\n",
    "    print(f\"  Stopword removal method: {stopword_method}\")\n",
    "    \n",
    "    if 'type' in df_final.columns:\n",
    "        type_dist = df_final['type'].value_counts()\n",
    "        print(f\"  Content distribution: {type_dist.to_dict()}\")\n",
    "    \n",
    "else:\n",
    "    print(\"No data for text normalization\")\n",
    "    df_final = pd.DataFrame()\n",
    "\n",
    "print(f\"\\nStep 7 Complete: {len(df_final)} rows ready for sentiment analysis (letzgoo ðŸš€)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5320390",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 8: Export Sentiment-Ready Data\n",
    "\n",
    "print(\"STEP 8: Export Processed Data\")\n",
    "\n",
    "if len(df_final) > 0:\n",
    "    print(f\"Available columns: {list(df_final.columns)}\")\n",
    "    \n",
    "    # Export to CSV \n",
    "    output_file = \"sentiment_ready_data.csv\"\n",
    "    \n",
    "    # Updated key columns \n",
    "    sentiment_columns = [\n",
    "        'id', 'text', 'sentiment_ready_text', 'type', 'subreddit', \n",
    "        'created_utc', 'score', 'mentioned_tickers', 'n_tickers',  \n",
    "        'text_length', 'word_count', 'date', 'hour', 'day_of_week']\n",
    "    \n",
    "    # Keep only columns that exist\n",
    "    export_columns = [col for col in sentiment_columns if col in df_final.columns]\n",
    "    export_df = df_final[export_columns].copy()\n",
    "    \n",
    "    # Save to CSV\n",
    "    export_df.to_csv(output_file, index=False)\n",
    "    print(f\"Exported {len(export_df)} rows to {output_file}\")\n",
    "    print(f\"Exported columns: {export_columns}\")\n",
    "    \n",
    "    print(f\"\\nData is ready for sentiment analysis\")\n",
    "    print(f\"Use the 'sentiment_ready_text' column for sentiment modeling\")\n",
    "    print(f\"Use the 'mentioned_tickers' column for ticker information\")\n",
    "    \n",
    "else:\n",
    "    print(\"No data to export (gg well played)\")\n",
    "\n",
    "print(f\"\\nPreprocessing Pipeline Complete (letzgoo ðŸš€)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
