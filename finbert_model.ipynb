{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e8672ff1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vitek\\AppData\\Roaming\\Python\\Python313\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "INFO:utils:spaCy model loaded successfully\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline\n",
    "import torch\n",
    "from utils import score_texts, get_oracle_connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f7e167bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"preprocessed_data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1f72b89",
   "metadata": {},
   "source": [
    "Load dat je tady zatím manuální. Mám blbej connection k Oraclu, takže to dole bude potom official"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c351b1d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading preprocessed data from Oracle...\n",
      "Error connecting to Oracle DB: DPY-6005: cannot connect to database (CONNECTION_ID=Pm2LZyv+xAu4LY2i1P6hGw==).\n",
      "DPY-6000: Listener refused connection. (Similar to ORA-12506)\n",
      "Could not obtain Oracle connection (check get_oracle_connection / .env).\n",
      "Input shape: (0, 0)\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        }
       ],
       "ref": "4dbe82ae-7d14-4603-ad82-e614ef3bc778",
       "rows": [],
       "shape": {
        "columns": 0,
        "rows": 0
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: []"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# === Load data ===\n",
    "# === Config ===\n",
    "DATA_PATH_OUT = \"outputs/finbert_sentiment.csv\"\n",
    "\n",
    "ID_COL   = \"id\"\n",
    "TEXT_COL = \"sentiment_ready_text\"\n",
    "\n",
    "try:\n",
    "    import oracledb\n",
    "    ORACLE_AVAILABLE = True\n",
    "except ImportError:\n",
    "    ORACLE_AVAILABLE = False\n",
    "    print(\"oracledb not installed. Install with: pip install oracledb\")\n",
    "\n",
    "# === Load data from Oracle preprocessed_data table ===\n",
    "print(\"Loading preprocessed data from Oracle...\")\n",
    "\n",
    "df = pd.DataFrame()\n",
    "\n",
    "if ORACLE_AVAILABLE:\n",
    "    conn = get_oracle_connection()   # uses your .env: db-username, db-password, db-dsn\n",
    "\n",
    "    if conn:\n",
    "        try:\n",
    "            query_preprocessed = \"\"\"\n",
    "                SELECT\n",
    "                    id,\n",
    "                    sentiment_ready_text,\n",
    "                    type,\n",
    "                    subreddit,\n",
    "                    created_utc,\n",
    "                    normalized_score,\n",
    "                    mentioned_tickers,\n",
    "                    n_tickers,\n",
    "                    text_length,\n",
    "                    word_count,\n",
    "                    date_col,\n",
    "                    hour,\n",
    "                    day_of_week\n",
    "                FROM preprocessed_data\n",
    "            \"\"\"\n",
    "\n",
    "            df = pd.read_sql_query(query_preprocessed, conn)\n",
    "\n",
    "            # close connection as soon as possible\n",
    "            conn.close()\n",
    "\n",
    "            print(\"Preprocessed rows loaded:\", len(df))\n",
    "            print(\"Columns:\", list(df.columns))\n",
    "\n",
    "            # Optional: convert types back to something nice for modelling\n",
    "            # created_utc was exported as epoch seconds in your preprocessing script\n",
    "            if \"created_utc\" in df.columns:\n",
    "                df[\"created_utc\"] = pd.to_datetime(df[\"created_utc\"], unit=\"s\", errors=\"coerce\")\n",
    "\n",
    "            # date_col is the Oracle name; rename back to 'date' if you prefer\n",
    "            if \"date_col\" in df.columns:\n",
    "                df.rename(columns={\"date_col\": \"date\"}, inplace=True)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(\"Error while loading from Oracle:\", e)\n",
    "            try:\n",
    "                conn.close()\n",
    "            except:\n",
    "                pass\n",
    "    else:\n",
    "        print(\"Could not obtain Oracle connection (check get_oracle_connection / .env).\")\n",
    "else:\n",
    "    print(\"Oracle not available in this environment.\")\n",
    "\n",
    "print(\"Input shape:\", df.shape)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f3c1b5f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'sentiment_ready_text', 'type', 'subreddit', 'created_utc',\n",
       "       'normalized_score', 'mentioned_tickers', 'n_tickers', 'text_length',\n",
       "       'word_count', 'date', 'hour', 'day_of_week'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abba3018",
   "metadata": {},
   "source": [
    "Starting off with the first of the models - a base FINBERT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0167a0e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: CPU\n"
     ]
    }
   ],
   "source": [
    "MODEL_NAME = \"ProsusAI/finbert\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# Use GPU if available; otherwise fall back to CPU\n",
    "device = 0 if torch.cuda.is_available() else -1\n",
    "print(\"Using device:\", \"GPU\" if device == 0 else \"CPU\")\n",
    "\n",
    "# === Build sentiment pipeline ===\n",
    "sentiment_pipe = pipeline(\n",
    "    \"text-classification\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    device=device,\n",
    "    return_all_scores=True,   # we want probabilities for all labels\n",
    "    truncation=True,\n",
    "    max_length=128            # can increase to 256 if your texts are longer\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7db96bda",
   "metadata": {},
   "source": [
    "Tohle se taky schová potom do utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "001ff428",
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_texts(texts):\n",
    "    \"\"\"\n",
    "    Run FinBERT on a list of texts and return structured sentiment info.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    texts : list of str\n",
    "        The input texts to classify.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    results : list of dict\n",
    "        Each dict has:\n",
    "        - sentiment_label : str       ('positive', 'neutral', 'negative')\n",
    "        - sentiment_score : float     (p_pos - p_neg in [-1, 1])\n",
    "        - p_pos, p_neu, p_neg : float (probabilities)\n",
    "    \"\"\"\n",
    "    # This calls the HF pipeline once for the whole batch\n",
    "    outputs = sentiment_pipe(texts)\n",
    "\n",
    "    results = []\n",
    "    for out in outputs:\n",
    "        # out is a list like:\n",
    "        # [{'label': 'positive', 'score': 0.7}, {'label': 'neutral', 'score': 0.2}, {'label': 'negative', 'score': 0.1}]\n",
    "        # Normalize label names to lowercase to be robust to variations\n",
    "        probs = {d[\"label\"].lower(): float(d[\"score\"]) for d in out}\n",
    "\n",
    "        p_pos = probs.get(\"positive\", 0.0)\n",
    "        p_neg = probs.get(\"negative\", 0.0)\n",
    "        p_neu = probs.get(\"neutral\", 0.0)\n",
    "\n",
    "        # Continuous sentiment score in [-1, 1]\n",
    "        sentiment_score = p_pos - p_neg\n",
    "\n",
    "        # Discrete label = argmax over the three probabilities\n",
    "        sentiment_label = max(probs, key=probs.get)\n",
    "\n",
    "        results.append({\n",
    "            \"sentiment_label\": sentiment_label,\n",
    "            \"sentiment_score\": sentiment_score,\n",
    "            \"p_pos\": p_pos,\n",
    "            \"p_neu\": p_neu,\n",
    "            \"p_neg\": p_neg\n",
    "        })\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caa2b650",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of texts to process: 29624\n",
      "Processed 32 / 29624 texts\n",
      "Processed 1632 / 29624 texts\n",
      "Processed 3232 / 29624 texts\n",
      "Processed 4832 / 29624 texts\n",
      "Processed 6432 / 29624 texts\n",
      "Processed 8032 / 29624 texts\n",
      "Processed 9632 / 29624 texts\n",
      "Processed 11232 / 29624 texts\n",
      "Processed 12832 / 29624 texts\n",
      "Processed 14432 / 29624 texts\n",
      "Processed 16032 / 29624 texts\n",
      "Processed 17632 / 29624 texts\n",
      "Processed 19232 / 29624 texts\n",
      "Processed 20832 / 29624 texts\n",
      "Processed 22432 / 29624 texts\n",
      "Processed 24032 / 29624 texts\n",
      "Processed 25632 / 29624 texts\n",
      "Processed 27232 / 29624 texts\n",
      "Processed 28832 / 29624 texts\n",
      "Scores computed: 29624 rows in df: 29624\n"
     ]
    }
   ],
   "source": [
    "# === Batch configuration ===\n",
    "# Larger batch_size => faster but more memory usage.\n",
    "# we can change the batch size as we wish...\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "sentiment_labels = []\n",
    "sentiment_scores = []\n",
    "p_pos_list = []\n",
    "p_neu_list = []\n",
    "p_neg_list = []\n",
    "\n",
    "# Replace NaNs with empty strings so the model doesn't crash\n",
    "texts = df[TEXT_COL].fillna(\"\").tolist()\n",
    "\n",
    "n_texts = len(texts)\n",
    "print(\"Number of texts to process:\", n_texts)\n",
    "\n",
    "for start in range(0, n_texts, BATCH_SIZE):\n",
    "    end = start + BATCH_SIZE\n",
    "    batch = texts[start:end]\n",
    "\n",
    "    scored = score_texts(batch)\n",
    "\n",
    "    # Extend our result lists\n",
    "    for r in scored:\n",
    "        sentiment_labels.append(r[\"sentiment_label\"])\n",
    "        sentiment_scores.append(r[\"sentiment_score\"])\n",
    "        p_pos_list.append(r[\"p_pos\"])\n",
    "        p_neu_list.append(r[\"p_neu\"])\n",
    "        p_neg_list.append(r[\"p_neg\"])\n",
    "\n",
    "    # Optional: simple progress print\n",
    "    if (start // BATCH_SIZE) % 50 == 0:\n",
    "        print(f\"Processed {min(end, n_texts)} / {n_texts} texts\")\n",
    "\n",
    "# Sanity check: number of scores should match number of rows\n",
    "print(\"Scores computed:\", len(sentiment_labels), \"rows in df:\", len(df))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a73cd01",
   "metadata": {},
   "source": [
    "Make it for every ticker in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca166e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['mentioned_tickers'] = df['mentioned_tickers'].apply(lambda x: x if isinstance(x, list) else [t.strip() for t in str(x).split(',') if t.strip()])\n",
    "df = df.explode('mentioned_tickers').reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "008776f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "export_df = df.copy()\n",
    "conn = get_oracle_connection()\n",
    "if conn:\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    # Truncate existing data (full refresh)\n",
    "    cursor.execute(\"TRUNCATE TABLE preprocessed_data\")\n",
    "    print(\"Existing data in 'preprocessed_data' truncated\")\n",
    "\n",
    "    # We now use 'date' directly (no renaming to date_col)\n",
    "    db_export_df = export_df.copy()\n",
    "\n",
    "    # Insert statement including sentiment columns\n",
    "    insert_sql = \"\"\"\n",
    "    INSERT INTO preprocessed_data (\n",
    "        id,\n",
    "        sentiment_ready_text,\n",
    "        type,\n",
    "        subreddit,\n",
    "        created_utc,\n",
    "        normalized_score,\n",
    "        mentioned_tickers,\n",
    "        n_tickers,\n",
    "        text_length,\n",
    "        word_count,\n",
    "        date,\n",
    "        hour,\n",
    "        day_of_week,\n",
    "        finbert_label,\n",
    "        finbert_score,\n",
    "        finbert_p_pos,\n",
    "        finbert_p_neu,\n",
    "        finbert_p_neg\n",
    "    ) VALUES (\n",
    "        :1, :2, :3, :4, :5, :6, :7, :8, :9,\n",
    "        :10, :11, :12, :13, :14, :15, :16, :17, :18\n",
    "    )\n",
    "    \"\"\"\n",
    "\n",
    "    # Prepare data for batch insert\n",
    "    insert_data = []\n",
    "    for _, row in db_export_df.iterrows():\n",
    "        insert_data.append((\n",
    "            # base columns (you specified these names)\n",
    "            str(row[\"id\"]),\n",
    "            str(row[\"sentiment_ready_text\"]),\n",
    "            str(row[\"type\"]),\n",
    "            str(row[\"subreddit\"]),\n",
    "            # created_utc: stored as epoch seconds (NUMBER) like your preprocessing step\n",
    "            row[\"created_utc\"].timestamp() if pd.notna(row.get(\"created_utc\")) else None,\n",
    "            float(row[\"normalized_score\"]) if pd.notna(row.get(\"normalized_score\")) else None,\n",
    "            str(row[\"mentioned_tickers\"]) if pd.notna(row.get(\"mentioned_tickers\")) else \"\",\n",
    "            int(row[\"n_tickers\"]) if pd.notna(row.get(\"n_tickers\")) else 0,\n",
    "            int(row[\"text_length\"]) if pd.notna(row.get(\"text_length\")) else 0,\n",
    "            int(row[\"word_count\"]) if pd.notna(row.get(\"word_count\")) else 0,\n",
    "            row.get(\"date\") if pd.notna(row.get(\"date\")) else None,\n",
    "            int(row[\"hour\"]) if pd.notna(row.get(\"hour\")) else None,\n",
    "            int(row[\"day_of_week\"]) if pd.notna(row.get(\"day_of_week\")) else None,\n",
    "\n",
    "            # FinBERT sentiment columns\n",
    "            str(row[\"finbert_label\"]),\n",
    "            float(row[\"finbert_score\"]) if pd.notna(row.get(\"finbert_score\")) else None,\n",
    "            float(row[\"finbert_p_pos\"]) if pd.notna(row.get(\"finbert_p_pos\")) else None,\n",
    "            float(row[\"finbert_p_neu\"]) if pd.notna(row.get(\"finbert_p_neu\")) else None,\n",
    "            float(row[\"finbert_p_neg\"]) if pd.notna(row.get(\"finbert_p_neg\")) else None,\n",
    "        ))\n",
    "\n",
    "    # Execute batch insert\n",
    "    cursor.executemany(insert_sql, insert_data)\n",
    "    conn.commit()\n",
    "\n",
    "    print(f\"Successfully exported {len(insert_data)} rows to Oracle table 'preprocessed_data'\")\n",
    "\n",
    "    cursor.close()\n",
    "    conn.close()\n",
    "    print(\"Database export complete!\")\n",
    "\n",
    "else:\n",
    "    print(\"Failed to connect to Oracle database\")\n",
    "    try:\n",
    "        if conn:\n",
    "            conn.close()\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "print(\"\\nStep 11 complete: Database export finished\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b454bcb",
   "metadata": {},
   "source": [
    "Do teď manual output, dokud nebudu mit fixed oracle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68572e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(DATA_PATH_OUT, index=False)\n",
    "print(\"Saved FinBERT sentiment data to:\", DATA_PATH_OUT)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
