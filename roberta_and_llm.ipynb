{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "05435721",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.13.5 (tags/v3.13.5:6cb20a2, Jun 11 2025, 16:15:46) [MSC v.1943 64 bit (AMD64)]\n",
      "Torch version: 2.7.1+cu118\n",
      "CUDA available: True\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "from utils import get_oracle_connection\n",
    "import sys\n",
    "print(sys.version)\n",
    "import torch\n",
    "print(\"Torch version:\", torch.__version__)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eebb41f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oracle connection successful!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vitek\\AppData\\Local\\Temp\\ipykernel_3700\\98098836.py:22: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql_query(query, conn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1000 rows\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "id",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "sentiment_ready_text",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "type",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "subreddit",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "created_utc",
         "rawType": "datetime64[ns]",
         "type": "datetime"
        },
        {
         "name": "normalized_upvotes",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "mentioned_tickers",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "n_tickers",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "text_length",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "word_count",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "hour",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "day_of_week",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "date",
         "rawType": "datetime64[ns]",
         "type": "datetime"
        }
       ],
       "ref": "be0c2d69-9e28-4031-8273-695f88ae9798",
       "rows": [
        [
         "0",
         "m48hiu4",
         "I'd considering splitting it across two banks in case you need access to funds in a pinch. It's a little more admin and you may get more mailers, but if the access goes down on one of the banks for whatever reason, you still have the other.",
         "comment",
         "investing",
         "2024-12-28 19:22:19",
         "0.018398966640233994",
         "UBS",
         "1",
         "241",
         "47",
         "19",
         "5",
         "2024-12-28 00:00:00"
        ],
        [
         "1",
         "m48hedi",
         "I had the opportunity for the IPO price thing and decided against it lol :/",
         "comment",
         "stocks",
         "2024-12-28 19:21:38",
         "0.011073613539338112",
         "AMD",
         "1",
         "75",
         "15",
         "19",
         "5",
         "2024-12-28 00:00:00"
        ],
        [
         "2",
         "m48heet",
         "Looks like It's down, but the 6 month chart shows so good growth. Sounds good, but hope this trend of going down doesn't persist. Suppose that's the risk factor, could get about 8.6 shares with an initial $200 investment. What broker you use? I'm using Webull, may look Into Fidelity.",
         "comment",
         "investing",
         "2024-12-28 19:21:38",
         "0.018076177686452866",
         "KO",
         "1",
         "286",
         "50",
         "19",
         "5",
         "2024-12-28 00:00:00"
        ],
        [
         "3",
         "m48gxg1",
         "Just look at 52 week lows. Stock screeners will mislead you since they are based on recent metrics. You want to find stocks that people hate and then sort out the ones with long term (not short term) deteriorating financials. You will be left with a few decent ideas. Like $HSY.",
         "comment",
         "ValueInvesting",
         "2024-12-28 19:19:04",
         "0.056502241641283035",
         "HSY",
         "1",
         "278",
         "51",
         "19",
         "5",
         "2024-12-28 00:00:00"
        ],
        [
         "4",
         "m48guco",
         "Are you looking to index? Wealthfront has a new index fund. Fidelity also has one.",
         "comment",
         "investing",
         "2024-12-28 19:18:36",
         "0.018076177686452866",
         "UBS",
         "1",
         "82",
         "15",
         "19",
         "5",
         "2024-12-28 00:00:00"
        ]
       ],
       "shape": {
        "columns": 13,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sentiment_ready_text</th>\n",
       "      <th>type</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>normalized_upvotes</th>\n",
       "      <th>mentioned_tickers</th>\n",
       "      <th>n_tickers</th>\n",
       "      <th>text_length</th>\n",
       "      <th>word_count</th>\n",
       "      <th>hour</th>\n",
       "      <th>day_of_week</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>m48hiu4</td>\n",
       "      <td>I'd considering splitting it across two banks ...</td>\n",
       "      <td>comment</td>\n",
       "      <td>investing</td>\n",
       "      <td>2024-12-28 19:22:19</td>\n",
       "      <td>0.018399</td>\n",
       "      <td>UBS</td>\n",
       "      <td>1</td>\n",
       "      <td>241</td>\n",
       "      <td>47</td>\n",
       "      <td>19</td>\n",
       "      <td>5</td>\n",
       "      <td>2024-12-28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>m48hedi</td>\n",
       "      <td>I had the opportunity for the IPO price thing ...</td>\n",
       "      <td>comment</td>\n",
       "      <td>stocks</td>\n",
       "      <td>2024-12-28 19:21:38</td>\n",
       "      <td>0.011074</td>\n",
       "      <td>AMD</td>\n",
       "      <td>1</td>\n",
       "      <td>75</td>\n",
       "      <td>15</td>\n",
       "      <td>19</td>\n",
       "      <td>5</td>\n",
       "      <td>2024-12-28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>m48heet</td>\n",
       "      <td>Looks like It's down, but the 6 month chart sh...</td>\n",
       "      <td>comment</td>\n",
       "      <td>investing</td>\n",
       "      <td>2024-12-28 19:21:38</td>\n",
       "      <td>0.018076</td>\n",
       "      <td>KO</td>\n",
       "      <td>1</td>\n",
       "      <td>286</td>\n",
       "      <td>50</td>\n",
       "      <td>19</td>\n",
       "      <td>5</td>\n",
       "      <td>2024-12-28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>m48gxg1</td>\n",
       "      <td>Just look at 52 week lows. Stock screeners wil...</td>\n",
       "      <td>comment</td>\n",
       "      <td>ValueInvesting</td>\n",
       "      <td>2024-12-28 19:19:04</td>\n",
       "      <td>0.056502</td>\n",
       "      <td>HSY</td>\n",
       "      <td>1</td>\n",
       "      <td>278</td>\n",
       "      <td>51</td>\n",
       "      <td>19</td>\n",
       "      <td>5</td>\n",
       "      <td>2024-12-28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>m48guco</td>\n",
       "      <td>Are you looking to index? Wealthfront has a ne...</td>\n",
       "      <td>comment</td>\n",
       "      <td>investing</td>\n",
       "      <td>2024-12-28 19:18:36</td>\n",
       "      <td>0.018076</td>\n",
       "      <td>UBS</td>\n",
       "      <td>1</td>\n",
       "      <td>82</td>\n",
       "      <td>15</td>\n",
       "      <td>19</td>\n",
       "      <td>5</td>\n",
       "      <td>2024-12-28</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                               sentiment_ready_text     type  \\\n",
       "0  m48hiu4  I'd considering splitting it across two banks ...  comment   \n",
       "1  m48hedi  I had the opportunity for the IPO price thing ...  comment   \n",
       "2  m48heet  Looks like It's down, but the 6 month chart sh...  comment   \n",
       "3  m48gxg1  Just look at 52 week lows. Stock screeners wil...  comment   \n",
       "4  m48guco  Are you looking to index? Wealthfront has a ne...  comment   \n",
       "\n",
       "        subreddit         created_utc  normalized_upvotes mentioned_tickers  \\\n",
       "0       investing 2024-12-28 19:22:19            0.018399               UBS   \n",
       "1          stocks 2024-12-28 19:21:38            0.011074               AMD   \n",
       "2       investing 2024-12-28 19:21:38            0.018076                KO   \n",
       "3  ValueInvesting 2024-12-28 19:19:04            0.056502               HSY   \n",
       "4       investing 2024-12-28 19:18:36            0.018076               UBS   \n",
       "\n",
       "   n_tickers  text_length  word_count  hour  day_of_week       date  \n",
       "0          1          241          47    19            5 2024-12-28  \n",
       "1          1           75          15    19            5 2024-12-28  \n",
       "2          1          286          50    19            5 2024-12-28  \n",
       "3          1          278          51    19            5 2024-12-28  \n",
       "4          1           82          15    19            5 2024-12-28  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conn = get_oracle_connection()\n",
    "\n",
    "query = \"\"\"\n",
    "SELECT\n",
    "    ID,\n",
    "    DBMS_LOB.SUBSTR(SENTIMENT_READY_TEXT, 20000, 1) as SENTIMENT_READY_TEXT,\n",
    "    TYPE,\n",
    "    SUBREDDIT,\n",
    "    CREATED_UTC,\n",
    "    NORMALIZED_UPVOTES,\n",
    "    DBMS_LOB.SUBSTR(MENTIONED_TICKERS, 100, 1) as MENTIONED_TICKERS,\n",
    "    N_TICKERS,\n",
    "    TEXT_LENGTH,\n",
    "    WORD_COUNT,\n",
    "    DATE_COL,\n",
    "    HOUR,\n",
    "    DAY_OF_WEEK\n",
    "FROM preprocessed_data\n",
    "FETCH FIRST 1000 ROWS ONLY\n",
    "\"\"\"\n",
    "\n",
    "df = pd.read_sql_query(query, conn)\n",
    "conn.close()\n",
    "\n",
    "df.columns = df.columns.str.lower()\n",
    "df['created_utc'] = pd.to_datetime(df['created_utc'], unit='s')\n",
    "if 'date_col' in df.columns:\n",
    "    df['date'] = pd.to_datetime(df['date_col'])\n",
    "    df.drop(columns=['date_col'], inplace=True)\n",
    "\n",
    "print(f\"Loaded {len(df)} rows\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b762ec03",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vitek\\AppData\\Roaming\\Python\\Python313\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "FIXED HYBRID SENTIMENT PIPELINE\n",
      "================================================================================\n",
      "Device: cuda\n",
      "Per-ticker rows: 1,950\n",
      "\n",
      "Loading Twitter-RoBERTa...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Loaded\n",
      "\n",
      "================================================================================\n",
      "RUNNING TWITTER ROBERTA\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Twitter RoBERTa: 100%|██████████| 31/31 [00:46<00:00,  1.49s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Twitter-RoBERTa Results:\n",
      "  Label distribution:\n",
      "tw_label\n",
      "positive    784\n",
      "neutral     648\n",
      "negative    518\n",
      "Name: count, dtype: int64\n",
      "  Score stats: mean=0.095, std=0.473\n",
      "\n",
      "================================================================================\n",
      "SELECTING UNCERTAIN CASES FOR LLM\n",
      "================================================================================\n",
      "Uncertain cases (tw_score ∈ [-0.1, 0.1]): 488 rows (25.0%)\n",
      "\n",
      "================================================================================\n",
      "LOADING QWEN LLM\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "INFO:accelerate.utils.modeling:We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Qwen loaded\n",
      "\n",
      "================================================================================\n",
      "RUNNING LLM ON UNCERTAIN CASES\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LLM:   0%|          | 0/61 [00:00<?, ?it/s]The following generation flags are not valid and may be ignored: ['top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLM:   2%|▏         | 1/61 [00:18<18:10, 18.17s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLM:   3%|▎         | 2/61 [00:34<16:51, 17.15s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLM:   5%|▍         | 3/61 [00:52<16:47, 17.38s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLM:   7%|▋         | 4/61 [01:08<16:08, 17.00s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLM:   8%|▊         | 5/61 [01:25<15:54, 17.05s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLM:  10%|▉         | 6/61 [01:44<16:05, 17.55s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLM:  11%|█▏        | 7/61 [02:02<16:04, 17.87s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLM:  13%|█▎        | 8/61 [02:20<15:49, 17.91s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLM:  15%|█▍        | 9/61 [02:37<15:15, 17.61s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLM:  16%|█▋        | 10/61 [02:54<14:36, 17.18s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLM:  18%|█▊        | 11/61 [03:10<14:15, 17.11s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLM:  20%|█▉        | 12/61 [03:27<13:46, 16.86s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLM:  21%|██▏       | 13/61 [03:43<13:20, 16.68s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLM:  23%|██▎       | 14/61 [04:00<13:07, 16.76s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLM:  25%|██▍       | 15/61 [04:17<13:00, 16.96s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLM:  26%|██▌       | 16/61 [04:35<12:49, 17.10s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLM:  28%|██▊       | 17/61 [04:52<12:29, 17.04s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLM:  30%|██▉       | 18/61 [05:09<12:10, 16.99s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLM:  31%|███       | 19/61 [05:26<11:52, 16.97s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLM:  33%|███▎      | 20/61 [05:42<11:26, 16.75s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLM:  34%|███▍      | 21/61 [05:59<11:12, 16.81s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLM:  36%|███▌      | 22/61 [06:16<10:57, 16.86s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLM:  38%|███▊      | 23/61 [06:33<10:42, 16.90s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLM:  39%|███▉      | 24/61 [06:48<10:13, 16.57s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLM:  41%|████      | 25/61 [07:05<09:53, 16.48s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLM:  43%|████▎     | 26/61 [07:22<09:47, 16.78s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Error at batch 208: 'NoneType' object is not subscriptable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LLM:  46%|████▌     | 28/61 [07:41<07:23, 13.45s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLM:  48%|████▊     | 29/61 [07:58<07:38, 14.34s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLM:  49%|████▉     | 30/61 [08:15<07:46, 15.04s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLM:  51%|█████     | 31/61 [08:33<07:51, 15.72s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLM:  52%|█████▏    | 32/61 [08:50<07:50, 16.22s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLM:  54%|█████▍    | 33/61 [09:07<07:40, 16.45s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLM:  56%|█████▌    | 34/61 [09:24<07:28, 16.62s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLM:  57%|█████▋    | 35/61 [09:41<07:14, 16.73s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLM:  59%|█████▉    | 36/61 [09:59<07:00, 16.82s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLM:  61%|██████    | 37/61 [10:16<06:45, 16.88s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLM:  62%|██████▏   | 38/61 [10:33<06:29, 16.92s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLM:  64%|██████▍   | 39/61 [10:50<06:12, 16.94s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLM:  66%|██████▌   | 40/61 [11:07<05:56, 16.97s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLM:  67%|██████▋   | 41/61 [11:24<05:42, 17.13s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLM:  69%|██████▉   | 42/61 [11:40<05:20, 16.89s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLM:  70%|███████   | 43/61 [11:58<05:07, 17.08s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLM:  72%|███████▏  | 44/61 [12:15<04:49, 17.06s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLM:  74%|███████▍  | 45/61 [12:31<04:29, 16.83s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLM:  75%|███████▌  | 46/61 [12:48<04:10, 16.68s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLM:  77%|███████▋  | 47/61 [13:05<03:55, 16.79s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLM:  79%|███████▊  | 48/61 [13:22<03:39, 16.86s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLM:  80%|████████  | 49/61 [13:39<03:22, 16.91s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLM:  82%|████████▏ | 50/61 [13:55<03:04, 16.73s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLM:  84%|████████▎ | 51/61 [14:12<02:48, 16.83s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLM:  85%|████████▌ | 52/61 [14:28<02:30, 16.68s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLM:  87%|████████▋ | 53/61 [14:45<02:14, 16.78s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLM:  89%|████████▊ | 54/61 [15:04<02:00, 17.26s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLM:  90%|█████████ | 55/61 [15:21<01:44, 17.34s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLM:  92%|█████████▏| 56/61 [15:39<01:27, 17.40s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLM:  93%|█████████▎| 57/61 [15:56<01:09, 17.28s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLM:  95%|█████████▌| 58/61 [16:13<00:51, 17.20s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLM:  97%|█████████▋| 59/61 [16:29<00:33, 16.91s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLM:  98%|█████████▊| 60/61 [16:47<00:17, 17.09s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLM: 100%|██████████| 61/61 [17:04<00:00, 16.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "LLM RESULTS ANALYSIS\n",
      "================================================================================\n",
      "\n",
      "LLM Label Distribution:\n",
      "llm_sentiment_label\n",
      "negative    292\n",
      "neutral     106\n",
      "positive     90\n",
      "Name: count, dtype: int64\n",
      "\n",
      "LLM Score Statistics:\n",
      "count    488.000000\n",
      "mean      -0.123590\n",
      "std        0.470279\n",
      "min       -1.000000\n",
      "25%       -0.300000\n",
      "50%       -0.300000\n",
      "75%        0.000000\n",
      "max        1.000000\n",
      "Name: llm_sentiment_score, dtype: float64\n",
      "\n",
      "LLM Score Diversity:\n",
      "  Unique values: 75 out of 488\n",
      "  ✓ Good diversity!\n",
      "\n",
      "================================================================================\n",
      "LLM EXAMPLES\n",
      "================================================================================\n",
      "\n",
      "Most positive by LLM:\n",
      "\n",
      "  Score: +1.00 | Ticker: HIVE\n",
      "  Twitter: +0.06 (uncertain)\n",
      "  Text: They have no debt. Trades at a discount to book value. SG&A is minimal. They have an atm just in cas...\n",
      "\n",
      "  Score: +1.00 | Ticker: AMD\n",
      "  Twitter: +0.04 (uncertain)\n",
      "  Text: You seriously think it’s more likely for NVDA to increase to 6T versus AMD going to 300-350B from 20...\n",
      "\n",
      "  Score: +1.00 | Ticker: WVE\n",
      "  Twitter: +0.05 (uncertain)\n",
      "  Text: I sold half of my PLTR position after it doubled to recover the initial investment- and put it into ...\n",
      "\n",
      "Most negative by LLM:\n",
      "\n",
      "  Score: -1.00 | Ticker: UBS\n",
      "  Twitter: +0.01 (uncertain)\n",
      "  Text: I'd considering splitting it across two banks in case you need access to funds in a pinch. It's a li...\n",
      "\n",
      "  Score: -1.00 | Ticker: ASTS\n",
      "  Twitter: -0.10 (uncertain)\n",
      "  Text: I will be messaging you in 5 years on **2029-12-27 07:21:34 UTC** ^(delete this message to hide from...\n",
      "\n",
      "  Score: -1.00 | Ticker: BND\n",
      "  Twitter: +0.03 (uncertain)\n",
      "  Text: I recently went through this with Edward Jones and the answer is: depends on what it's invested in a...\n",
      "\n",
      "================================================================================\n",
      "SAVING\n",
      "================================================================================\n",
      "✓ Saved to sentiment_hybrid_twitter_llm.csv\n",
      "\n",
      "================================================================================\n",
      "PIPELINE SUMMARY\n",
      "================================================================================\n",
      "\n",
      "Total rows: 1,950\n",
      "\n",
      "Twitter-RoBERTa (all rows):\n",
      "  Positive: 784\n",
      "  Neutral:  648\n",
      "  Negative: 518\n",
      "\n",
      "LLM (uncertain cases only):\n",
      "  Processed: 488\n",
      "  Positive: 90\n",
      "  Neutral:  106\n",
      "  Negative: 292\n",
      "\n",
      "✓ Complete!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "C:\\Users\\vitek\\AppData\\Local\\Temp\\ipykernel_3700\\2456477133.py:371: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '['negative' 'negative' 'negative' 'negative' 'negative' 'neutral'\n",
      " 'neutral' 'neutral' 'negative' 'neutral' 'negative' 'negative' 'positive'\n",
      " 'positive' 'positive' 'positive' 'negative' 'negative' 'negative'\n",
      " 'negative' 'negative' 'negative' 'neutral' 'negative' 'negative'\n",
      " 'positive' 'negative' 'negative' 'positive' 'positive' 'positive'\n",
      " 'positive' 'negative' 'negative' 'negative' 'negative' 'neutral'\n",
      " 'positive' 'positive' 'negative' 'negative' 'negative' 'negative'\n",
      " 'negative' 'negative' 'negative' 'negative' 'positive' 'negative'\n",
      " 'positive' 'positive' 'positive' 'positive' 'negative' 'neutral'\n",
      " 'negative' 'negative' 'negative' 'negative' 'negative' 'negative'\n",
      " 'negative' 'neutral' 'neutral' 'negative' 'positive' 'positive'\n",
      " 'positive' 'negative' 'negative' 'negative' 'neutral' 'negative'\n",
      " 'negative' 'negative' 'negative' 'negative' 'positive' 'positive'\n",
      " 'negative' 'neutral' 'negative' 'positive' 'positive' 'negative'\n",
      " 'negative' 'negative' 'negative' 'neutral' 'negative' 'neutral'\n",
      " 'negative' 'neutral' 'negative' 'negative' 'positive' 'negative'\n",
      " 'positive' 'neutral' 'negative' 'negative' 'negative' 'negative'\n",
      " 'negative' 'negative' 'negative' 'negative' 'positive' 'positive'\n",
      " 'positive' 'neutral' 'neutral' 'negative' 'negative' 'positive'\n",
      " 'negative' 'negative' 'negative' 'positive' 'neutral' 'positive'\n",
      " 'negative' 'positive' 'neutral' 'negative' 'negative' 'negative'\n",
      " 'negative' 'negative' 'negative' 'negative' 'negative' 'positive'\n",
      " 'neutral' 'negative' 'negative' 'negative' 'negative' 'negative'\n",
      " 'negative' 'neutral' 'positive' 'neutral' 'neutral' 'negative' 'negative'\n",
      " 'negative' 'negative' 'negative' 'negative' 'positive' 'positive'\n",
      " 'negative' 'neutral' 'neutral' 'neutral' 'neutral' 'neutral' 'neutral'\n",
      " 'neutral' 'positive' 'negative' 'negative' 'positive' 'positive'\n",
      " 'negative' 'negative' 'negative' 'negative' 'neutral' 'negative'\n",
      " 'negative' 'neutral' 'neutral' 'positive' 'negative' 'neutral' 'negative'\n",
      " 'neutral' 'neutral' 'negative' 'negative' 'negative' 'negative'\n",
      " 'negative' 'positive' 'positive' 'negative' 'neutral' 'negative'\n",
      " 'negative' 'negative' 'positive' 'positive' 'positive' 'negative'\n",
      " 'neutral' 'negative' 'negative' 'positive' 'positive' 'positive'\n",
      " 'positive' 'neutral' 'positive' 'neutral' 'positive' 'negative' 'neutral'\n",
      " 'neutral' 'neutral' 'neutral' 'neutral' 'neutral' 'neutral' 'neutral'\n",
      " 'negative' 'negative' 'negative' 'negative' 'negative' 'negative'\n",
      " 'negative' 'positive' 'negative' 'negative' 'neutral' 'negative'\n",
      " 'negative' 'negative' 'negative' 'neutral' 'negative' 'negative'\n",
      " 'negative' 'positive' 'negative' 'negative' 'neutral' 'neutral' 'neutral'\n",
      " 'negative' 'neutral' 'negative' 'negative' 'neutral' 'positive'\n",
      " 'negative' 'negative' 'negative' 'neutral' 'negative' 'negative'\n",
      " 'neutral' 'negative' 'negative' 'negative' 'negative' 'positive'\n",
      " 'negative' 'neutral' 'positive' 'negative' 'negative' 'neutral'\n",
      " 'negative' 'negative' 'positive' 'negative' 'positive' 'negative'\n",
      " 'negative' 'positive' 'negative' 'neutral' 'negative' 'negative'\n",
      " 'neutral' 'negative' 'neutral' 'negative' 'negative' 'negative'\n",
      " 'positive' 'negative' 'neutral' 'negative' 'negative' 'negative'\n",
      " 'negative' 'negative' 'neutral' 'negative' 'negative' 'negative'\n",
      " 'negative' 'negative' 'negative' 'positive' 'negative' 'negative'\n",
      " 'negative' 'neutral' 'negative' 'negative' 'negative' 'negative'\n",
      " 'negative' 'negative' 'negative' 'negative' 'neutral' 'negative'\n",
      " 'neutral' 'negative' 'positive' 'negative' 'negative' 'neutral'\n",
      " 'negative' 'negative' 'neutral' 'positive' 'positive' 'neutral' 'neutral'\n",
      " 'neutral' 'positive' 'neutral' 'neutral' 'neutral' 'negative' 'negative'\n",
      " 'negative' 'negative' 'negative' 'negative' 'negative' 'positive'\n",
      " 'negative' 'positive' 'negative' 'negative' 'negative' 'neutral'\n",
      " 'negative' 'negative' 'negative' 'neutral' 'positive' 'positive'\n",
      " 'negative' 'neutral' 'neutral' 'negative' 'negative' 'negative'\n",
      " 'negative' 'negative' 'negative' 'negative' 'neutral' 'neutral'\n",
      " 'negative' 'negative' 'negative' 'negative' 'negative' 'negative'\n",
      " 'negative' 'negative' 'negative' 'positive' 'positive' 'neutral'\n",
      " 'neutral' 'positive' 'negative' 'negative' 'neutral' 'negative'\n",
      " 'positive' 'negative' 'negative' 'negative' 'negative' 'positive'\n",
      " 'negative' 'negative' 'negative' 'negative' 'negative' 'negative'\n",
      " 'negative' 'negative' 'neutral' 'negative' 'negative' 'negative'\n",
      " 'negative' 'negative' 'positive' 'positive' 'positive' 'negative'\n",
      " 'negative' 'negative' 'positive' 'negative' 'negative' 'negative'\n",
      " 'negative' 'negative' 'positive' 'positive' 'positive' 'positive'\n",
      " 'negative' 'negative' 'negative' 'negative' 'negative' 'negative'\n",
      " 'negative' 'neutral' 'neutral' 'positive' 'negative' 'neutral' 'neutral'\n",
      " 'positive' 'neutral' 'positive' 'negative' 'negative' 'negative'\n",
      " 'neutral' 'neutral' 'neutral' 'neutral' 'neutral' 'negative' 'negative'\n",
      " 'negative' 'negative' 'negative' 'negative' 'neutral' 'neutral' 'neutral'\n",
      " 'positive' 'negative' 'negative' 'negative' 'negative' 'negative'\n",
      " 'neutral' 'neutral' 'negative' 'neutral' 'neutral' 'negative' 'negative'\n",
      " 'negative' 'negative' 'neutral' 'negative' 'negative' 'negative'\n",
      " 'negative' 'negative' 'negative' 'negative' 'negative' 'negative'\n",
      " 'negative' 'negative' 'neutral' 'positive' 'neutral' 'positive' 'neutral'\n",
      " 'negative' 'positive' 'positive' 'negative' 'negative' 'negative']' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  df.loc[df_llm.index, \"llm_sentiment_label\"] = df_llm[\"llm_sentiment_label\"]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "FIXED HYBRID SENTIMENT PIPELINE\n",
    "\n",
    "Improvements:\n",
    "1. Better LLM prompt with more diverse score examples\n",
    "2. Asks for nuanced scores (not just -0.8, 0, 0.8)\n",
    "3. Better parsing with fallback strategies\n",
    "4. Validation to catch bad outputs\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoModelForCausalLM\n",
    ")\n",
    "from tqdm import tqdm\n",
    "import ast\n",
    "import time\n",
    "import numpy as np\n",
    "import json\n",
    "import re\n",
    "\n",
    "# ============================================================================\n",
    "# CONFIG\n",
    "# ============================================================================\n",
    "\n",
    "OUTPUT_FILE = \"sentiment_hybrid_twitter_llm.csv\"\n",
    "\n",
    "TEXT_COL   = \"sentiment_ready_text\"\n",
    "TICKER_COL = \"mentioned_tickers\"\n",
    "\n",
    "TW_MODEL_NAME  = \"cardiffnlp/twitter-roberta-base-sentiment-latest\"\n",
    "LLM_MODEL_NAME = \"Qwen/Qwen2.5-1.5B-Instruct\"\n",
    "\n",
    "BATCH_SIZE_CLS = 64\n",
    "BATCH_SIZE_LLM = 8\n",
    "\n",
    "MAX_INPUT_TOKENS = 512\n",
    "MAX_NEW_TOKENS   = 64\n",
    "USE_FP16_LLM = True\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"FIXED HYBRID SENTIMENT PIPELINE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# ============================================================================\n",
    "# DEVICE\n",
    "# ============================================================================\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Device:\", device)\n",
    "\n",
    "# ============================================================================\n",
    "# BASIC PREP\n",
    "# ============================================================================\n",
    "\n",
    "def parse_tickers(x):\n",
    "    if pd.isna(x) or x == \"\":\n",
    "        return []\n",
    "    try:\n",
    "        return [str(t).strip() for t in ast.literal_eval(str(x)) if t]\n",
    "    except:\n",
    "        return [t.strip() for t in str(x).split(\",\") if t.strip()]\n",
    "\n",
    "df[\"tickers_list\"] = df[TICKER_COL].apply(parse_tickers)\n",
    "df[\"n_tickers\"] = df[\"tickers_list\"].apply(len)\n",
    "\n",
    "df = df[df[\"n_tickers\"] > 0].copy()\n",
    "\n",
    "# Explode to per-ticker rows\n",
    "df = df.explode(\"tickers_list\").reset_index(drop=True)\n",
    "df = df.rename(columns={\"tickers_list\": \"ticker\"})\n",
    "\n",
    "print(f\"Per-ticker rows: {len(df):,}\")\n",
    "\n",
    "# ============================================================================\n",
    "# LOAD TWITTER ROBERTA\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\nLoading Twitter-RoBERTa...\")\n",
    "tw_tokenizer = AutoTokenizer.from_pretrained(TW_MODEL_NAME)\n",
    "tw_model     = AutoModelForSequenceClassification.from_pretrained(TW_MODEL_NAME)\n",
    "tw_model.to(device)\n",
    "tw_model.eval()\n",
    "print(\"✓ Loaded\")\n",
    "\n",
    "# ============================================================================\n",
    "# TWITTER ROBERTA FUNCTION\n",
    "# ============================================================================\n",
    "\n",
    "def twitter_batch(texts, tickers):\n",
    "    texts = [f\"{tic}: {txt}\" for txt, tic in zip(texts, tickers)]\n",
    "\n",
    "    enc = tw_tokenizer(\n",
    "        texts,\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=MAX_INPUT_TOKENS,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        probs = torch.softmax(tw_model(**enc).logits, dim=-1).cpu().numpy()\n",
    "\n",
    "    id2label = tw_model.config.id2label\n",
    "    results = []\n",
    "\n",
    "    for prob in probs:\n",
    "        label_map = {id2label[i].lower(): prob[i] for i in range(len(prob))}\n",
    "        \n",
    "        p_pos = label_map.get(\"positive\", 0.0)\n",
    "        p_neg = label_map.get(\"negative\", 0.0)\n",
    "        p_neu = label_map.get(\"neutral\", 0.0)\n",
    "\n",
    "        # Weighted score for better distribution\n",
    "        score = p_pos * 1.0 + p_neu * 0.0 + p_neg * (-1.0)\n",
    "\n",
    "        if score > 0.15:\n",
    "            label = \"positive\"\n",
    "        elif score < -0.15:\n",
    "            label = \"negative\"\n",
    "        else:\n",
    "            label = \"neutral\"\n",
    "\n",
    "        results.append((score, label))\n",
    "\n",
    "    return results\n",
    "\n",
    "# ============================================================================\n",
    "# RUN TWITTER ROBERTA\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"RUNNING TWITTER ROBERTA\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "tw_scores = []\n",
    "tw_labels = []\n",
    "\n",
    "texts = df[TEXT_COL].fillna(\"\").tolist()\n",
    "tickers = df[\"ticker\"].tolist()\n",
    "\n",
    "for i in tqdm(range(0, len(texts), BATCH_SIZE_CLS), desc=\"Twitter RoBERTa\"):\n",
    "    batch_texts = texts[i:i+BATCH_SIZE_CLS]\n",
    "    batch_tickers = tickers[i:i+BATCH_SIZE_CLS]\n",
    "    res = twitter_batch(batch_texts, batch_tickers)\n",
    "    for s,l in res:\n",
    "        tw_scores.append(s)\n",
    "        tw_labels.append(l)\n",
    "\n",
    "df[\"tw_score\"] = tw_scores\n",
    "df[\"tw_label\"] = tw_labels\n",
    "\n",
    "print(f\"\\nTwitter-RoBERTa Results:\")\n",
    "print(f\"  Label distribution:\\n{df['tw_label'].value_counts()}\")\n",
    "print(f\"  Score stats: mean={df['tw_score'].mean():.3f}, std={df['tw_score'].std():.3f}\")\n",
    "\n",
    "# ============================================================================\n",
    "# SELECT UNCERTAIN FOR LLM\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SELECTING UNCERTAIN CASES FOR LLM\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Uncertain: tw_score between -0.1 and 0.1\n",
    "df_llm = df[df[\"tw_score\"].between(-0.1, 0.1)].copy()\n",
    "print(f\"Uncertain cases (tw_score ∈ [-0.1, 0.1]): {len(df_llm):,} rows ({len(df_llm)/len(df)*100:.1f}%)\")\n",
    "\n",
    "if len(df_llm) == 0:\n",
    "    print(\"⚠️ No uncertain cases! Skipping LLM.\")\n",
    "    df[\"llm_sentiment_label\"] = np.nan\n",
    "    df[\"llm_sentiment_score\"] = np.nan\n",
    "    df.to_csv(OUTPUT_FILE, index=False)\n",
    "    print(f\"✓ Saved to {OUTPUT_FILE}\")\n",
    "else:\n",
    "    # ============================================================================\n",
    "    # LOAD QWEN LLM\n",
    "    # ============================================================================\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"LOADING QWEN LLM\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    llm_tokenizer = AutoTokenizer.from_pretrained(LLM_MODEL_NAME)\n",
    "    llm_model = AutoModelForCausalLM.from_pretrained(\n",
    "        LLM_MODEL_NAME,\n",
    "        torch_dtype=torch.float16 if device==\"cuda\" else None,\n",
    "        device_map=\"auto\" if device==\"cuda\" else None\n",
    "    )\n",
    "    \n",
    "    llm_model.eval()\n",
    "    \n",
    "    if llm_tokenizer.pad_token is None:\n",
    "        llm_tokenizer.pad_token = llm_tokenizer.eos_token\n",
    "    \n",
    "    print(\"✓ Qwen loaded\")\n",
    "    \n",
    "    # ============================================================================\n",
    "    # IMPROVED PROMPT (KEY FIX!)\n",
    "    # ============================================================================\n",
    "    \n",
    "    def build_prompt(text, ticker):\n",
    "        \"\"\"\n",
    "        Better prompt with:\n",
    "        1. More diverse score examples (not just -0.8, 0, 0.8)\n",
    "        2. Clear instructions about continuous scale\n",
    "        3. Emphasis on nuance\n",
    "        \"\"\"\n",
    "        return f\"\"\"You are analyzing stock market sentiment. Rate sentiment from -1.0 (very negative) to +1.0 (very positive).\n",
    "\n",
    "Examples with various scores:\n",
    "\n",
    "Text: \"NVDA absolutely crushing it, best quarter ever!\"\n",
    "Ticker: NVDA\n",
    "Score: 0.95 (very positive)\n",
    "\n",
    "Text: \"TSLA down but might recover, who knows\"\n",
    "Ticker: TSLA\n",
    "Score: -0.2 (slightly negative, uncertain)\n",
    "\n",
    "Text: \"AAPL sideways, nothing happening\"\n",
    "Ticker: AAPL\n",
    "Score: 0.05 (neutral, slightly positive)\n",
    "\n",
    "Text: \"MSFT looks concerning, sales dropping\"\n",
    "Ticker: MSFT\n",
    "Score: -0.6 (negative)\n",
    "\n",
    "Text: \"AMD decent earnings, pretty good results\"\n",
    "Ticker: AMD\n",
    "Score: 0.5 (positive)\n",
    "\n",
    "Now analyze this text. Give a score from -1.0 to +1.0:\n",
    "\n",
    "Text: \"{text[:200]}\"\n",
    "Ticker: {ticker}\n",
    "\n",
    "Respond ONLY with a number from -1.0 to +1.0, nothing else.\"\"\"\n",
    "\n",
    "    # ============================================================================\n",
    "    # IMPROVED PARSING (KEY FIX!)\n",
    "    # ============================================================================\n",
    "    \n",
    "    def parse_llm_response(text):\n",
    "        \"\"\"\n",
    "        Better parsing with multiple strategies and validation.\n",
    "        \"\"\"\n",
    "        text = str(text).strip()\n",
    "        \n",
    "        # Strategy 1: Find any decimal number\n",
    "        number_match = re.search(r'(-?\\d*\\.?\\d+)', text)\n",
    "        if number_match:\n",
    "            try:\n",
    "                score = float(number_match.group(1))\n",
    "                \n",
    "                # Clamp to [-1, 1]\n",
    "                score = max(-1.0, min(1.0, score))\n",
    "                \n",
    "                # Determine label\n",
    "                if score > 0.15:\n",
    "                    label = \"positive\"\n",
    "                elif score < -0.15:\n",
    "                    label = \"negative\"\n",
    "                else:\n",
    "                    label = \"neutral\"\n",
    "                \n",
    "                return label, score\n",
    "            except ValueError:\n",
    "                pass\n",
    "        \n",
    "        # Strategy 2: Look for sentiment words as fallback\n",
    "        text_lower = text.lower()\n",
    "        \n",
    "        if any(word in text_lower for word in ['very positive', 'extremely positive', 'bullish']):\n",
    "            return \"positive\", 0.8\n",
    "        elif any(word in text_lower for word in ['positive', 'good']):\n",
    "            return \"positive\", 0.5\n",
    "        elif any(word in text_lower for word in ['very negative', 'extremely negative', 'bearish']):\n",
    "            return \"negative\", -0.8\n",
    "        elif any(word in text_lower for word in ['negative', 'bad']):\n",
    "            return \"negative\", -0.5\n",
    "        \n",
    "        # Strategy 3: Default to slight neutral with randomness\n",
    "        # (avoids clustering at exactly 0)\n",
    "        import random\n",
    "        return \"neutral\", random.uniform(-0.05, 0.05)\n",
    "    \n",
    "    # ============================================================================\n",
    "    # RUN LLM WITH BETTER GENERATION PARAMS\n",
    "    # ============================================================================\n",
    "    \n",
    "    def run_llm_batch(texts, tickers):\n",
    "        \"\"\"Run LLM with improved generation parameters.\"\"\"\n",
    "        prompts = [build_prompt(t, tic) for t, tic in zip(texts, tickers)]\n",
    "    \n",
    "        inputs = llm_tokenizer(\n",
    "            prompts, \n",
    "            return_tensors=\"pt\",\n",
    "            padding=True, \n",
    "            truncation=True,\n",
    "            max_length=MAX_INPUT_TOKENS\n",
    "        ).to(device)\n",
    "    \n",
    "        with torch.no_grad():\n",
    "            outputs = llm_model.generate(\n",
    "                **inputs, \n",
    "                max_new_tokens=MAX_NEW_TOKENS,\n",
    "                do_sample=False,  # Greedy for consistency\n",
    "                temperature=1.0,\n",
    "                top_p=1.0,\n",
    "                pad_token_id=llm_tokenizer.pad_token_id\n",
    "            )\n",
    "        \n",
    "        # Decode only NEW tokens (skip the prompt)\n",
    "        decoded_outputs = []\n",
    "        for i, output in enumerate(outputs):\n",
    "            prompt_length = inputs['input_ids'][i].shape[0]\n",
    "            new_tokens = output[prompt_length:]\n",
    "            decoded = llm_tokenizer.decode(new_tokens, skip_special_tokens=True)\n",
    "            decoded_outputs.append(decoded)\n",
    "        \n",
    "        return [parse_llm_response(o) for o in decoded_outputs]\n",
    "    \n",
    "    # ============================================================================\n",
    "    # RUN LLM\n",
    "    # ============================================================================\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"RUNNING LLM ON UNCERTAIN CASES\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    llm_labels = []\n",
    "    llm_scores = []\n",
    "    llm_raw_responses = []\n",
    "    \n",
    "    texts_llm = df_llm[TEXT_COL].tolist()\n",
    "    tickers_llm = df_llm[\"ticker\"].tolist()\n",
    "    \n",
    "    for i in tqdm(range(0, len(texts_llm), BATCH_SIZE_LLM), desc=\"LLM\"):\n",
    "        batch_texts = texts_llm[i:i+BATCH_SIZE_LLM]\n",
    "        batch_tickers = tickers_llm[i:i+BATCH_SIZE_LLM]\n",
    "        \n",
    "        try:\n",
    "            res = run_llm_batch(batch_texts, batch_tickers)\n",
    "            \n",
    "            for label, score in res:\n",
    "                llm_labels.append(label)\n",
    "                llm_scores.append(score)\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"\\nError at batch {i}: {e}\")\n",
    "            # Fill with slight neutral\n",
    "            import random\n",
    "            for _ in range(len(batch_texts)):\n",
    "                llm_labels.append(\"neutral\")\n",
    "                llm_scores.append(random.uniform(-0.05, 0.05))\n",
    "    \n",
    "    df_llm[\"llm_sentiment_label\"] = llm_labels\n",
    "    df_llm[\"llm_sentiment_score\"] = llm_scores\n",
    "    \n",
    "    # ============================================================================\n",
    "    # MERGE BACK\n",
    "    # ============================================================================\n",
    "    \n",
    "    df[\"llm_sentiment_label\"] = np.nan\n",
    "    df[\"llm_sentiment_score\"] = np.nan\n",
    "    \n",
    "    df.loc[df_llm.index, \"llm_sentiment_label\"] = df_llm[\"llm_sentiment_label\"]\n",
    "    df.loc[df_llm.index, \"llm_sentiment_score\"] = df_llm[\"llm_sentiment_score\"]\n",
    "    \n",
    "    # ============================================================================\n",
    "    # ANALYSIS\n",
    "    # ============================================================================\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"LLM RESULTS ANALYSIS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    print(f\"\\nLLM Label Distribution:\")\n",
    "    print(df_llm[\"llm_sentiment_label\"].value_counts())\n",
    "    \n",
    "    print(f\"\\nLLM Score Statistics:\")\n",
    "    print(df_llm[\"llm_sentiment_score\"].describe())\n",
    "    \n",
    "    print(f\"\\nLLM Score Diversity:\")\n",
    "    unique_scores = df_llm[\"llm_sentiment_score\"].nunique()\n",
    "    print(f\"  Unique values: {unique_scores} out of {len(df_llm)}\")\n",
    "    \n",
    "    if unique_scores < 10:\n",
    "        print(\"  ⚠️ Low diversity - showing most common scores:\")\n",
    "        print(df_llm[\"llm_sentiment_score\"].value_counts().head(10))\n",
    "    else:\n",
    "        print(\"  ✓ Good diversity!\")\n",
    "    \n",
    "    # Show examples\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"LLM EXAMPLES\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    print(\"\\nMost positive by LLM:\")\n",
    "    for _, row in df_llm.nlargest(3, \"llm_sentiment_score\").iterrows():\n",
    "        print(f\"\\n  Score: {row['llm_sentiment_score']:+.2f} | Ticker: {row['ticker']}\")\n",
    "        print(f\"  Twitter: {row['tw_score']:+.2f} (uncertain)\")\n",
    "        print(f\"  Text: {row[TEXT_COL][:100]}...\")\n",
    "    \n",
    "    print(\"\\nMost negative by LLM:\")\n",
    "    for _, row in df_llm.nsmallest(3, \"llm_sentiment_score\").iterrows():\n",
    "        print(f\"\\n  Score: {row['llm_sentiment_score']:+.2f} | Ticker: {row['ticker']}\")\n",
    "        print(f\"  Twitter: {row['tw_score']:+.2f} (uncertain)\")\n",
    "        print(f\"  Text: {row[TEXT_COL][:100]}...\")\n",
    "    \n",
    "    # ============================================================================\n",
    "    # SAVE\n",
    "    # ============================================================================\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"SAVING\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    df.to_csv(OUTPUT_FILE, index=False)\n",
    "    print(f\"✓ Saved to {OUTPUT_FILE}\")\n",
    "    \n",
    "    # ============================================================================\n",
    "    # SUMMARY\n",
    "    # ============================================================================\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"PIPELINE SUMMARY\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    print(f\"\\nTotal rows: {len(df):,}\")\n",
    "    print(f\"\\nTwitter-RoBERTa (all rows):\")\n",
    "    print(f\"  Positive: {(df['tw_label'] == 'positive').sum():,}\")\n",
    "    print(f\"  Neutral:  {(df['tw_label'] == 'neutral').sum():,}\")\n",
    "    print(f\"  Negative: {(df['tw_label'] == 'negative').sum():,}\")\n",
    "    \n",
    "    print(f\"\\nLLM (uncertain cases only):\")\n",
    "    print(f\"  Processed: {len(df_llm):,}\")\n",
    "    print(f\"  Positive: {(df_llm['llm_sentiment_label'] == 'positive').sum():,}\")\n",
    "    print(f\"  Neutral:  {(df_llm['llm_sentiment_label'] == 'neutral').sum():,}\")\n",
    "    print(f\"  Negative: {(df_llm['llm_sentiment_label'] == 'negative').sum():,}\")\n",
    "    \n",
    "    print(\"\\n✓ Complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "96ee3b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df=pd.read_csv('sentiment_hybrid_twitter_llm.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3391f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
