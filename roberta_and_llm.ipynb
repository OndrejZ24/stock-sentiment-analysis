{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05435721",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'utils'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdotenv\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_dotenv\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_oracle_connection\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(sys\u001b[38;5;241m.\u001b[39mversion)\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'utils'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "from utils import get_oracle_connection\n",
    "import sys\n",
    "print(sys.version)\n",
    "import torch\n",
    "print(\"Torch version:\", torch.__version__)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoModelForCausalLM\n",
    ")\n",
    "from tqdm import tqdm\n",
    "import ast\n",
    "import numpy as np\n",
    "import json\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eebb41f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oracle connection successful!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vitek\\AppData\\Local\\Temp\\ipykernel_5372\\4140934876.py:22: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql_query(query, conn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 2000 rows\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "id",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "sentiment_ready_text",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "type",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "subreddit",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "created_utc",
         "rawType": "datetime64[ns]",
         "type": "datetime"
        },
        {
         "name": "normalized_upvotes",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "mentioned_tickers",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "n_tickers",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "text_length",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "word_count",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "hour",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "day_of_week",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "date",
         "rawType": "datetime64[ns]",
         "type": "datetime"
        }
       ],
       "ref": "5152b293-de9c-4186-96fc-913afc10a5c1",
       "rows": [
        [
         "0",
         "lqm91bb",
         "POOR THING BUT THIS DOESN t SOUND GOOD ANY IDEA WHAT SHE IS paying TO borrow margin almost always HAS a COST AND AS someone WHO HAS worked WITH MANY financial advisors almost NONE OF THEM WOULD suggest margin 1 RISKY FOR IF stocks GO DOWN 2 SUPER expensive interest RATE AT MOST FIRMS 3 advanced investing approach MOST individual investors wouldn t understand IT IS possible YOU COULD STILL PULL MONEY OUT a NEG margin balance IS common IN THE WAY reported FOR example ONE CAN HAVE 50k IN equity IN AN account 30k margin FOR a TOTAL account VALUE OF 80k THE 50 IS THE amount i HAVE OR equity AND MY CASH balance WOULD SAY 30k AT LEAST AT a PLACE LIKE IBKR IF ONE SOLD ALL positions 30 k OR THE 80k WOULD GO BACK TO PAY DOWN margin TO ZERO THEN ANY LEFT OVER OR THE 50k WOULD GO BACK TO ME IN MY account LONG STORY OF YOU MAY NOT BE DOWN AS MUCH AS YOU THINK BUT investing IN precious metals OFF THE BAT AND HIGH pressure SALES tactics IS a BAD SIGN DEF consider pulling WHEN ABLE AND MOVE TO a reputable broker LIKE schwab OR fidelity WHICH HAVE SOME LOWER COST managed BY professional options GOOD LUCK",
         "comment",
         "investing",
         "2024-10-06 14:09:50",
         "0.12309116125106812",
         "IBKR",
         "1",
         "1141",
         "215",
         "14",
         "6",
         "2024-10-06 00:00:00"
        ],
        [
         "1",
         "lqm90lu",
         "ROKU THEY VE DONE nothing BUT expand market SHARE AND profit vertices EVEN domestically SINCE THE 2022 BEAR market AND ARE trading AT a p s 1 20th THE rediculous 2021 DAYS THEIR product BRAND expansion PLAN AND margins ARE ALL completely IN TACT AND improving leaving IT SET UP FOR explosive growth AS THE VIDEO AD market continues TO recover SO FEW people KNOW anything logical ABOUT THE company OR sector leaving IT mispriced IMO ONE OF MY favorite companies OF ALL TIME",
         "comment",
         "stocks",
         "2024-10-06 14:09:43",
         "0.05091093108057976",
         "ASML",
         "1",
         "483",
         "81",
         "14",
         "6",
         "2024-10-06 00:00:00"
        ],
        [
         "2",
         "lqm8vkv",
         "BMBL LOTS OF people KNOW IT AND HAVE USED IT BUT NO ONE IS looking AT THE STOCK because IT HAS BEEN a falling KNIFE IT s trading AT 800m market CAP BUT pulling IN 1bn OF revenue WITH HIGH margins AND CASH FLOW buyback YIELD IS 20 AND THEY ARE STILL growing TOP LINE MTCH IS ALSO looking GOOD AT THESE prices FOR similar reasons dating AND social networking APPS AREN t GOING AWAY AND people WANT something NOT TIED TO facebook IG",
         "comment",
         "stocks",
         "2024-10-06 14:08:53",
         "0.05091093108057976",
         "ASML,BMBL,MTCH",
         "3",
         "439",
         "81",
         "14",
         "6",
         "2024-10-06 00:00:00"
        ],
        [
         "3",
         "lqm8tfg",
         "i VE BEEN BAG holding chinese stocks SINCE EARLY pandemic THIS IS a GREAT TIME TO GET OUT lesson learned",
         "comment",
         "ValueInvesting",
         "2024-10-06 14:08:31",
         "0.054083287715911865",
         "BABA,PDD",
         "2",
         "107",
         "19",
         "14",
         "6",
         "2024-10-06 00:00:00"
        ],
        [
         "4",
         "lqm8pgy",
         "please DON t assume anyone CAN JUST WORK PART TIME AT a community college graduate schools ARE cranking OUT WAY TOO MANY masters AND doctorate graduates AND neither industry NOR academia CAN absorb THEM ALL AT THE CC WHERE i WORK WE rarely HAVE adjunct openings AND IF WE DO WE GET dozens OF applicants IT s CRAZY",
         "comment",
         "financialindependence",
         "2024-10-06 14:07:53",
         "0.27809447050094604",
         "CC",
         "1",
         "320",
         "56",
         "14",
         "6",
         "2024-10-06 00:00:00"
        ]
       ],
       "shape": {
        "columns": 13,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sentiment_ready_text</th>\n",
       "      <th>type</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>normalized_upvotes</th>\n",
       "      <th>mentioned_tickers</th>\n",
       "      <th>n_tickers</th>\n",
       "      <th>text_length</th>\n",
       "      <th>word_count</th>\n",
       "      <th>hour</th>\n",
       "      <th>day_of_week</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>lqm91bb</td>\n",
       "      <td>POOR THING BUT THIS DOESN t SOUND GOOD ANY IDE...</td>\n",
       "      <td>comment</td>\n",
       "      <td>investing</td>\n",
       "      <td>2024-10-06 14:09:50</td>\n",
       "      <td>0.123091</td>\n",
       "      <td>IBKR</td>\n",
       "      <td>1</td>\n",
       "      <td>1141</td>\n",
       "      <td>215</td>\n",
       "      <td>14</td>\n",
       "      <td>6</td>\n",
       "      <td>2024-10-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>lqm90lu</td>\n",
       "      <td>ROKU THEY VE DONE nothing BUT expand market SH...</td>\n",
       "      <td>comment</td>\n",
       "      <td>stocks</td>\n",
       "      <td>2024-10-06 14:09:43</td>\n",
       "      <td>0.050911</td>\n",
       "      <td>ASML</td>\n",
       "      <td>1</td>\n",
       "      <td>483</td>\n",
       "      <td>81</td>\n",
       "      <td>14</td>\n",
       "      <td>6</td>\n",
       "      <td>2024-10-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>lqm8vkv</td>\n",
       "      <td>BMBL LOTS OF people KNOW IT AND HAVE USED IT B...</td>\n",
       "      <td>comment</td>\n",
       "      <td>stocks</td>\n",
       "      <td>2024-10-06 14:08:53</td>\n",
       "      <td>0.050911</td>\n",
       "      <td>ASML,BMBL,MTCH</td>\n",
       "      <td>3</td>\n",
       "      <td>439</td>\n",
       "      <td>81</td>\n",
       "      <td>14</td>\n",
       "      <td>6</td>\n",
       "      <td>2024-10-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>lqm8tfg</td>\n",
       "      <td>i VE BEEN BAG holding chinese stocks SINCE EAR...</td>\n",
       "      <td>comment</td>\n",
       "      <td>ValueInvesting</td>\n",
       "      <td>2024-10-06 14:08:31</td>\n",
       "      <td>0.054083</td>\n",
       "      <td>BABA,PDD</td>\n",
       "      <td>2</td>\n",
       "      <td>107</td>\n",
       "      <td>19</td>\n",
       "      <td>14</td>\n",
       "      <td>6</td>\n",
       "      <td>2024-10-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>lqm8pgy</td>\n",
       "      <td>please DON t assume anyone CAN JUST WORK PART ...</td>\n",
       "      <td>comment</td>\n",
       "      <td>financialindependence</td>\n",
       "      <td>2024-10-06 14:07:53</td>\n",
       "      <td>0.278094</td>\n",
       "      <td>CC</td>\n",
       "      <td>1</td>\n",
       "      <td>320</td>\n",
       "      <td>56</td>\n",
       "      <td>14</td>\n",
       "      <td>6</td>\n",
       "      <td>2024-10-06</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                               sentiment_ready_text     type  \\\n",
       "0  lqm91bb  POOR THING BUT THIS DOESN t SOUND GOOD ANY IDE...  comment   \n",
       "1  lqm90lu  ROKU THEY VE DONE nothing BUT expand market SH...  comment   \n",
       "2  lqm8vkv  BMBL LOTS OF people KNOW IT AND HAVE USED IT B...  comment   \n",
       "3  lqm8tfg  i VE BEEN BAG holding chinese stocks SINCE EAR...  comment   \n",
       "4  lqm8pgy  please DON t assume anyone CAN JUST WORK PART ...  comment   \n",
       "\n",
       "               subreddit         created_utc  normalized_upvotes  \\\n",
       "0              investing 2024-10-06 14:09:50            0.123091   \n",
       "1                 stocks 2024-10-06 14:09:43            0.050911   \n",
       "2                 stocks 2024-10-06 14:08:53            0.050911   \n",
       "3         ValueInvesting 2024-10-06 14:08:31            0.054083   \n",
       "4  financialindependence 2024-10-06 14:07:53            0.278094   \n",
       "\n",
       "  mentioned_tickers  n_tickers  text_length  word_count  hour  day_of_week  \\\n",
       "0              IBKR          1         1141         215    14            6   \n",
       "1              ASML          1          483          81    14            6   \n",
       "2    ASML,BMBL,MTCH          3          439          81    14            6   \n",
       "3          BABA,PDD          2          107          19    14            6   \n",
       "4                CC          1          320          56    14            6   \n",
       "\n",
       "        date  \n",
       "0 2024-10-06  \n",
       "1 2024-10-06  \n",
       "2 2024-10-06  \n",
       "3 2024-10-06  \n",
       "4 2024-10-06  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conn = get_oracle_connection()\n",
    "\n",
    "query = \"\"\"\n",
    "SELECT\n",
    "    ID,\n",
    "    DBMS_LOB.SUBSTR(SENTIMENT_READY_TEXT, 20000, 1) as SENTIMENT_READY_TEXT,\n",
    "    TYPE,\n",
    "    SUBREDDIT,\n",
    "    CREATED_UTC,\n",
    "    NORMALIZED_UPVOTES,\n",
    "    DBMS_LOB.SUBSTR(MENTIONED_TICKERS, 100, 1) as MENTIONED_TICKERS,\n",
    "    N_TICKERS,\n",
    "    TEXT_LENGTH,\n",
    "    WORD_COUNT,\n",
    "    DATE_COL,\n",
    "    HOUR,\n",
    "    DAY_OF_WEEK\n",
    "FROM preprocessed_data\n",
    "FETCH FIRST 2000 ROWS ONLY\n",
    "\"\"\"\n",
    "\n",
    "df = pd.read_sql_query(query, conn)\n",
    "conn.close()\n",
    "\n",
    "df.columns = df.columns.str.lower()\n",
    "df['created_utc'] = pd.to_datetime(df['created_utc'], unit='s')\n",
    "if 'date_col' in df.columns:\n",
    "    df['date'] = pd.to_datetime(df['date_col'])\n",
    "    df.drop(columns=['date_col'], inplace=True)\n",
    "\n",
    "print(f\"Loaded {len(df)} rows\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60f36b2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vitek\\AppData\\Roaming\\Python\\Python313\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "HYBRID PIPELINE: Drop<0.4 | LLM 0.4-0.65 | RoBERTa >0.65\n",
      "================================================================================\n",
      "Device: cuda\n",
      "Total rows to process: 3,993\n",
      "\n",
      "Loading Twitter-RoBERTa...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Loaded\n",
      "Running RoBERTa...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "RoBERTa: 100%|██████████| 63/63 [01:25<00:00,  1.35s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "APPLYING SMART FILTERS\n",
      "================================================================================\n",
      "1. Dropped (Confidence < 0.4): 4 rows\n",
      "2. Sent to LLM (0.4 <= Conf < 0.65): 1,427 rows\n",
      "3. Kept RoBERTa (Confidence >= 0.65): 2,562 rows\n",
      "   -> New Dataset Size: 3,989 (was 3,993)\n",
      "\n",
      "================================================================================\n",
      "LOADING QWEN LLM\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "INFO:accelerate.utils.modeling:We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Qwen loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LLM Inference:   0%|          | 0/23 [00:00<?, ?it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLM Inference:   4%|▍         | 1/23 [03:47<1:23:33, 227.88s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLM Inference:   9%|▊         | 2/23 [07:23<1:17:15, 220.76s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLM Inference:  13%|█▎        | 3/23 [11:20<1:15:58, 227.95s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLM Inference:  17%|█▋        | 4/23 [15:02<1:11:31, 225.89s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLM Inference:  22%|██▏       | 5/23 [18:59<1:08:52, 229.61s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLM Inference:  26%|██▌       | 6/23 [23:15<1:07:37, 238.66s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLM Inference:  30%|███       | 7/23 [27:01<1:02:33, 234.57s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLM Inference:  35%|███▍      | 8/23 [30:45<57:46, 231.13s/it]  A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLM Inference:  39%|███▉      | 9/23 [34:27<53:16, 228.33s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLM Inference:  43%|████▎     | 10/23 [37:59<48:22, 223.29s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLM Inference:  48%|████▊     | 11/23 [41:43<44:40, 223.38s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLM Inference:  52%|█████▏    | 12/23 [45:29<41:06, 224.23s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLM Inference:  57%|█████▋    | 13/23 [49:09<37:11, 223.17s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLM Inference:  61%|██████    | 14/23 [51:44<30:22, 202.45s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLM Inference:  65%|██████▌   | 15/23 [56:00<29:08, 218.55s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLM Inference:  70%|██████▉   | 16/23 [59:01<24:11, 207.36s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLM Inference:  74%|███████▍  | 17/23 [1:01:33<19:03, 190.56s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLM Inference:  78%|███████▊  | 18/23 [1:05:45<17:25, 209.05s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLM Inference:  83%|████████▎ | 19/23 [1:08:21<12:52, 193.03s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLM Inference:  87%|████████▋ | 20/23 [1:12:02<10:04, 201.56s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLM Inference:  91%|█████████▏| 21/23 [1:15:39<06:52, 206.24s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLM Inference:  96%|█████████▌| 22/23 [1:18:51<03:21, 201.94s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLM Inference: 100%|██████████| 23/23 [1:19:39<00:00, 207.80s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "SAVING\n",
      "================================================================================\n",
      "✓ Saved to sentiment_hybrid_twitter_llm.csv\n",
      "  - RoBERTa rows: 2,562\n",
      "  - LLM rows:     1,427\n",
      "  - Dropped rows: 4\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "OPTIMIZED HYBRID SENTIMENT PIPELINE\n",
    "- RoBERTa for scoring\n",
    "- Qwen 1.5B for reasoning\n",
    "- With filtering logic to minimize LLM calls and drop noise\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoModelForCausalLM\n",
    ")\n",
    "from tqdm import tqdm\n",
    "import ast\n",
    "import numpy as np\n",
    "import json\n",
    "import re\n",
    "\n",
    "# ============================================================================\n",
    "# CONFIG\n",
    "# ============================================================================\n",
    "\n",
    "OUTPUT_FILE = \"sentiment_hybrid_twitter_llm.csv\"\n",
    "\n",
    "TEXT_COL   = \"sentiment_ready_text\"\n",
    "TICKER_COL = \"mentioned_tickers\"\n",
    "\n",
    "TW_MODEL_NAME  = \"cardiffnlp/twitter-roberta-base-sentiment-latest\"\n",
    "LLM_MODEL_NAME = \"Qwen/Qwen2.5-1.5B-Instruct\"\n",
    "\n",
    "BATCH_SIZE_CLS = 64\n",
    "BATCH_SIZE_LLM = 64\n",
    "\n",
    "# DECOUPLED TOKEN LIMITS (Crucial Fix)\n",
    "ROBERTA_MAX_TOKENS = 512  # Model limit\n",
    "LLM_MAX_TOKENS     = 1500 # Expanded context for reasoning\n",
    "MAX_NEW_TOKENS     = 128  # Output length for JSON\n",
    "\n",
    "# LOGIC THRESHOLDS\n",
    "DROP_THRESHOLD = 0.50  # Drop rows below this\n",
    "LLM_THRESHOLD  = 0.65  # Send rows below but aboive DROP to LLM\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(f\"HYBRID PIPELINE: Drop<{DROP_THRESHOLD} | LLM {DROP_THRESHOLD}-{LLM_THRESHOLD} | RoBERTa >{LLM_THRESHOLD}\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# ============================================================================\n",
    "# DEVICE - to use GPU\n",
    "# ============================================================================\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Device:\", device)\n",
    "\n",
    "# ============================================================================\n",
    "# DATA PREPARATION\n",
    "# ============================================================================\n",
    "\n",
    "def parse_tickers(x):\n",
    "    if pd.isna(x) or x == \"\":\n",
    "        return []\n",
    "    try:\n",
    "        return [str(t).strip() for t in ast.literal_eval(str(x)) if t]\n",
    "    except:\n",
    "        return [t.strip() for t in str(x).split(\",\") if t.strip()]\n",
    "\n",
    "# Ensure we start clean\n",
    "if \"tickers_list\" not in df.columns:\n",
    "    df[\"tickers_list\"] = df[TICKER_COL].apply(parse_tickers)\n",
    "    df[\"n_tickers\"] = df[\"tickers_list\"].apply(len)\n",
    "    df = df[df[\"n_tickers\"] > 0].copy()\n",
    "    df = df.explode(\"tickers_list\").reset_index(drop=True)\n",
    "    df = df.rename(columns={\"tickers_list\": \"ticker\"})\n",
    "\n",
    "print(f\"Total rows to process: {len(df):,}\")\n",
    "\n",
    "# ============================================================================\n",
    "# 1.RoBERTa scoring\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\nLoading Twitter-RoBERTa...\")\n",
    "tw_tokenizer = AutoTokenizer.from_pretrained(TW_MODEL_NAME)\n",
    "tw_model     = AutoModelForSequenceClassification.from_pretrained(TW_MODEL_NAME)\n",
    "tw_model.to(device)\n",
    "tw_model.eval()\n",
    "print(\"✓ Loaded\")\n",
    "\n",
    "def twitter_batch(texts, tickers):\n",
    "    inputs = [f\"{tic}: {txt}\" for txt, tic in zip(texts, tickers)]\n",
    "\n",
    "    enc = tw_tokenizer(\n",
    "        inputs,\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=ROBERTA_MAX_TOKENS, # Fixed 512 limit\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        probs = torch.softmax(tw_model(**enc).logits, dim=-1).cpu().numpy()\n",
    "\n",
    "    results = []\n",
    "    for prob in probs:\n",
    "        p_neg, p_neu, p_pos = prob[0], prob[1], prob[2]\n",
    "        \n",
    "        # Weighted Score\n",
    "        score = (p_pos * 1.0) + (p_neu * 0.0) + (p_neg * -1.0)\n",
    "        \n",
    "        # Confidence = Max probability\n",
    "        confidence = max(p_pos, p_neu, p_neg)\n",
    "        \n",
    "        if p_pos > p_neg and p_pos > p_neu: label = \"positive\"\n",
    "        elif p_neg > p_pos and p_neg > p_neu: label = \"negative\"\n",
    "        else: label = \"neutral\"\n",
    "\n",
    "        results.append({\n",
    "            \"score\": score,\n",
    "            \"label\": label,\n",
    "            \"confidence\": confidence,\n",
    "            \"p_pos\": p_pos, \"p_neg\": p_neg, \"p_neu\": p_neu\n",
    "        })\n",
    "\n",
    "    return results\n",
    "\n",
    "# Run RoBERTa\n",
    "tw_results = []\n",
    "texts = df[TEXT_COL].fillna(\"\").tolist()\n",
    "tickers = df[\"ticker\"].tolist()\n",
    "\n",
    "print(\"Running RoBERTa...\")\n",
    "for i in tqdm(range(0, len(texts), BATCH_SIZE_CLS), desc=\"RoBERTa\"):\n",
    "    batch_texts = texts[i:i+BATCH_SIZE_CLS]\n",
    "    batch_tickers = tickers[i:i+BATCH_SIZE_CLS]\n",
    "    tw_results.extend(twitter_batch(batch_texts, batch_tickers))\n",
    "\n",
    "# Attach results\n",
    "df[\"tw_score\"] = [r[\"score\"] for r in tw_results]\n",
    "df[\"tw_label\"] = [r[\"label\"] for r in tw_results]\n",
    "df[\"tw_confidence\"] = [r[\"confidence\"] for r in tw_results]\n",
    "df[\"tw_p_pos\"] = [r[\"p_pos\"] for r in tw_results]\n",
    "df[\"tw_p_neg\"] = [r[\"p_neg\"] for r in tw_results]\n",
    "df[\"tw_p_neu\"] = [r[\"p_neu\"] for r in tw_results]\n",
    "\n",
    "# ============================================================================\n",
    "# 2. Filtering RoBERTa results\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"APPLYING SMART FILTERS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "initial_count = len(df)\n",
    "\n",
    "# drop below DROP_THRESHOLD\n",
    "df_dropped = df[df[\"tw_confidence\"] < DROP_THRESHOLD]\n",
    "df = df[df[\"tw_confidence\"] >= DROP_THRESHOLD].copy()\n",
    "\n",
    "print(f\"1. Dropped (Confidence < {DROP_THRESHOLD}): {len(df_dropped):,} rows\")\n",
    "\n",
    "# Set llm subset for reevaluation\n",
    "df_uncertain = df[df[\"tw_confidence\"] < LLM_THRESHOLD].copy()\n",
    "print(f\"2. Sent to LLM ({DROP_THRESHOLD} <= Conf < {LLM_THRESHOLD}): {len(df_uncertain):,} rows\")\n",
    "\n",
    "# Keep confident above LLM_THRESHOLD\n",
    "df_confident = df[df[\"tw_confidence\"] >= LLM_THRESHOLD].copy()\n",
    "print(f\"3. Kept RoBERTa (Confidence >= {LLM_THRESHOLD}): {len(df_confident):,} rows\")\n",
    "\n",
    "print(f\"   -> New Dataset Size: {len(df):,} (was {initial_count:,})\")\n",
    "\n",
    "if len(df_uncertain) == 0:\n",
    "    print(\"No uncertain cases found. Saving...\")\n",
    "    df.to_csv(OUTPUT_FILE, index=False)\n",
    "    exit()\n",
    "\n",
    "# ============================================================================\n",
    "# 3. Qwen LLM for uncertain cases\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"LOADING QWEN LLM\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "llm_tokenizer = AutoTokenizer.from_pretrained(LLM_MODEL_NAME)\n",
    "llm_model = AutoModelForCausalLM.from_pretrained(\n",
    "    LLM_MODEL_NAME,\n",
    "    torch_dtype=torch.float16 if device==\"cuda\" else None,\n",
    "    device_map=\"auto\" if device==\"cuda\" else None\n",
    ")\n",
    "llm_model.eval()\n",
    "if llm_tokenizer.pad_token is None: llm_tokenizer.pad_token = llm_tokenizer.eos_token\n",
    "print(\"✓ Qwen loaded\")\n",
    "\n",
    "def build_prompt(text, ticker):\n",
    "    # Increased context window here to 1500\n",
    "    return f\"\"\"You are a financial sentiment expert.\n",
    "Analyze the sentiment of the text from a comment or post below regarding the ticker: {ticker}.\n",
    "\n",
    "Return a JSON object with:\n",
    "1. \"reasoning\": A brief explanation (max 15 words).\n",
    "2. \"sentiment\": \"Positive\", \"Negative\", or \"Neutral\".\n",
    "3. \"score\": A float between -1.0 (Very Negative) and 1.0 (Very Positive).\n",
    "\n",
    "Text: \"{text[:1500]}\"\n",
    "Ticker: {ticker}\n",
    "\n",
    "JSON Response:\"\"\"\n",
    "\n",
    "def parse_llm_response(text):\n",
    "    try:\n",
    "        match = re.search(r\"\\{.*\\}\", text, re.DOTALL)\n",
    "        if match:\n",
    "            data = json.loads(match.group(0))\n",
    "            return data.get(\"sentiment\", \"Neutral\").lower(), float(data.get(\"score\", 0.0))\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    # Fallback regex\n",
    "    match = re.search(r\"score\\\":\\s*(-?0\\.\\d+|1\\.0|-1\\.0|-?\\d+)\", text)\n",
    "    if match:\n",
    "        val = float(match.group(1))\n",
    "        label = \"positive\" if val > 0.1 else (\"negative\" if val < -0.1 else \"neutral\")\n",
    "        return label, val\n",
    "    return \"neutral\", 0.0\n",
    "\n",
    "def run_llm_batch(texts, tickers):\n",
    "    prompts = [build_prompt(t, tic) for t, tic in zip(texts, tickers)]\n",
    "    inputs = llm_tokenizer(\n",
    "        prompts, return_tensors=\"pt\", padding=True, truncation=True, \n",
    "        max_length=LLM_MAX_TOKENS # 1500 limit for LLM\n",
    "    ).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = llm_model.generate(\n",
    "            **inputs, max_new_tokens=MAX_NEW_TOKENS, do_sample=False,\n",
    "            pad_token_id=llm_tokenizer.pad_token_id\n",
    "        )\n",
    "    decoded = llm_tokenizer.batch_decode(outputs[:, inputs.input_ids.shape[1]:], skip_special_tokens=True)\n",
    "    return [parse_llm_response(d) for d in decoded]\n",
    "\n",
    "# Run LLM Loop\n",
    "llm_labels = []\n",
    "llm_scores = []\n",
    "texts_llm = df_uncertain[TEXT_COL].tolist()\n",
    "tickers_llm = df_uncertain[\"ticker\"].tolist()\n",
    "\n",
    "for i in tqdm(range(0, len(texts_llm), BATCH_SIZE_LLM), desc=\"LLM Inference\"):\n",
    "    b_texts = texts_llm[i:i+BATCH_SIZE_LLM]\n",
    "    b_tickers = tickers_llm[i:i+BATCH_SIZE_LLM]\n",
    "    try:\n",
    "        results = run_llm_batch(b_texts, b_tickers)\n",
    "        for l, s in results:\n",
    "            llm_labels.append(l)\n",
    "            llm_scores.append(s)\n",
    "    except Exception as e:\n",
    "        print(f\"Batch {i} error: {e}\")\n",
    "        llm_labels.extend([\"neutral\"] * len(b_texts))\n",
    "        llm_scores.extend([0.0] * len(b_texts))\n",
    "\n",
    "# ============================================================================\n",
    "# 4. MERGE & SAVE\n",
    "# ============================================================================\n",
    "\n",
    "# Default all to RoBERTa\n",
    "df[\"final_sentiment_label\"] = df[\"tw_label\"]\n",
    "df[\"final_sentiment_score\"] = df[\"tw_score\"]\n",
    "df[\"source_model\"] = \"RoBERTa\"\n",
    "\n",
    "# Overwrite LLM rows\n",
    "df.loc[df_uncertain.index, \"final_sentiment_label\"] = llm_labels\n",
    "df.loc[df_uncertain.index, \"final_sentiment_score\"] = llm_scores\n",
    "df.loc[df_uncertain.index, \"source_model\"] = \"LLM\"\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SAVING\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "df.to_csv(OUTPUT_FILE, index=False)\n",
    "print(f\"✓ Saved to {OUTPUT_FILE}\")\n",
    "print(f\"  - RoBERTa rows: {len(df[df['source_model']=='RoBERTa']):,}\")\n",
    "print(f\"  - LLM rows:     {len(df[df['source_model']=='LLM']):,}\")\n",
    "print(f\"  - Dropped rows: {len(df_dropped):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b762ec03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "FIXED HYBRID SENTIMENT PIPELINE\n",
      "================================================================================\n",
      "Device: cuda\n",
      "Per-ticker rows: 10,326\n",
      "\n",
      "Loading Twitter-RoBERTa...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Loaded\n",
      "\n",
      "================================================================================\n",
      "RUNNING TWITTER ROBERTA\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Twitter RoBERTa: 100%|██████████| 162/162 [23:51<00:00,  8.84s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Twitter-RoBERTa Results:\n",
      "  Label distribution:\n",
      "tw_label\n",
      "neutral     5461\n",
      "negative    2546\n",
      "positive    2319\n",
      "Name: count, dtype: int64\n",
      "  Avg Confidence: 0.719\n",
      "\n",
      "================================================================================\n",
      "SELECTING UNCERTAIN CASES FOR LLM\n",
      "================================================================================\n",
      "Total rows: 10326\n",
      "Confident rows: 9662\n",
      "Uncertain rows (to LLM): 664 (6.4%)\n",
      "\n",
      "================================================================================\n",
      "LOADING QWEN LLM\n",
      "================================================================================\n",
      "✓ Qwen loaded\n",
      "\n",
      "================================================================================\n",
      "RUNNING LLM ON UNCERTAIN CASES\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LLM Inference:   0%|          | 0/11 [00:00<?, ?it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLM Inference:   9%|▉         | 1/11 [05:09<51:35, 309.54s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLM Inference:  18%|█▊        | 2/11 [09:13<40:39, 271.02s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLM Inference:  27%|██▋       | 3/11 [13:44<36:08, 271.12s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLM Inference:  36%|███▋      | 4/11 [19:29<35:02, 300.34s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLM Inference:  45%|████▌     | 5/11 [23:40<28:13, 282.26s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLM Inference:  55%|█████▍    | 6/11 [26:50<20:55, 251.03s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLM Inference:  64%|██████▎   | 7/11 [30:07<15:33, 233.44s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLM Inference:  73%|███████▎  | 8/11 [33:16<10:57, 219.29s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLM Inference:  82%|████████▏ | 9/11 [36:18<06:55, 207.51s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLM Inference:  91%|█████████ | 10/11 [39:25<03:21, 201.22s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLM Inference: 100%|██████████| 11/11 [40:43<00:00, 222.11s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "SAVING FINAL RESULTS\n",
      "================================================================================\n",
      "✓ Saved to sentiment_hybrid_twitter_llm.csv\n",
      "  - RoBERTa rows: 9662\n",
      "  - LLM rows:     664\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "FIXED HYBRID SENTIMENT PIPELINE (RoBERTa + Qwen 1.5B)\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoModelForCausalLM\n",
    ")\n",
    "from tqdm import tqdm\n",
    "import ast\n",
    "import time\n",
    "import numpy as np\n",
    "import json\n",
    "import re\n",
    "\n",
    "# ============================================================================\n",
    "# CONFIG\n",
    "# ============================================================================\n",
    "\n",
    "OUTPUT_FILE = \"sentiment_hybrid_twitter_llm.csv\"\n",
    "\n",
    "TEXT_COL   = \"sentiment_ready_text\"\n",
    "TICKER_COL = \"mentioned_tickers\"\n",
    "\n",
    "TW_MODEL_NAME  = \"cardiffnlp/twitter-roberta-base-sentiment-latest\"\n",
    "LLM_MODEL_NAME = \"Qwen/Qwen2.5-1.5B-Instruct\"\n",
    "\n",
    "BATCH_SIZE_CLS = 64\n",
    "BATCH_SIZE_LLM = 64\n",
    "\n",
    "MAX_INPUT_TOKENS = 512 # cant really change (for roberta)\n",
    "\n",
    "MAX_NEW_TOKENS   = 128 # Increased for JSON reasoning\n",
    "CONFIDENCE_THRESHOLD = 0.5 # Confidence below this triggers LLM!!!!\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"FIXED HYBRID SENTIMENT PIPELINE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# ============================================================================\n",
    "# DEVICE\n",
    "# ============================================================================\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Device:\", device)\n",
    "\n",
    "# ============================================================================\n",
    "# DATA PREPARATION\n",
    "# ============================================================================\n",
    "# Processing rows for tickers\n",
    "def parse_tickers(x):\n",
    "    if pd.isna(x) or x == \"\":\n",
    "        return []\n",
    "    try:\n",
    "        return [str(t).strip() for t in ast.literal_eval(str(x)) if t]\n",
    "    except:\n",
    "        return [t.strip() for t in str(x).split(\",\") if t.strip()]\n",
    "\n",
    "df[\"tickers_list\"] = df[TICKER_COL].apply(parse_tickers)\n",
    "df[\"n_tickers\"] = df[\"tickers_list\"].apply(len)\n",
    "df = df[df[\"n_tickers\"] > 0].copy()\n",
    "\n",
    "# Explode to per-ticker rows\n",
    "df = df.explode(\"tickers_list\").reset_index(drop=True)\n",
    "df = df.rename(columns={\"tickers_list\": \"ticker\"})\n",
    "\n",
    "print(f\"Per-ticker rows: {len(df):,}\")\n",
    "\n",
    "# ============================================================================\n",
    "# LOAD TWITTER ROBERTA\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\nLoading Twitter-RoBERTa...\")\n",
    "tw_tokenizer = AutoTokenizer.from_pretrained(TW_MODEL_NAME)\n",
    "tw_model     = AutoModelForSequenceClassification.from_pretrained(TW_MODEL_NAME)\n",
    "tw_model.to(device)\n",
    "tw_model.eval()\n",
    "print(\"✓ Loaded\")\n",
    "\n",
    "# ============================================================================\n",
    "# TWITTER ROBERTA FUNCTION\n",
    "# ============================================================================\n",
    "\n",
    "def twitter_batch(texts, tickers):\n",
    "    # Context-aware input: \"TICKER: Text\"\n",
    "    inputs = [f\"{tic}: {txt}\" for txt, tic in zip(texts, tickers)]\n",
    "\n",
    "    enc = tw_tokenizer(\n",
    "        inputs,\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=MAX_INPUT_TOKENS,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        probs = torch.softmax(tw_model(**enc).logits, dim=-1).cpu().numpy()\n",
    "\n",
    "    results = []\n",
    "    for prob in probs:\n",
    "        # cardiffnlp mapping: 0 -> Negative, 1 -> Neutral, 2 -> Positive\n",
    "        p_neg = prob[0]\n",
    "        p_neu = prob[1]\n",
    "        p_pos = prob[2]\n",
    "        \n",
    "        # Weighted Score (-1 to 1)\n",
    "        score = (p_pos * 1.0) + (p_neu * 0.0) + (p_neg * -1.0)\n",
    "        \n",
    "        # Confidence: The highest probability of the three classes\n",
    "        confidence = max(p_pos, p_neu, p_neg)\n",
    "        \n",
    "        # Determine Label\n",
    "        if p_pos > p_neg and p_pos > p_neu:\n",
    "            label = \"positive\"\n",
    "        elif p_neg > p_pos and p_neg > p_neu:\n",
    "            label = \"negative\"\n",
    "        else:\n",
    "            label = \"neutral\"\n",
    "\n",
    "        results.append({\n",
    "            \"score\": score,\n",
    "            \"label\": label,\n",
    "            \"confidence\": confidence,\n",
    "            \"p_pos\": p_pos, \n",
    "            \"p_neg\": p_neg,\n",
    "            \"p_neu\": p_neu\n",
    "        })\n",
    "\n",
    "    return results\n",
    "\n",
    "# ============================================================================\n",
    "# RUN TWITTER ROBERTA\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"RUNNING TWITTER ROBERTA\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Storage lists\n",
    "tw_scores = []\n",
    "tw_labels = []\n",
    "tw_confs  = []\n",
    "tw_pos_probs = []\n",
    "tw_neg_probs = []\n",
    "tw_neu_probs = []\n",
    "\n",
    "texts = df[TEXT_COL].fillna(\"\").tolist()\n",
    "tickers = df[\"ticker\"].tolist()\n",
    "\n",
    "for i in tqdm(range(0, len(texts), BATCH_SIZE_CLS), desc=\"Twitter RoBERTa\"):\n",
    "    batch_texts = texts[i:i+BATCH_SIZE_CLS]\n",
    "    batch_tickers = tickers[i:i+BATCH_SIZE_CLS]\n",
    "    \n",
    "    results = twitter_batch(batch_texts, batch_tickers)\n",
    "    \n",
    "    # UNPACKING FIXED HERE\n",
    "    for res in results:\n",
    "        tw_scores.append(res[\"score\"])\n",
    "        tw_labels.append(res[\"label\"])\n",
    "        tw_confs.append(res[\"confidence\"])\n",
    "        tw_pos_probs.append(res[\"p_pos\"])\n",
    "        tw_neg_probs.append(res[\"p_neg\"])\n",
    "        tw_neu_probs.append(res[\"p_neu\"])\n",
    "\n",
    "# Save all metrics to DataFrame\n",
    "df[\"tw_score\"] = tw_scores\n",
    "df[\"tw_label\"] = tw_labels\n",
    "df[\"tw_confidence\"] = tw_confs\n",
    "df[\"tw_prob_pos\"] = tw_pos_probs # Useful for visualization\n",
    "df[\"tw_prob_neg\"] = tw_neg_probs # Useful for visualization\n",
    "df[\"tw_prob_neu\"] = tw_neu_probs\n",
    "\n",
    "print(f\"\\nTwitter-RoBERTa Results:\")\n",
    "print(f\"  Label distribution:\\n{df['tw_label'].value_counts()}\")\n",
    "print(f\"  Avg Confidence: {df['tw_confidence'].mean():.3f}\")\n",
    "\n",
    "# ============================================================================\n",
    "# SELECT UNCERTAIN FOR LLM\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SELECTING UNCERTAIN CASES FOR LLM\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Filter: Send to LLM if RoBERTa isn't at least 65% sure of its answer\n",
    "df_uncertain = df[df[\"tw_confidence\"] < CONFIDENCE_THRESHOLD].copy()\n",
    "\n",
    "print(f\"Total rows: {len(df)}\")\n",
    "print(f\"Confident rows: {len(df) - len(df_uncertain)}\")\n",
    "print(f\"Uncertain rows (to LLM): {len(df_uncertain)} ({len(df_uncertain)/len(df)*100:.1f}%)\")\n",
    "\n",
    "if len(df_uncertain) == 0:\n",
    "    print(\"No uncertain cases found. Saving immediately.\")\n",
    "    df.to_csv(OUTPUT_FILE, index=False)\n",
    "    exit()\n",
    "\n",
    "# ============================================================================\n",
    "# LOAD QWEN LLM\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"LOADING QWEN LLM\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "llm_tokenizer = AutoTokenizer.from_pretrained(LLM_MODEL_NAME)\n",
    "llm_model = AutoModelForCausalLM.from_pretrained(\n",
    "    LLM_MODEL_NAME,\n",
    "    torch_dtype=torch.float16 if device==\"cuda\" else None,\n",
    "    device_map=\"auto\" if device==\"cuda\" else None\n",
    ")\n",
    "\n",
    "llm_model.eval()\n",
    "\n",
    "if llm_tokenizer.pad_token is None:\n",
    "    llm_tokenizer.pad_token = llm_tokenizer.eos_token\n",
    "\n",
    "print(\"✓ Qwen loaded\")\n",
    "\n",
    "# ============================================================================\n",
    "# LLM PROMPTING & PARSING (JSON MODE)\n",
    "# ============================================================================\n",
    "\n",
    "def build_prompt(text, ticker):\n",
    "    # Force JSON output for easier parsing with small models\n",
    "    return f\"\"\"You are a financial sentiment expert.\n",
    "Analyze the sentiment of the text below regarding the ticker: {ticker}.\n",
    "\n",
    "Return a JSON object with:\n",
    "1. \"reasoning\": A brief explanation (max 15 words).\n",
    "2. \"sentiment\": \"Positive\", \"Negative\", or \"Neutral\".\n",
    "3. \"score\": A float between -1.0 (Very Negative) and 1.0 (Very Positive).\n",
    "\n",
    "Text: \"{text[:300]}\"\n",
    "Ticker: {ticker}\n",
    "\n",
    "JSON Response:\"\"\"\n",
    "\n",
    "def parse_llm_response(text):\n",
    "    \"\"\"Robust parsing of pseudo-JSON output.\"\"\"\n",
    "    try:\n",
    "        # Attempt to find JSON-like structure\n",
    "        match = re.search(r\"\\{.*\\}\", text, re.DOTALL)\n",
    "        if match:\n",
    "            json_str = match.group(0)\n",
    "            data = json.loads(json_str)\n",
    "            return data.get(\"sentiment\", \"Neutral\").lower(), float(data.get(\"score\", 0.0))\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    # Fallback: Regex for score if JSON fails\n",
    "    score_match = re.search(r\"score\\\":\\s*(-?0\\.\\d+|1\\.0|-1\\.0|-?\\d+)\", text)\n",
    "    if score_match:\n",
    "        val = float(score_match.group(1))\n",
    "        label = \"positive\" if val > 0.1 else (\"negative\" if val < -0.1 else \"neutral\")\n",
    "        return label, max(-1.0, min(1.0, val))\n",
    "\n",
    "    return \"neutral\", 0.0  # complete failure fallback\n",
    "\n",
    "# ============================================================================\n",
    "# RUN LLM\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"RUNNING LLM ON UNCERTAIN CASES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "llm_labels = []\n",
    "llm_scores = []\n",
    "\n",
    "texts_llm = df_uncertain[TEXT_COL].tolist()\n",
    "tickers_llm = df_uncertain[\"ticker\"].tolist()\n",
    "\n",
    "# Define batch function for LLM\n",
    "def run_llm_batch(texts, tickers):\n",
    "    prompts = [build_prompt(t, tic) for t, tic in zip(texts, tickers)]\n",
    "    \n",
    "    inputs = llm_tokenizer(\n",
    "        prompts, \n",
    "        return_tensors=\"pt\",\n",
    "        padding=True, \n",
    "        truncation=True,\n",
    "        max_length=MAX_INPUT_TOKENS\n",
    "    ).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = llm_model.generate(\n",
    "            **inputs, \n",
    "            max_new_tokens=MAX_NEW_TOKENS,\n",
    "            do_sample=False, # Deterministic\n",
    "            pad_token_id=llm_tokenizer.pad_token_id\n",
    "        )\n",
    "    \n",
    "    decoded = llm_tokenizer.batch_decode(outputs[:, inputs.input_ids.shape[1]:], skip_special_tokens=True)\n",
    "    return [parse_llm_response(d) for d in decoded]\n",
    "\n",
    "# Loop\n",
    "for i in tqdm(range(0, len(texts_llm), BATCH_SIZE_LLM), desc=\"LLM Inference\"):\n",
    "    b_texts = texts_llm[i:i+BATCH_SIZE_LLM]\n",
    "    b_tickers = tickers_llm[i:i+BATCH_SIZE_LLM]\n",
    "    \n",
    "    try:\n",
    "        results = run_llm_batch(b_texts, b_tickers)\n",
    "        for lbl, scr in results:\n",
    "            llm_labels.append(lbl)\n",
    "            llm_scores.append(scr)\n",
    "    except Exception as e:\n",
    "        print(f\"Error in batch {i}: {e}\")\n",
    "        # Error fallback\n",
    "        for _ in range(len(b_texts)):\n",
    "            llm_labels.append(\"neutral\")\n",
    "            llm_scores.append(0.0)\n",
    "\n",
    "# ============================================================================\n",
    "# MERGE & SAVE\n",
    "# ============================================================================\n",
    "\n",
    "# Initialize columns with RoBERTa values\n",
    "df[\"final_sentiment_label\"] = df[\"tw_label\"]\n",
    "df[\"final_sentiment_score\"] = df[\"tw_score\"]\n",
    "df[\"source_model\"] = \"RoBERTa\"\n",
    "\n",
    "# Update with LLM values\n",
    "df.loc[df_uncertain.index, \"final_sentiment_label\"] = llm_labels\n",
    "df.loc[df_uncertain.index, \"final_sentiment_score\"] = llm_scores\n",
    "df.loc[df_uncertain.index, \"source_model\"] = \"LLM\"\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SAVING FINAL RESULTS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "df.to_csv(OUTPUT_FILE, index=False)\n",
    "print(f\"✓ Saved to {OUTPUT_FILE}\")\n",
    "print(f\"  - RoBERTa rows: {len(df[df['source_model']=='RoBERTa'])}\")\n",
    "print(f\"  - LLM rows:     {len(df[df['source_model']=='LLM'])}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
