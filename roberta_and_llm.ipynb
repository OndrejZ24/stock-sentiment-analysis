{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "05435721",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:utils:spaCy model loaded successfully\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.13.5 (tags/v3.13.5:6cb20a2, Jun 11 2025, 16:15:46) [MSC v.1943 64 bit (AMD64)]\n",
      "Torch version: 2.7.1+cu118\n",
      "CUDA available: True\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "from utils import get_oracle_connection\n",
    "import sys\n",
    "print(sys.version)\n",
    "import torch\n",
    "print(\"Torch version:\", torch.__version__)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eebb41f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oracle connection successful!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vitek\\AppData\\Local\\Temp\\ipykernel_17652\\98098836.py:22: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql_query(query, conn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1000 rows\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "id",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "sentiment_ready_text",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "type",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "subreddit",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "created_utc",
         "rawType": "datetime64[ns]",
         "type": "datetime"
        },
        {
         "name": "normalized_upvotes",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "mentioned_tickers",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "n_tickers",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "text_length",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "word_count",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "hour",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "day_of_week",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "date",
         "rawType": "datetime64[ns]",
         "type": "datetime"
        }
       ],
       "ref": "4eb216ae-adbe-4667-8c22-e3d80af6902a",
       "rows": [
        [
         "0",
         "khxavmq",
         "Neither are oversold yet BA is getting close Tesla is not. Keep paying attention to RSI and MF. If they do become oversold then their may be a buying opportunity.",
         "comment",
         "ValueInvesting",
         "2024-01-15 04:44:39",
         "0.054083287715911865",
         "RSI,TSLA",
         "2",
         "164",
         "30",
         "4",
         "0",
         "2024-01-15 00:00:00"
        ],
        [
         "1",
         "khxav0f",
         "if you backtest even considering circuit breakers its still >>99% loss peak to trough. none of the market crashes happened in one single day.",
         "comment",
         "investing",
         "2024-01-15 04:44:31",
         "0.12309116125106812",
         "TQQQ",
         "1",
         "141",
         "24",
         "4",
         "0",
         "2024-01-15 00:00:00"
        ],
        [
         "2",
         "khxarso",
         "I will be messaging you in 6 months on **2024-07-15 04:43:12 UTC** ^(delete this message to hide from others.)](|^(Custom)](|[^(Feedback)](| |-|-|-|-|",
         "comment",
         "stocks",
         "2024-01-15 04:43:48",
         "0.05091093108057976",
         "LINK",
         "1",
         "1351",
         "48",
         "4",
         "0",
         "2024-01-15 00:00:00"
        ],
        [
         "3",
         "khxa0kb",
         "RemindMe! 6 months Let’s see u/carsonthecarsinogen",
         "comment",
         "ValueInvesting",
         "2024-01-15 04:37:44",
         "0.05300162360072136",
         "TSLA",
         "1",
         "52",
         "6",
         "4",
         "0",
         "2024-01-15 00:00:00"
        ],
        [
         "4",
         "khx9ymi",
         "Your submission was removed because it is a short post. Any self-post below 250 characters in the body will be removed. Please refer to rule 6 and make sure your post meets our standards of effort. Automod evasion will be met with a lengthy ban. Do not post just an article; highlight the parts of the article you find relevant or offer some commentary surrounding the article. Self-posts that offer some simple thoughts or questions like \"what do you think\", \"here's my thoughts\", etc. belong as comments to existing posts or the Daily Advice Thread if you have any questions or concerns.*",
         "comment",
         "investing",
         "2024-01-15 04:37:19",
         "0.12309116125106812",
         "IQ",
         "1",
         "793",
         "119",
         "4",
         "0",
         "2024-01-15 00:00:00"
        ]
       ],
       "shape": {
        "columns": 13,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sentiment_ready_text</th>\n",
       "      <th>type</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>normalized_upvotes</th>\n",
       "      <th>mentioned_tickers</th>\n",
       "      <th>n_tickers</th>\n",
       "      <th>text_length</th>\n",
       "      <th>word_count</th>\n",
       "      <th>hour</th>\n",
       "      <th>day_of_week</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>khxavmq</td>\n",
       "      <td>Neither are oversold yet BA is getting close T...</td>\n",
       "      <td>comment</td>\n",
       "      <td>ValueInvesting</td>\n",
       "      <td>2024-01-15 04:44:39</td>\n",
       "      <td>0.054083</td>\n",
       "      <td>RSI,TSLA</td>\n",
       "      <td>2</td>\n",
       "      <td>164</td>\n",
       "      <td>30</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>2024-01-15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>khxav0f</td>\n",
       "      <td>if you backtest even considering circuit break...</td>\n",
       "      <td>comment</td>\n",
       "      <td>investing</td>\n",
       "      <td>2024-01-15 04:44:31</td>\n",
       "      <td>0.123091</td>\n",
       "      <td>TQQQ</td>\n",
       "      <td>1</td>\n",
       "      <td>141</td>\n",
       "      <td>24</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>2024-01-15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>khxarso</td>\n",
       "      <td>I will be messaging you in 6 months on **2024-...</td>\n",
       "      <td>comment</td>\n",
       "      <td>stocks</td>\n",
       "      <td>2024-01-15 04:43:48</td>\n",
       "      <td>0.050911</td>\n",
       "      <td>LINK</td>\n",
       "      <td>1</td>\n",
       "      <td>1351</td>\n",
       "      <td>48</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>2024-01-15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>khxa0kb</td>\n",
       "      <td>RemindMe! 6 months Let’s see u/carsonthecarsin...</td>\n",
       "      <td>comment</td>\n",
       "      <td>ValueInvesting</td>\n",
       "      <td>2024-01-15 04:37:44</td>\n",
       "      <td>0.053002</td>\n",
       "      <td>TSLA</td>\n",
       "      <td>1</td>\n",
       "      <td>52</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>2024-01-15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>khx9ymi</td>\n",
       "      <td>Your submission was removed because it is a sh...</td>\n",
       "      <td>comment</td>\n",
       "      <td>investing</td>\n",
       "      <td>2024-01-15 04:37:19</td>\n",
       "      <td>0.123091</td>\n",
       "      <td>IQ</td>\n",
       "      <td>1</td>\n",
       "      <td>793</td>\n",
       "      <td>119</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>2024-01-15</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                               sentiment_ready_text     type  \\\n",
       "0  khxavmq  Neither are oversold yet BA is getting close T...  comment   \n",
       "1  khxav0f  if you backtest even considering circuit break...  comment   \n",
       "2  khxarso  I will be messaging you in 6 months on **2024-...  comment   \n",
       "3  khxa0kb  RemindMe! 6 months Let’s see u/carsonthecarsin...  comment   \n",
       "4  khx9ymi  Your submission was removed because it is a sh...  comment   \n",
       "\n",
       "        subreddit         created_utc  normalized_upvotes mentioned_tickers  \\\n",
       "0  ValueInvesting 2024-01-15 04:44:39            0.054083          RSI,TSLA   \n",
       "1       investing 2024-01-15 04:44:31            0.123091              TQQQ   \n",
       "2          stocks 2024-01-15 04:43:48            0.050911              LINK   \n",
       "3  ValueInvesting 2024-01-15 04:37:44            0.053002              TSLA   \n",
       "4       investing 2024-01-15 04:37:19            0.123091                IQ   \n",
       "\n",
       "   n_tickers  text_length  word_count  hour  day_of_week       date  \n",
       "0          2          164          30     4            0 2024-01-15  \n",
       "1          1          141          24     4            0 2024-01-15  \n",
       "2          1         1351          48     4            0 2024-01-15  \n",
       "3          1           52           6     4            0 2024-01-15  \n",
       "4          1          793         119     4            0 2024-01-15  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conn = get_oracle_connection()\n",
    "\n",
    "query = \"\"\"\n",
    "SELECT\n",
    "    ID,\n",
    "    DBMS_LOB.SUBSTR(SENTIMENT_READY_TEXT, 20000, 1) as SENTIMENT_READY_TEXT,\n",
    "    TYPE,\n",
    "    SUBREDDIT,\n",
    "    CREATED_UTC,\n",
    "    NORMALIZED_UPVOTES,\n",
    "    DBMS_LOB.SUBSTR(MENTIONED_TICKERS, 100, 1) as MENTIONED_TICKERS,\n",
    "    N_TICKERS,\n",
    "    TEXT_LENGTH,\n",
    "    WORD_COUNT,\n",
    "    DATE_COL,\n",
    "    HOUR,\n",
    "    DAY_OF_WEEK\n",
    "FROM preprocessed_data\n",
    "FETCH FIRST 1000 ROWS ONLY\n",
    "\"\"\"\n",
    "\n",
    "df = pd.read_sql_query(query, conn)\n",
    "conn.close()\n",
    "\n",
    "df.columns = df.columns.str.lower()\n",
    "df['created_utc'] = pd.to_datetime(df['created_utc'], unit='s')\n",
    "if 'date_col' in df.columns:\n",
    "    df['date'] = pd.to_datetime(df['date_col'])\n",
    "    df.drop(columns=['date_col'], inplace=True)\n",
    "\n",
    "print(f\"Loaded {len(df)} rows\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b762ec03",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vitek\\AppData\\Roaming\\Python\\Python313\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "FIXED HYBRID SENTIMENT PIPELINE\n",
      "================================================================================\n",
      "Device: cuda\n",
      "Per-ticker rows: 1,863\n",
      "\n",
      "Loading Twitter-RoBERTa...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Loaded\n",
      "\n",
      "================================================================================\n",
      "RUNNING TWITTER ROBERTA\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Twitter RoBERTa: 100%|██████████| 30/30 [00:41<00:00,  1.37s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Twitter-RoBERTa Results:\n",
      "  Label distribution:\n",
      "tw_label\n",
      "neutral     993\n",
      "negative    498\n",
      "positive    372\n",
      "Name: count, dtype: int64\n",
      "  Avg Confidence: 0.718\n",
      "\n",
      "================================================================================\n",
      "SELECTING UNCERTAIN CASES FOR LLM\n",
      "================================================================================\n",
      "Total rows: 1863\n",
      "Confident rows: 1160\n",
      "Uncertain rows (to LLM): 703 (37.7%)\n",
      "\n",
      "================================================================================\n",
      "LOADING QWEN LLM\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "INFO:accelerate.utils.modeling:We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Qwen loaded\n",
      "\n",
      "================================================================================\n",
      "RUNNING LLM ON UNCERTAIN CASES\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LLM Inference:   0%|          | 0/88 [00:00<?, ?it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLM Inference:   1%|          | 1/88 [00:21<31:32, 21.76s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLM Inference:   2%|▏         | 2/88 [00:43<30:53, 21.55s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLM Inference:   3%|▎         | 3/88 [01:04<30:04, 21.23s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLM Inference:   5%|▍         | 4/88 [01:24<29:30, 21.08s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLM Inference:   6%|▌         | 5/88 [01:45<28:44, 20.78s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLM Inference:   7%|▋         | 6/88 [02:06<28:27, 20.82s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLM Inference:   8%|▊         | 7/88 [02:26<27:49, 20.61s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLM Inference:   9%|▉         | 8/88 [02:47<27:36, 20.71s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLM Inference:  10%|█         | 9/88 [03:08<27:21, 20.77s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLM Inference:  11%|█▏        | 10/88 [03:28<27:03, 20.82s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLM Inference:  12%|█▎        | 11/88 [03:49<26:46, 20.86s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLM Inference:  14%|█▎        | 12/88 [04:10<26:28, 20.90s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLM Inference:  15%|█▍        | 13/88 [04:31<26:09, 20.93s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLM Inference:  16%|█▌        | 14/88 [04:53<25:59, 21.07s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLM Inference:  17%|█▋        | 15/88 [05:14<25:46, 21.18s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLM Inference:  18%|█▊        | 16/88 [05:34<25:03, 20.88s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLM Inference:  19%|█▉        | 17/88 [05:55<24:27, 20.67s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLM Inference:  20%|██        | 18/88 [06:15<24:10, 20.72s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLM Inference:  22%|██▏       | 19/88 [06:37<24:02, 20.91s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLM Inference:  23%|██▎       | 20/88 [06:58<23:40, 20.88s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLM Inference:  24%|██▍       | 21/88 [07:18<23:19, 20.88s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLM Inference:  25%|██▌       | 22/88 [07:39<22:44, 20.67s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLM Inference:  26%|██▌       | 23/88 [07:59<22:26, 20.72s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLM Inference:  27%|██▋       | 24/88 [08:20<22:08, 20.76s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLM Inference:  28%|██▊       | 25/88 [08:41<21:49, 20.79s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLM Inference:  30%|██▉       | 26/88 [09:02<21:30, 20.81s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLM Inference:  31%|███       | 27/88 [09:23<21:10, 20.83s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLM Inference:  32%|███▏      | 28/88 [09:44<20:50, 20.85s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLM Inference:  33%|███▎      | 29/88 [10:05<20:38, 21.00s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLM Inference:  34%|███▍      | 30/88 [10:25<20:03, 20.75s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLM Inference:  35%|███▌      | 31/88 [10:46<19:44, 20.78s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLM Inference:  36%|███▋      | 32/88 [11:07<19:24, 20.79s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLM Inference:  38%|███▊      | 33/88 [11:28<19:05, 20.82s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLM Inference:  39%|███▊      | 34/88 [11:49<18:44, 20.82s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLM Inference:  40%|███▉      | 35/88 [12:10<18:23, 20.82s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLM Inference:  41%|████      | 36/88 [12:30<18:03, 20.84s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLM Inference:  42%|████▏     | 37/88 [12:51<17:43, 20.86s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLM Inference:  43%|████▎     | 38/88 [13:12<17:22, 20.85s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLM Inference:  44%|████▍     | 39/88 [13:33<17:01, 20.85s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLM Inference:  45%|████▌     | 40/88 [13:53<16:30, 20.64s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLM Inference:  47%|████▋     | 41/88 [14:15<16:20, 20.87s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLM Inference:  48%|████▊     | 42/88 [14:35<16:00, 20.88s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLM Inference:  49%|████▉     | 43/88 [14:56<15:39, 20.88s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLM Inference:  50%|█████     | 44/88 [15:18<15:26, 21.05s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLM Inference:  51%|█████     | 45/88 [15:38<14:54, 20.81s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLM Inference:  52%|█████▏    | 46/88 [15:58<14:26, 20.64s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLM Inference:  53%|█████▎    | 47/88 [16:19<14:05, 20.63s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLM Inference:  55%|█████▍    | 48/88 [16:40<13:52, 20.82s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLM Inference:  56%|█████▌    | 49/88 [17:01<13:32, 20.84s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLM Inference:  57%|█████▋    | 50/88 [17:23<13:21, 21.09s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLM Inference:  58%|█████▊    | 51/88 [17:44<13:02, 21.15s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLM Inference:  59%|█████▉    | 52/88 [18:04<12:29, 20.82s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLM Inference:  60%|██████    | 53/88 [18:26<12:25, 21.30s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLM Inference:  61%|██████▏   | 54/88 [18:47<11:59, 21.17s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLM Inference:  62%|██████▎   | 55/88 [19:08<11:35, 21.07s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLM Inference:  64%|██████▎   | 56/88 [19:29<11:11, 21.00s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLM Inference:  65%|██████▍   | 57/88 [19:50<10:47, 20.90s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLM Inference:  66%|██████▌   | 58/88 [20:11<10:29, 20.98s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLM Inference:  67%|██████▋   | 59/88 [20:32<10:09, 21.02s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLM Inference:  68%|██████▊   | 60/88 [20:51<09:33, 20.49s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLM Inference:  69%|██████▉   | 61/88 [21:10<09:02, 20.10s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLM Inference:  70%|███████   | 62/88 [21:31<08:46, 20.24s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLM Inference:  72%|███████▏  | 63/88 [21:52<08:28, 20.33s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLM Inference:  73%|███████▎  | 64/88 [22:13<08:13, 20.55s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLM Inference:  74%|███████▍  | 65/88 [22:33<07:52, 20.55s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLM Inference:  75%|███████▌  | 66/88 [22:53<07:27, 20.36s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLM Inference:  76%|███████▌  | 67/88 [23:14<07:08, 20.42s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLM Inference:  77%|███████▋  | 68/88 [23:34<06:45, 20.27s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLM Inference:  78%|███████▊  | 69/88 [23:54<06:26, 20.36s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLM Inference:  80%|███████▉  | 70/88 [24:15<06:07, 20.43s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLM Inference:  81%|████████  | 71/88 [24:35<05:44, 20.27s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLM Inference:  82%|████████▏ | 72/88 [24:55<05:26, 20.38s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLM Inference:  83%|████████▎ | 73/88 [25:16<05:06, 20.45s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLM Inference:  84%|████████▍ | 74/88 [25:36<04:46, 20.49s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLM Inference:  85%|████████▌ | 75/88 [25:58<04:28, 20.69s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLM Inference:  86%|████████▋ | 76/88 [26:19<04:10, 20.85s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLM Inference:  88%|████████▊ | 77/88 [26:39<03:48, 20.79s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLM Inference:  89%|████████▊ | 78/88 [27:00<03:27, 20.74s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLM Inference:  90%|████████▉ | 79/88 [27:20<03:04, 20.50s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLM Inference:  91%|█████████ | 80/88 [27:41<02:44, 20.55s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLM Inference:  92%|█████████▏| 81/88 [28:01<02:24, 20.59s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLM Inference:  93%|█████████▎| 82/88 [28:22<02:03, 20.62s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLM Inference:  94%|█████████▍| 83/88 [28:44<01:44, 20.90s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLM Inference:  95%|█████████▌| 84/88 [29:05<01:23, 20.98s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLM Inference:  97%|█████████▋| 85/88 [29:25<01:02, 20.75s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLM Inference:  98%|█████████▊| 86/88 [29:46<00:41, 20.97s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLM Inference:  99%|█████████▉| 87/88 [30:08<00:21, 21.03s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLM Inference: 100%|██████████| 88/88 [30:26<00:00, 20.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "SAVING FINAL RESULTS\n",
      "================================================================================\n",
      "✓ Saved to sentiment_hybrid_twitter_llm.csv\n",
      "  - RoBERTa rows: 1160\n",
      "  - LLM rows:     703\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "FIXED HYBRID SENTIMENT PIPELINE (RoBERTa + Qwen 1.5B)\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoModelForCausalLM\n",
    ")\n",
    "from tqdm import tqdm\n",
    "import ast\n",
    "import time\n",
    "import numpy as np\n",
    "import json\n",
    "import re\n",
    "\n",
    "# ============================================================================\n",
    "# CONFIG\n",
    "# ============================================================================\n",
    "\n",
    "OUTPUT_FILE = \"sentiment_hybrid_twitter_llm.csv\"\n",
    "\n",
    "TEXT_COL   = \"sentiment_ready_text\"\n",
    "TICKER_COL = \"mentioned_tickers\"\n",
    "\n",
    "TW_MODEL_NAME  = \"cardiffnlp/twitter-roberta-base-sentiment-latest\"\n",
    "LLM_MODEL_NAME = \"Qwen/Qwen2.5-1.5B-Instruct\"\n",
    "\n",
    "BATCH_SIZE_CLS = 64\n",
    "BATCH_SIZE_LLM = 8\n",
    "\n",
    "MAX_INPUT_TOKENS = 1500 # change this for length\n",
    "\n",
    "MAX_NEW_TOKENS   = 128 # Increased for JSON reasoning\n",
    "CONFIDENCE_THRESHOLD = 0.5 # Confidence below this triggers LLM\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"FIXED HYBRID SENTIMENT PIPELINE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# ============================================================================\n",
    "# DEVICE\n",
    "# ============================================================================\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Device:\", device)\n",
    "\n",
    "# ============================================================================\n",
    "# DATA PREPARATION\n",
    "# ============================================================================\n",
    "# Processing rows for tickers\n",
    "def parse_tickers(x):\n",
    "    if pd.isna(x) or x == \"\":\n",
    "        return []\n",
    "    try:\n",
    "        return [str(t).strip() for t in ast.literal_eval(str(x)) if t]\n",
    "    except:\n",
    "        return [t.strip() for t in str(x).split(\",\") if t.strip()]\n",
    "\n",
    "df[\"tickers_list\"] = df[TICKER_COL].apply(parse_tickers)\n",
    "df[\"n_tickers\"] = df[\"tickers_list\"].apply(len)\n",
    "df = df[df[\"n_tickers\"] > 0].copy()\n",
    "\n",
    "# Explode to per-ticker rows\n",
    "df = df.explode(\"tickers_list\").reset_index(drop=True)\n",
    "df = df.rename(columns={\"tickers_list\": \"ticker\"})\n",
    "\n",
    "print(f\"Per-ticker rows: {len(df):,}\")\n",
    "\n",
    "# ============================================================================\n",
    "# LOAD TWITTER ROBERTA\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\nLoading Twitter-RoBERTa...\")\n",
    "tw_tokenizer = AutoTokenizer.from_pretrained(TW_MODEL_NAME)\n",
    "tw_model     = AutoModelForSequenceClassification.from_pretrained(TW_MODEL_NAME)\n",
    "tw_model.to(device)\n",
    "tw_model.eval()\n",
    "print(\"✓ Loaded\")\n",
    "\n",
    "# ============================================================================\n",
    "# TWITTER ROBERTA FUNCTION\n",
    "# ============================================================================\n",
    "\n",
    "def twitter_batch(texts, tickers):\n",
    "    # Context-aware input: \"TICKER: Text\"\n",
    "    inputs = [f\"{tic}: {txt}\" for txt, tic in zip(texts, tickers)]\n",
    "\n",
    "    enc = tw_tokenizer(\n",
    "        inputs,\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=MAX_INPUT_TOKENS,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        probs = torch.softmax(tw_model(**enc).logits, dim=-1).cpu().numpy()\n",
    "\n",
    "    results = []\n",
    "    for prob in probs:\n",
    "        # cardiffnlp mapping: 0 -> Negative, 1 -> Neutral, 2 -> Positive\n",
    "        p_neg = prob[0]\n",
    "        p_neu = prob[1]\n",
    "        p_pos = prob[2]\n",
    "        \n",
    "        # Weighted Score (-1 to 1)\n",
    "        score = (p_pos * 1.0) + (p_neu * 0.0) + (p_neg * -1.0)\n",
    "        \n",
    "        # Confidence: The highest probability of the three classes\n",
    "        confidence = max(p_pos, p_neu, p_neg)\n",
    "        \n",
    "        # Determine Label\n",
    "        if p_pos > p_neg and p_pos > p_neu:\n",
    "            label = \"positive\"\n",
    "        elif p_neg > p_pos and p_neg > p_neu:\n",
    "            label = \"negative\"\n",
    "        else:\n",
    "            label = \"neutral\"\n",
    "\n",
    "        results.append({\n",
    "            \"score\": score,\n",
    "            \"label\": label,\n",
    "            \"confidence\": confidence,\n",
    "            \"p_pos\": p_pos, \n",
    "            \"p_neg\": p_neg,\n",
    "            \"p_neu\": p_neu\n",
    "        })\n",
    "\n",
    "    return results\n",
    "\n",
    "# ============================================================================\n",
    "# RUN TWITTER ROBERTA\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"RUNNING TWITTER ROBERTA\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Storage lists\n",
    "tw_scores = []\n",
    "tw_labels = []\n",
    "tw_confs  = []\n",
    "tw_pos_probs = []\n",
    "tw_neg_probs = []\n",
    "tw_neu_probs = []\n",
    "\n",
    "texts = df[TEXT_COL].fillna(\"\").tolist()\n",
    "tickers = df[\"ticker\"].tolist()\n",
    "\n",
    "for i in tqdm(range(0, len(texts), BATCH_SIZE_CLS), desc=\"Twitter RoBERTa\"):\n",
    "    batch_texts = texts[i:i+BATCH_SIZE_CLS]\n",
    "    batch_tickers = tickers[i:i+BATCH_SIZE_CLS]\n",
    "    \n",
    "    results = twitter_batch(batch_texts, batch_tickers)\n",
    "    \n",
    "    # UNPACKING FIXED HERE\n",
    "    for res in results:\n",
    "        tw_scores.append(res[\"score\"])\n",
    "        tw_labels.append(res[\"label\"])\n",
    "        tw_confs.append(res[\"confidence\"])\n",
    "        tw_pos_probs.append(res[\"p_pos\"])\n",
    "        tw_neg_probs.append(res[\"p_neg\"])\n",
    "        tw_neu_probs.append(res[\"p_neu\"])\n",
    "\n",
    "# Save all metrics to DataFrame\n",
    "df[\"tw_score\"] = tw_scores\n",
    "df[\"tw_label\"] = tw_labels\n",
    "df[\"tw_confidence\"] = tw_confs\n",
    "df[\"tw_prob_pos\"] = tw_pos_probs # Useful for visualization\n",
    "df[\"tw_prob_neg\"] = tw_neg_probs # Useful for visualization\n",
    "df[\"tw_prob_neu\"] = tw_neu_probs\n",
    "\n",
    "print(f\"\\nTwitter-RoBERTa Results:\")\n",
    "print(f\"  Label distribution:\\n{df['tw_label'].value_counts()}\")\n",
    "print(f\"  Avg Confidence: {df['tw_confidence'].mean():.3f}\")\n",
    "\n",
    "# ============================================================================\n",
    "# SELECT UNCERTAIN FOR LLM\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SELECTING UNCERTAIN CASES FOR LLM\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Filter: Send to LLM if RoBERTa isn't at least 65% sure of its answer\n",
    "df_uncertain = df[df[\"tw_confidence\"] < CONFIDENCE_THRESHOLD].copy()\n",
    "\n",
    "print(f\"Total rows: {len(df)}\")\n",
    "print(f\"Confident rows: {len(df) - len(df_uncertain)}\")\n",
    "print(f\"Uncertain rows (to LLM): {len(df_uncertain)} ({len(df_uncertain)/len(df)*100:.1f}%)\")\n",
    "\n",
    "if len(df_uncertain) == 0:\n",
    "    print(\"No uncertain cases found. Saving immediately.\")\n",
    "    df.to_csv(OUTPUT_FILE, index=False)\n",
    "    exit()\n",
    "\n",
    "# ============================================================================\n",
    "# LOAD QWEN LLM\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"LOADING QWEN LLM\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "llm_tokenizer = AutoTokenizer.from_pretrained(LLM_MODEL_NAME)\n",
    "llm_model = AutoModelForCausalLM.from_pretrained(\n",
    "    LLM_MODEL_NAME,\n",
    "    torch_dtype=torch.float16 if device==\"cuda\" else None,\n",
    "    device_map=\"auto\" if device==\"cuda\" else None\n",
    ")\n",
    "\n",
    "llm_model.eval()\n",
    "\n",
    "if llm_tokenizer.pad_token is None:\n",
    "    llm_tokenizer.pad_token = llm_tokenizer.eos_token\n",
    "\n",
    "print(\"✓ Qwen loaded\")\n",
    "\n",
    "# ============================================================================\n",
    "# LLM PROMPTING & PARSING (JSON MODE)\n",
    "# ============================================================================\n",
    "\n",
    "def build_prompt(text, ticker):\n",
    "    # Force JSON output for easier parsing with small models\n",
    "    return f\"\"\"You are a financial sentiment expert.\n",
    "Analyze the sentiment of the text below regarding the ticker: {ticker}.\n",
    "\n",
    "Return a JSON object with:\n",
    "1. \"reasoning\": A brief explanation (max 15 words).\n",
    "2. \"sentiment\": \"Positive\", \"Negative\", or \"Neutral\".\n",
    "3. \"score\": A float between -1.0 (Very Negative) and 1.0 (Very Positive).\n",
    "\n",
    "Text: \"{text[:300]}\"\n",
    "Ticker: {ticker}\n",
    "\n",
    "JSON Response:\"\"\"\n",
    "\n",
    "def parse_llm_response(text):\n",
    "    \"\"\"Robust parsing of pseudo-JSON output.\"\"\"\n",
    "    try:\n",
    "        # Attempt to find JSON-like structure\n",
    "        match = re.search(r\"\\{.*\\}\", text, re.DOTALL)\n",
    "        if match:\n",
    "            json_str = match.group(0)\n",
    "            data = json.loads(json_str)\n",
    "            return data.get(\"sentiment\", \"Neutral\").lower(), float(data.get(\"score\", 0.0))\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    # Fallback: Regex for score if JSON fails\n",
    "    score_match = re.search(r\"score\\\":\\s*(-?0\\.\\d+|1\\.0|-1\\.0|-?\\d+)\", text)\n",
    "    if score_match:\n",
    "        val = float(score_match.group(1))\n",
    "        label = \"positive\" if val > 0.1 else (\"negative\" if val < -0.1 else \"neutral\")\n",
    "        return label, max(-1.0, min(1.0, val))\n",
    "\n",
    "    return \"neutral\", 0.0  # complete failure fallback\n",
    "\n",
    "# ============================================================================\n",
    "# RUN LLM\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"RUNNING LLM ON UNCERTAIN CASES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "llm_labels = []\n",
    "llm_scores = []\n",
    "\n",
    "texts_llm = df_uncertain[TEXT_COL].tolist()\n",
    "tickers_llm = df_uncertain[\"ticker\"].tolist()\n",
    "\n",
    "# Define batch function for LLM\n",
    "def run_llm_batch(texts, tickers):\n",
    "    prompts = [build_prompt(t, tic) for t, tic in zip(texts, tickers)]\n",
    "    \n",
    "    inputs = llm_tokenizer(\n",
    "        prompts, \n",
    "        return_tensors=\"pt\",\n",
    "        padding=True, \n",
    "        truncation=True,\n",
    "        max_length=MAX_INPUT_TOKENS\n",
    "    ).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = llm_model.generate(\n",
    "            **inputs, \n",
    "            max_new_tokens=MAX_NEW_TOKENS,\n",
    "            do_sample=False, # Deterministic\n",
    "            pad_token_id=llm_tokenizer.pad_token_id\n",
    "        )\n",
    "    \n",
    "    decoded = llm_tokenizer.batch_decode(outputs[:, inputs.input_ids.shape[1]:], skip_special_tokens=True)\n",
    "    return [parse_llm_response(d) for d in decoded]\n",
    "\n",
    "# Loop\n",
    "for i in tqdm(range(0, len(texts_llm), BATCH_SIZE_LLM), desc=\"LLM Inference\"):\n",
    "    b_texts = texts_llm[i:i+BATCH_SIZE_LLM]\n",
    "    b_tickers = tickers_llm[i:i+BATCH_SIZE_LLM]\n",
    "    \n",
    "    try:\n",
    "        results = run_llm_batch(b_texts, b_tickers)\n",
    "        for lbl, scr in results:\n",
    "            llm_labels.append(lbl)\n",
    "            llm_scores.append(scr)\n",
    "    except Exception as e:\n",
    "        print(f\"Error in batch {i}: {e}\")\n",
    "        # Error fallback\n",
    "        for _ in range(len(b_texts)):\n",
    "            llm_labels.append(\"neutral\")\n",
    "            llm_scores.append(0.0)\n",
    "\n",
    "# ============================================================================\n",
    "# MERGE & SAVE\n",
    "# ============================================================================\n",
    "\n",
    "# Initialize columns with RoBERTa values\n",
    "df[\"final_sentiment_label\"] = df[\"tw_label\"]\n",
    "df[\"final_sentiment_score\"] = df[\"tw_score\"]\n",
    "df[\"source_model\"] = \"RoBERTa\"\n",
    "\n",
    "# Update with LLM values\n",
    "df.loc[df_uncertain.index, \"final_sentiment_label\"] = llm_labels\n",
    "df.loc[df_uncertain.index, \"final_sentiment_score\"] = llm_scores\n",
    "df.loc[df_uncertain.index, \"source_model\"] = \"LLM\"\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SAVING FINAL RESULTS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "df.to_csv(OUTPUT_FILE, index=False)\n",
    "print(f\"✓ Saved to {OUTPUT_FILE}\")\n",
    "print(f\"  - RoBERTa rows: {len(df[df['source_model']=='RoBERTa'])}\")\n",
    "print(f\"  - LLM rows:     {len(df[df['source_model']=='LLM'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "96ee3b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df=pd.read_csv('sentiment_hybrid_twitter_llm.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d3391f98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "id",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "sentiment_ready_text",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "type",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "subreddit",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "created_utc",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "normalized_upvotes",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "mentioned_tickers",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "n_tickers",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "text_length",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "word_count",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "hour",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "day_of_week",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "date",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "ticker",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "tw_score",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "tw_label",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "tw_confidence",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "tw_prob_pos",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "tw_prob_neg",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "tw_prob_neu",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "final_sentiment_label",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "final_sentiment_score",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "source_model",
         "rawType": "object",
         "type": "string"
        }
       ],
       "ref": "d6305400-b800-4852-b101-b238fbadb2d9",
       "rows": [
        [
         "0",
         "khxavmq",
         "Neither are oversold yet BA is getting close Tesla is not. Keep paying attention to RSI and MF. If they do become oversold then their may be a buying opportunity.",
         "comment",
         "ValueInvesting",
         "2024-01-15 04:44:39",
         "0.0540832877159118",
         "RSI,TSLA",
         "2",
         "164",
         "30",
         "4",
         "0",
         "2024-01-15",
         "RSI",
         "0.111930855",
         "neutral",
         "0.638876",
         "0.23652744",
         "0.12459659",
         "0.638876",
         "negative",
         "-0.4",
         "LLM"
        ],
        [
         "1",
         "khxavmq",
         "Neither are oversold yet BA is getting close Tesla is not. Keep paying attention to RSI and MF. If they do become oversold then their may be a buying opportunity.",
         "comment",
         "ValueInvesting",
         "2024-01-15 04:44:39",
         "0.0540832877159118",
         "RSI,TSLA",
         "2",
         "164",
         "30",
         "4",
         "0",
         "2024-01-15",
         "TSLA",
         "0.12239517",
         "neutral",
         "0.6191686",
         "0.2516133",
         "0.12921812",
         "0.6191686",
         "neutral",
         "0.0",
         "LLM"
        ],
        [
         "2",
         "khxav0f",
         "if you backtest even considering circuit breakers its still >>99% loss peak to trough. none of the market crashes happened in one single day.",
         "comment",
         "investing",
         "2024-01-15 04:44:31",
         "0.1230911612510681",
         "TQQQ",
         "1",
         "141",
         "24",
         "4",
         "0",
         "2024-01-15",
         "TQQQ",
         "-0.43147087",
         "neutral",
         "0.48704162",
         "0.04074375",
         "0.4722146",
         "0.48704162",
         "negative",
         "-0.8",
         "LLM"
        ],
        [
         "3",
         "khxarso",
         "I will be messaging you in 6 months on **2024-07-15 04:43:12 UTC** ^(delete this message to hide from others.)](|^(Custom)](|[^(Feedback)](| |-|-|-|-|",
         "comment",
         "stocks",
         "2024-01-15 04:43:48",
         "0.0509109310805797",
         "LINK",
         "1",
         "1351",
         "48",
         "4",
         "0",
         "2024-01-15",
         "LINK",
         "-0.17677087",
         "neutral",
         "0.7367967",
         "0.043216214",
         "0.21998708",
         "0.7367967",
         "neutral",
         "-0.17677087",
         "RoBERTa"
        ],
        [
         "4",
         "khxa0kb",
         "RemindMe! 6 months Let’s see u/carsonthecarsinogen",
         "comment",
         "ValueInvesting",
         "2024-01-15 04:37:44",
         "0.0530016236007213",
         "TSLA",
         "1",
         "52",
         "6",
         "4",
         "0",
         "2024-01-15",
         "TSLA",
         "0.32956335",
         "neutral",
         "0.65117425",
         "0.33919454",
         "0.0096312",
         "0.65117425",
         "neutral",
         "0.32956335",
         "RoBERTa"
        ]
       ],
       "shape": {
        "columns": 23,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sentiment_ready_text</th>\n",
       "      <th>type</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>normalized_upvotes</th>\n",
       "      <th>mentioned_tickers</th>\n",
       "      <th>n_tickers</th>\n",
       "      <th>text_length</th>\n",
       "      <th>word_count</th>\n",
       "      <th>...</th>\n",
       "      <th>ticker</th>\n",
       "      <th>tw_score</th>\n",
       "      <th>tw_label</th>\n",
       "      <th>tw_confidence</th>\n",
       "      <th>tw_prob_pos</th>\n",
       "      <th>tw_prob_neg</th>\n",
       "      <th>tw_prob_neu</th>\n",
       "      <th>final_sentiment_label</th>\n",
       "      <th>final_sentiment_score</th>\n",
       "      <th>source_model</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>khxavmq</td>\n",
       "      <td>Neither are oversold yet BA is getting close T...</td>\n",
       "      <td>comment</td>\n",
       "      <td>ValueInvesting</td>\n",
       "      <td>2024-01-15 04:44:39</td>\n",
       "      <td>0.054083</td>\n",
       "      <td>RSI,TSLA</td>\n",
       "      <td>2</td>\n",
       "      <td>164</td>\n",
       "      <td>30</td>\n",
       "      <td>...</td>\n",
       "      <td>RSI</td>\n",
       "      <td>0.111931</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.638876</td>\n",
       "      <td>0.236527</td>\n",
       "      <td>0.124597</td>\n",
       "      <td>0.638876</td>\n",
       "      <td>negative</td>\n",
       "      <td>-0.400000</td>\n",
       "      <td>LLM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>khxavmq</td>\n",
       "      <td>Neither are oversold yet BA is getting close T...</td>\n",
       "      <td>comment</td>\n",
       "      <td>ValueInvesting</td>\n",
       "      <td>2024-01-15 04:44:39</td>\n",
       "      <td>0.054083</td>\n",
       "      <td>RSI,TSLA</td>\n",
       "      <td>2</td>\n",
       "      <td>164</td>\n",
       "      <td>30</td>\n",
       "      <td>...</td>\n",
       "      <td>TSLA</td>\n",
       "      <td>0.122395</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.619169</td>\n",
       "      <td>0.251613</td>\n",
       "      <td>0.129218</td>\n",
       "      <td>0.619169</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>LLM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>khxav0f</td>\n",
       "      <td>if you backtest even considering circuit break...</td>\n",
       "      <td>comment</td>\n",
       "      <td>investing</td>\n",
       "      <td>2024-01-15 04:44:31</td>\n",
       "      <td>0.123091</td>\n",
       "      <td>TQQQ</td>\n",
       "      <td>1</td>\n",
       "      <td>141</td>\n",
       "      <td>24</td>\n",
       "      <td>...</td>\n",
       "      <td>TQQQ</td>\n",
       "      <td>-0.431471</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.487042</td>\n",
       "      <td>0.040744</td>\n",
       "      <td>0.472215</td>\n",
       "      <td>0.487042</td>\n",
       "      <td>negative</td>\n",
       "      <td>-0.800000</td>\n",
       "      <td>LLM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>khxarso</td>\n",
       "      <td>I will be messaging you in 6 months on **2024-...</td>\n",
       "      <td>comment</td>\n",
       "      <td>stocks</td>\n",
       "      <td>2024-01-15 04:43:48</td>\n",
       "      <td>0.050911</td>\n",
       "      <td>LINK</td>\n",
       "      <td>1</td>\n",
       "      <td>1351</td>\n",
       "      <td>48</td>\n",
       "      <td>...</td>\n",
       "      <td>LINK</td>\n",
       "      <td>-0.176771</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.736797</td>\n",
       "      <td>0.043216</td>\n",
       "      <td>0.219987</td>\n",
       "      <td>0.736797</td>\n",
       "      <td>neutral</td>\n",
       "      <td>-0.176771</td>\n",
       "      <td>RoBERTa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>khxa0kb</td>\n",
       "      <td>RemindMe! 6 months Let’s see u/carsonthecarsin...</td>\n",
       "      <td>comment</td>\n",
       "      <td>ValueInvesting</td>\n",
       "      <td>2024-01-15 04:37:44</td>\n",
       "      <td>0.053002</td>\n",
       "      <td>TSLA</td>\n",
       "      <td>1</td>\n",
       "      <td>52</td>\n",
       "      <td>6</td>\n",
       "      <td>...</td>\n",
       "      <td>TSLA</td>\n",
       "      <td>0.329563</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.651174</td>\n",
       "      <td>0.339195</td>\n",
       "      <td>0.009631</td>\n",
       "      <td>0.651174</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.329563</td>\n",
       "      <td>RoBERTa</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                               sentiment_ready_text     type  \\\n",
       "0  khxavmq  Neither are oversold yet BA is getting close T...  comment   \n",
       "1  khxavmq  Neither are oversold yet BA is getting close T...  comment   \n",
       "2  khxav0f  if you backtest even considering circuit break...  comment   \n",
       "3  khxarso  I will be messaging you in 6 months on **2024-...  comment   \n",
       "4  khxa0kb  RemindMe! 6 months Let’s see u/carsonthecarsin...  comment   \n",
       "\n",
       "        subreddit          created_utc  normalized_upvotes mentioned_tickers  \\\n",
       "0  ValueInvesting  2024-01-15 04:44:39            0.054083          RSI,TSLA   \n",
       "1  ValueInvesting  2024-01-15 04:44:39            0.054083          RSI,TSLA   \n",
       "2       investing  2024-01-15 04:44:31            0.123091              TQQQ   \n",
       "3          stocks  2024-01-15 04:43:48            0.050911              LINK   \n",
       "4  ValueInvesting  2024-01-15 04:37:44            0.053002              TSLA   \n",
       "\n",
       "   n_tickers  text_length  word_count  ...  ticker  tw_score tw_label  \\\n",
       "0          2          164          30  ...     RSI  0.111931  neutral   \n",
       "1          2          164          30  ...    TSLA  0.122395  neutral   \n",
       "2          1          141          24  ...    TQQQ -0.431471  neutral   \n",
       "3          1         1351          48  ...    LINK -0.176771  neutral   \n",
       "4          1           52           6  ...    TSLA  0.329563  neutral   \n",
       "\n",
       "  tw_confidence  tw_prob_pos tw_prob_neg  tw_prob_neu  final_sentiment_label  \\\n",
       "0      0.638876     0.236527    0.124597     0.638876               negative   \n",
       "1      0.619169     0.251613    0.129218     0.619169                neutral   \n",
       "2      0.487042     0.040744    0.472215     0.487042               negative   \n",
       "3      0.736797     0.043216    0.219987     0.736797                neutral   \n",
       "4      0.651174     0.339195    0.009631     0.651174                neutral   \n",
       "\n",
       "   final_sentiment_score  source_model  \n",
       "0              -0.400000           LLM  \n",
       "1               0.000000           LLM  \n",
       "2              -0.800000           LLM  \n",
       "3              -0.176771       RoBERTa  \n",
       "4               0.329563       RoBERTa  \n",
       "\n",
       "[5 rows x 23 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
